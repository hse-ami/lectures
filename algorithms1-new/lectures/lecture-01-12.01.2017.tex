\documentclass[../book.tex]{subfiles}


\begin{document}
	\section{Лекция 1. Асимптотика, простые алгоритмы, сортировка вставками}
	
	Пусть перед нами стоит задача: найти в некотором массиве медиану. Техническое задание выглядит так: на вход программе подается массив $A$, на выходе хотим получить одну из медиан, неважно какую.
	
	Напомним определение медианы $m$:
	\[
	m \in A = 
	\begin{cases}
		|\{ a \in A \ | \ a < m \}| \leqslant \frac{|A|}{2} \\
		|\{ a \in A \ | \ a > m \}| \leqslant \frac{|A|}{2} \\
	\end{cases}
	\]
	Словами: медиана это такое число, что оно не больше половины элементов, но и не меньше половины элементов.
	
	Легко видеть, что разных медиан в массиве может быть не больше двух, в зависимости от четности числа элементов.
	
	Есть несколько способов решить эту задачу. Приведем несколько из них:
	\begin{algorithm}[H]
		\caption{Алгоритм поиска медианы}
		\begin{algorithmic}[1]
			\Require Массив $A$
			\Ensure Медиана $m$ массива $A$
			\Function{Median}{$A$}
				\State $n := len(A)$
				\For{$i := 0 \ to \ (n-1) $}
					\State $l := 0$
					\State $g := 0$
					\For{$j := 0 \ to \ (n - 1) $}
						\If{$A[j] < A[i]$}
							\State $l := l + 1$
						\ElsIf{$A[j] > A[i]$}
							\State $g := g + 1$
						\EndIf
					\EndFor
				\If{$l \leqslant n/2 $ \textbf{and} $ g \leqslant n/2$}	
					\State \Return $A[i]$
				\EndIf
				\EndFor
			\EndFunction
		\end{algorithmic}
	\end{algorithm}
	

	
	Посмотрим еще на один способ:
	
	\begin{algorithm}[H]
		\caption{Примитивный алгоритм поиска медианы}
		\begin{algorithmic}[1]
			\Require Массив $A$
			\Ensure Медиана $m$ массива $A$
			\Function{Median}{$A$}
				\State $n$ := $len(A)$
				\State $B$ := $sorted(A)$
				\State \Return $B[\lfloor\frac{n}{2}\rfloor]$
			\EndFunction
		\end{algorithmic}
	\end{algorithm}
	
	На первый взгляд это сложный подход, так как мы должны отсортировать массив и пока не знаем, как это сделать.
		
	Итак, у нас есть как минимум два способа найти медиану. Возникает абсолютно естественное желание как-нибудь выяснить, какой лучше. Оказывается, в программировании можно провести сразу несколько таких оценок по разным критериям. Два главных ресурса, которые потребляют алгоритмы, это процессорное время и память вычислительного устройства.
	
	\begin{definition}
		Время (измеренное в некой абстрактной единице), необходимое алгоритму для завершения своей работы, называется \textit{временем работы алгоритма} и обозначается как $T(n)$, где $n$ - длина входных данных.
	\end{definition}
	
	Время работы можно считать в разных единицах, например в \textit{секундах}, если реализация алгоритма и исполнитель фиксированы, или в \textit{элементарных операциях}, если речь идет про машину Тьюринга.
	
	Различают несколько оценок времени работы:
	\begin{enumerate}
		\item \textit{Худший случай} - максимально возможное $T(n)$ на входе длины $n$. Чаще всего используется на практике, так как дает верхнюю оценку времени работы алгоритма.
		\item \textit{Средний случай} - математическое ожидание $T(n)$ на входе длины $n$. Используется на практике реже, чем худший случай, в силу частой неопределенности вероятностного пространства для вычисления матожидания.
		\item \textit{Лучший случай} - минимально возможное $T(n)$ на входе длины $n$. На практике не используется, так как к любому сколько угодно неэффективному алгоритму можно приписать проверку на оптимальность входных данных и выдать ответ быстрее, чем средний или худший случай. Например, в задаче про поиск медианы можно проверять, отсортирован ли массив, и, если он не отсортирован, честно запускать поиск.
	\end{enumerate}
	
	Для всего зоопарка алгоритмов существует инструмент их анализа - \textit{асимптотический анализ}. Это методология, в которой время работы и занимаемая память алгоритма ставятся в соответствие классу функций.
	
	Для начала дадим несколько определений.
	\begin{definition}
		О-большим от $g(n)$ называют такое множество функций, которое удовлетворяет условию
		\[
		\underline{\underline{O}}(g(n)) = \{f(n) \ | \ \exists c_2>0, n_0>0 \ \forall n\geqslant n_0: 0 \leqslant f(n) \leqslant c_2 g(n)\}
		\]
		Иными словами, запись $f(n) \in O(g(n)) $ означает, что $f(n)$ растет не быстрее, чем $g(n)$.
	\end{definition}
	\begin{definition}
		о-малым от $g(n)$  называют такое множество функций, которое удовлетворяет условию
		\[
		\overline{\overline{o}}(g(n)) = \{f(n)|\forall c_2>0\exists n_0>0:\forall n \geqslant  n_0: 0 \leqslant f(n) < c_2 g(n)\}
		\]
	\end{definition}
	
	\begin{definition}
		$\Omega$-большим от $g(n)$ называют такое множество функций, которое удовлетворяет условию
		\[
		f(n) \in \Omega(g(n)) \leftrightarrow g(n) = \underline{\underline{O}}(f(n))
		\]
	\end{definition}
	
	\begin{definition}
		$\omega$-малым от $g(n)$ называют такое множество функций, которое удовлетворяет условию
		\[
		f(n) \in \omega(g(n)) \leftrightarrow g(n) = \overline{\overline{o}}(f(n))
		\]
	\end{definition}
		

	
	\begin{definition}
		$\Theta(g(n))$ называется такое множество функций, которое удовлетворяет условию
		\[\Theta(g(n)) = 
		\{
		f(n) \ | \ 
		\exists c_1, c_2,n_0 > 0 \ \forall n\geqslant n_0: \
		0\leqslant c_1\cdot g(n)\leqslant f(n) \leqslant c_2 \cdot g(n)
		\}
		\]
		Иными словами, $\Theta(g(n))$ растет примерно также, как и $f(n)$.
		%Theta есть пересечение O и \Omega
	\end{definition}

	
	В нашем курсе мы часто будем писать что-то похожее на
	\[
	T(n) = \Theta(f(n))
	\]
	Такая запись с точки зрения математики некорректна, но мы будем понимать знак равенства как
	\[
	T(n) \in \Theta(f(n))
	\]
	
	Например:
	\[
	4n^2+12n+12 = \Theta(n^2)
	\]
	\[
	c_1 = 1, \ c_2 = 16, \	n_0 = 2
	\]
	\[
	\forall n \geqslant n_0: \ 0 \leqslant n^2 \leqslant 4n^2+12n+12 \leqslant 16n^2
	\]
	
	В общем случае верно следующее:
	\begin{lemma}
		Если многочлен $p(n)$ представим в виде
		\[
		p(n) = \sum_{i = 0}^{d}	a_in^i, \ d = \deg(p), \ a_d > 0, 
		\]
		то 
		\[
		p(n) = \Theta(n^d)
		\]
	\end{lemma}
	
	\begin{remark}
		Обычно функцию, описывающую время работы или память, занимаемую алгоритмом, называют \textit{оценкой времени работы или памяти} алгоритма.
	\end{remark}
	
	\begin{remark}
		По соглашению мы рассматриваем \textit{асимптотически неотрицательные} функции, то есть такие, что 
		\[
		\exists n_0 \ \forall n > n_0: \ f(n) \geqslant 0
		\]
	\end{remark}
	
	Теперь поймем, что скрывается за классами функций $\Theta$.
	
	Пусть есть классы $\Theta(n)$, $\Theta(n^2)$, $\Theta(n^3)$. Для некоторых алгоритмов существует \textit{оценка}, принадлежащая одному из этих классов. Помня про константу, можно сказать, что на достаточно большом объеме данных алгоритм с меньшей оценкой будет работать в среднем быстрее. Это ключевая мысль асимптотического анализа. Представить ее можно, построив графики неких линейной, квадратичной и кубической функций.
	
	
	%точки пересечения, n_0
	\begin{center}
		\begin{tikzpicture}
			\begin{axis}[
				axis x line=center,
				axis y line=center,
				xticklabels={,,},
				yticklabels={,,},
				ticks=none,
				xlabel={$length$},
				ylabel={$time$},
				xlabel style={below right},
				ylabel style={above left},
				xmin=-0.5,
				xmax=5.5,
				ymin=-0.5,
				ymax=5.5
			]
			\addplot[smooth, name path global=Linear, domain=0:5] {x + 0.5};
			\addplot[smooth, name path global=Parabola, domain=0:5] {0.5 * x^2 + 0.5};
			\addplot[smooth, name path global=Cubic, domain=0:5] {x^3 + 0.5};	
			\end{axis}
		\end{tikzpicture}
	\end{center}
	
	Для теоретического анализа сложности алгоритма берутся достаточно большие числа, но нужно понимать, что на практике может оказаться так, что входные данные могут быть меньше, чем $n_0$
	
	
	Теперь получив матаппарат, оценим время работы алгоритмов поиска медианы.
	
	\begin{remark}
		Будем считать, что элементарные арифметические операции, операции присваивания, копирования и тому подобные выполняются за $\Theta(1)$, иначе говоря, время их выполнения константо. 
	\end{remark}
	
	
	\textbf{Первый алгоритм:}
	\begin{enumerate}
		\item Лучший случай: медиана на первом месте. Тогда алгоритм выполнит одну итерацию внешнего цикла, $n$ итераций внутреннего цикла, каждая из которых занимает константное время, и завершит работу. Сложность: $\Theta(1 \cdot n) = \Theta(n)$. Такая сложность считается достаточно хорошей.
		\item Худший случай: медиана на последнем месте. Тогда алгоритм выполнит $n$ итераций внешнего цикла, на каждой итерации произойдет $n$ итераций внутреннего цикла. Сложность: $\Theta(n^2 - n) = \Theta(n^2)$. 
	\end{enumerate}
	
	Доказательство корректности заключается в том, что алгоритм \textit{реализует} определение медианы. В таком случае он корректен, пока нет ошибок на уровне написания кода.
	
	\textbf{Второй алгоритм:}
	
	Второй алгоритм сложнее для оценки, так как мы не знаем, как сортируем массив. Операция взятия элемента выполняется за $\Theta(1)$. Остается сортировка, которую можно выполнить разными способами за разное время. 
	
	Давайте возьмем простой алгоритм сортировки и оценим его сложность.
	
	\begin{algorithm}[H]
		\caption{Сортировка вставками}
		\begin{algorithmic}[1]
			\Require Массив $A$ с заданным на нем порядком <.
			\Ensure Отсортированный по возрастанию массив $A$.
			\Function{insertion\_sort}{$A$}
				\State $n:=len(A)$
				\For{$i := 1 $ to $ (n-1)$}
					\State $k: = A[i]$
					\For{$j := i-1$ to 0}
						\If{$k < A[j]$}
							\State $A[j+1] := A[j]$
						\Else
							\State \textbf{break}
						\EndIf
					\EndFor
					\State $A[j + 1] := k$
				\EndFor
				\State \Return $A$
			\EndFunction
		\end{algorithmic}
	\end{algorithm}
	
	Словами: смотрим каждый $i$ элемент и ищем его место среди первых $i-1$ элементов.
	
	Для начала докажем корректность алгоритма. Для этого будем использовать \textit{инвариант} - свойство математического объекта, которое не меняется после преобразования объекта. 
	\begin{theorem}
		Пусть есть неупорядоченный пронумерованный набор $A$ элементов с заданным на них порядком меньше, и мы исполняем над ним алгоритм. Инвариант: элементы $A[0], \ldots, A[i - 1]$ являются перестановкой исходных элементов в правильном порядке.
	\end{theorem}
	
	\begin{proof}
		%переформулировать
		
		Докажем по индукции. База $i = 1$ верна. Пусть инвариант верен для $i - 1$ шага. Тогда смотрим $k = A[i]$ элемент. 
		
		Возможны 3 случая:
		\begin{enumerate}
			\item $k$ - самый большой среди первых $i$ элементов. Тогда алгоритм пропустит эту итерацию и перейдет к следующему.
			
			\item $k$ - самый маленький среди первых $i$ элементов. Тогда алгоритм передвинет его в начало, пройдя весь цикл.
			
			\item В противном случае, мы начинаем перебирать все элементы среди первых $i$ до тех пор, пока операция сравнения на "меньше" \ не вернет ложь. Это означает, что мы в отсортированном массиве нашли элемент под номером $j$, который меньше либо равен $k$:
			\[
			A[j] \leqslant k \ \leqslant \ A[j+1] 
			\]
			
			Тогда мы сдвигаем элементы с $j+1$ до $i$ на одну позицию вправо, и на $j+1$ место ставим $k$.
			\[
			A[j] \leqslant k = A[j + 1] < A[j + 2]
			\]
			
			Все элементы с 0 по $j$ позицию меньше либо равны $k$, а элементы с $j+2$ по $i$ позицию они больше $k$.
		\end{enumerate}	
	\end{proof}
	
	Из доказательства корректности инварианта прямо следует доказательство корректности алгоритма: когда алгоритм закончит свою работу, $i = n$, а значит инвариант верен для $n$ элементов, а значит массив отсортирован.
	
	Теперь можно оценить время работы сортировки вставками:
	\begin{enumerate}
		\item Лучший случай: массив уже отсортирован. Но тогда внешний цикл совершит $n - 1$ итерацию, на каждой из которых произойдет одно сравнение. Сложность получилась $\Theta(n)$.
		
		\item Худший случай: массив отсортирован в обратном порядке. Тогда на каждой итерации число шагов внутреннего цикла будет уменьшаться на 1. Значит 
		\[
		T(n) = \sum_{i = 1}^{n - 1}i = \Theta(n^2)
		\]
	\end{enumerate}
	
	\pagebreak
\end{document}