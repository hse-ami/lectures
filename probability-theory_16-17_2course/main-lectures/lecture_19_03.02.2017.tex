\section{Лекция от 03.02.2017}
\subsection{Гауссовские векторы. Продолжение}
На предыдущей лекции мы доказали теорему о трёх эквивалентных определениях. Выпишем основные следствия из неё:
\begin{consequence}[Смысл параметров]
	Если случайный вектор \(\xi \sim \mathcal{N}(\mathbf{a}, \mathbf{\Sigma})\), то \(\mathbf{a} = \E{\xi}\), \(\mathbf{\Sigma} = \D{\xi}\).
\end{consequence}
\begin{proof}
	См. доказательство перехода от (2) к (3).
\end{proof}

\begin{consequence}[Корректность определения гауссовского вектора]
	Гауссовский вектор определён корректно, т.е. существует случайный вектор с таким распределением.
\end{consequence}
\begin{proof}
	Возьмём \(\eta\), \(\mathbf{C}\) и \(\mathbf{B}\) из доказательства перехода от (1) к (2) (матрицы свободно восстанавливаются по \(\mathbf{\Sigma}\)). Тогда случайный вектор \(\xi = \mathbf{C^{\top}B}\eta + \mathbf{a}\) обладает нужным распределением.
\end{proof}
\begin{consequence}[Линейные преобразования]
	Если \(\xi = (\xi_1, \dots, \xi_n)^{\top}\)~--- гауссовский вектор, то его любое линейное преобразование тоже является гауссовским вектором.
\end{consequence}
\begin{proof}
	Пусть \(\eta = \mathbf{B}\xi + \mathbf{b}\)~--- некоторое линейное преобразование \(\xi\), где \(\mathbf{B} \in \mathrm{Mat}(k \times n)\) и \(\mathbf{b} = (b_1, \dots, b_k)^{\top}\). Тогда рассмотрим произвольный вектор \(\mathbf{t} \in \R^k\) и его скалярное произведение с \(\eta\):
	\[
		\left\langle \mathbf{t}, \eta\right\rangle = \left\langle \mathbf{t}, \mathbf{B}\xi + \mathbf{b}\right\rangle = \left\langle \mathbf{B^{\top}t}, \xi\right\rangle + \left\langle \mathbf{t}, \mathbf{b}\right\rangle
	\]
	Первое слагаемое есть нормальная случайная величина (по теореме о трёх определениях), а второе~--- просто константа. Тогда \(\left\langle \mathbf{t}, \eta\right\rangle\)~--- это нормальная случайная величина. Следовательно, \(\eta\)~--- это гауссовский вектор.
\end{proof}

Теперь докажем одну простую, но достаточно полезную лемму:
\begin{lemma}[Критерий независимости компонент]
	Пусть \(\xi = (\xi_1, \dots, \xi_n)\)~--- это гауссовский вектор. Тогда его 
	компоненты независимы тогда и только тогда, когда они некоррелированны.
\end{lemma}
\begin{proof}
	Как известно, если случайные величины независимы, то они некоррелированны. 
	Теперь докажем, что в данном случае верно и обратное.
	
	Пусть \(\cov(\xi_i, \xi_j) = 0\) при \(i \neq j\). Тогда матрица ковариаций \(\mathbf{\Sigma}\) диагональна. Теперь рассмотрим характеристическую функцию \(\phi_{\xi}(\mathbf{t})\):
	\begin{align*}
		\phi_{\xi}(\mathbf{t}) &= \exp\left\{i\left\langle \mathbf{a}, \mathbf{t}\right\rangle - \frac{1}{2}\left\langle \mathbf{\Sigma t}, \mathbf{t}\right\rangle\right\} = \exp\left\{i\sum_{i = 1}^{n}a_i t_i - \frac{1}{2}\sum_{i = 1}^{n}\cov(\xi_i, \xi_i)t_i^2\right\} \\
		&= \exp\left\{\sum_{i = 1}^{n}\left(ia_i t_i - \frac{1}{2}\D{\xi_i}t_i^2\right)\right\} = \prod_{i = 1}^{n} \exp\left\{ia_i t_i - \frac{1}{2}\D{\xi_i}t_i^2\right\} = \prod_{i = 1}^{n} \phi_{\xi_i}(t_i)
	\end{align*}
	
	Отсюда по критерию независимости через характеристические функции получаем, что компоненты \(\xi\) независимы в совокупности.
\end{proof}

Порой хочется сказать, что любой вектор, состоящий из нормальных случайных величин, является гауссовским. Но, к сожалению, это не так.
\begin{example}
	Пусть \(\xi \sim \mathcal{N}(0, 1)\), а \(\eta\) равновероятно принимает значения из \(\{1, -1\}\). Введём случайную величину \(\delta = \xi\eta\). Легко проверить, что \(\delta \sim \mathcal{N}(0, 1)\) и \(\cov(\xi, \delta) = 0\):
	\[
		\cov(\xi, \delta) = \E{\xi^2\eta} - \E{\xi}\E{\xi\eta} = 0.
	\]
	
	Однако вектор \((\xi, \delta)\) не является гауссовским. (Почему?)
\end{example}

Как мы помним, у нормальных случайных величин была плотность. Возникает вопрос: а есть ли она у гауссовских векторов? Не всегда.
\begin{theorem}[о плотности гауссовских векторов]
	Пусть \(\xi \sim \mathcal{N}(\mathbf{a}, \mathbf{\Sigma})\)~--- \(n\)-мерный гауссовский вектор. Тогда, если \(\mathbf{\Sigma}\) положительно определена, то существует плотность \(p_{\xi}(\mathbf{t})\) и она равна
	\[
		p_{\xi}(\mathbf{t}) = 
		\frac{1}{(2\pi)^{n/2}\sqrt{\det{\mathbf{\Sigma}}}} 
		\exp\left\{-\frac{1}{2}\left\langle\mathbf{\Sigma}^{-1}(\mathbf{t}
		 - \mathbf{a}), (\mathbf{t} - \mathbf{a})\right\rangle\right\}
	\]
\end{theorem}
\begin{proof}
	Пусть \(\mathbf{D} = \sqrt{\mathbf{\Sigma}}\), то есть матрица \(\mathbf{D}\) такова, что \(\mathbf{D}^2 = \mathbf{\Sigma}\). Если \(\Sigma\) положительно определённая, то \(\mathbf{D}\) единственна. Тогда \(\mathbf{D}\) тоже положительно определённая и симметрическая.
	
	Рассмотрим \(\eta = \mathbf{D}^{-1}(\xi - \mathbf{a})\). Тогда \(\eta\)~--- тоже гауссовский вектор (как линейное преобразование \(\xi\)) и \(\eta \sim \mathcal{N}(0, \mathbf{D}^{-1}\mathbf{\Sigma}\mathbf{D}^{-1}) = \mathcal{N}(0, \mathbf{E}_n)\). Следовательно, \(\eta = (\eta_1, \dots, \eta_n)\), где \(\eta_i\) независимы в совокупности и имеют стандартное нормальное распределение. Тогда
	\[
		p_{\eta}(\mathbf{t}) = \left(\frac{1}{\sqrt{2\pi}}\right)^{n}\exp\left\{-\frac{1}{2}\sum_{i = 1}^{n}t_k^2\right\} = \frac{1}{(2\pi)^{n/2}}e^{-\frac{1}{2}\left\langle\mathbf{t}, \mathbf{t}\right\rangle}
	\]
	
	Теперь рассмотрим вероятностную меру \(\xi\). Возьмём произвольное \(B \in \B(\R^n)\)
	\begin{align*}
		\Pr{\xi \in B} = \Pr{\eta \in \mathbf{D}^{-1}(B - \mathbf{a})} = \int\limits_{\mathbf{D}^{-1}(B - \mathbf{a})} p_{\eta}(\mathbf{x})\diff \mathbf{x}
	\end{align*}
	
	Сделаем замену
	\[
		\mathbf{y} = \mathbf{Dx} + a \iff \diff \mathbf{y} = \det{\mathbf{D}}\diff\mathbf{x}
	\]
	
	Тогда
	\[
		\Pr{\xi \in B} = \int\limits_{B} \frac{1}{(2\pi)^{n/2}\det{\mathbf{D}}} 
		\exp\left\{-\frac{1}{2}\left\langle\mathbf{D}^{-1}(\mathbf{y} - 
		\mathbf{a}), \mathbf{D}^{-1}(\mathbf{y} - 
		\mathbf{a})\right\rangle\right\}\diff\mathbf{y}
	\]
	
	А это в свою очередь равно
	\[
		\int\limits_{B} \frac{1}{(2\pi)^{n/2}\sqrt{\det{\mathbf{\Sigma}}}}
		\exp\left\{-\frac{1}{2}\left\langle\mathbf{\Sigma}^{-1}(\mathbf{y} - 
		\mathbf{a}), (\mathbf{y} - \mathbf{a})\right\rangle 
		\right\}\diff\mathbf{y}
	\]
	
	Дифференцируя, получаем желаемое.
\end{proof}

\subsection{Многомерная ЦПТ. Пример применения}
Центральная предельная теорема верна и для случайных векторов:
\begin{theorem}[многомерная ЦПТ]
	Пусть \(\{\xi_n \mid n \in \N\}\)~--- последовательность независимых и одинаково распределённых случайных векторов из \(\R^{m}\). Далее, пусть \(\mathbf{a} = \E{\xi_1}\) и \(\mathbf{\Sigma} = \D{\xi_1}\) конечны. Введём обозначение \(S_n = \xi_1 + \dots + \xi_n\). Тогда
	\[
		\sqrt{n}\left(\frac{S_n}{n} - \mathbf{a}\right) \dto \mathcal{N}(0, \mathbf{\Sigma})
	\]
\end{theorem}
\begin{proof}
	Доказательство идейно аналогично одномерному случаю: воспользуемся теоремой о непрерывности для характеристических функций и покажем, что последовательность характеристических функций сходится именно туда, куда нужно.
\end{proof}

Рассмотрим какой-нибудь пример применения многомерной ЦПТ.
\begin{problem}
	Пусть выполнены условия многомерной ЦПТ и \(h : \R^{m} \mapsto \R\)~--- некоторая дифференцируемая в точке \(\mathbf{a}\) функция. Докажите, что
	\[
		\sqrt{n}\left(h\left(\frac{S_n}{n}\right) - h(\mathbf{a})\right) \dto \left\langle h'(\mathbf{a}), \mathcal{N}(0, \mathbf{\Sigma})\right\rangle
	\]
	где \(h'(\mathbf{a})\)~--- вектор частных производных \(h\) в точке \(\mathbf{a}\).
\end{problem}
\begin{proof}
	Рассмотрим разложение \(h\) рядом с точкой \(\mathbf{a}\):
	\[
		h(\mathbf{x} + \mathbf{a}) = h(\mathbf{a}) + \left\langle h'(\mathbf{a}), \mathbf{x}\right\rangle + \psi(\mathbf{x})
	\]
	где \(\frac{\psi(\mathbf{x})}{\|\mathbf{x}\|} \to 0\) при \(\mathbf{x} \to 
	\mathbf{0}\). Подставим \(\mathbf{x} = S_n/n - \mathbf{a}\) и помножим обе 
	части уравнения на \(\sqrt{n}\). Это легально, так как по ЗБЧ \(S_n/n \asto 
	\mathbf{a}\). Тогда
	\[
		\sqrt{n}h\left(\frac{S_n}{n}\right) = \sqrt{n}h(\mathbf{a}) + \left\langle h'(\mathbf{a}), \sqrt{n}\left(\frac{S_n}{n} - \mathbf{a}\right)\right\rangle + \sqrt{n}\psi\left(\frac{S_n}{n} - \mathbf{a}\right)
	\]
	
	По многомерной ЦПТ
	\[
		\sqrt{n}\left(\frac{S_n}{n} - \mathbf{a}\right) \dto \mathcal{N}(0, \mathbf{\Sigma})
	\]
	
	Рассмотрим последний член. Преобразуем его:
	\[
		\sqrt{n}\psi\left(\frac{S_n}{n} - \mathbf{a}\right) = \frac{\psi\left(\frac{S_n}{n} - \mathbf{a}\right)}{\left\|\frac{S_n}{n} - \mathbf{a}\right\|}\left\|\sqrt{n}\left(\frac{S_n}{n} - \mathbf{a}\right)\right\|
	\]
	
	Как мы сказали ранее, второй член произведения сходится к \(\mathcal{N}(0, \mathbf{\Sigma})\) по распределению. Однако первый сходится к нулю почти наверное по теореме о наследовании сходимости. Тогда по лемме Слуцкого \(\sqrt{n}\psi\left(\frac{S_n}{n} - \mathbf{a}\right) \dto 0\).
	
	Следовательно,
	\[
		\sqrt{n}\left(h\left(\frac{S_n}{n}\right) - h(\mathbf{a})\right) \dto \left\langle h'(\mathbf{a}), \mathcal{N}(0, \mathbf{\Sigma})\right\rangle = \mathcal{N}(0, (h'(\mathbf{a}))^{\top}\mathbf{\Sigma}h'(\mathbf{a})).\qedhere
	\]
\end{proof}

\subsection{Условное математическое ожидание}
\epigraph{Будет больно, но мы попробуем сделать это немного помягче.}{Д.А. Шабанов}
Как мы все помним, в теории вероятностей есть понятие условной вероятности. Однако можно поставить вопрос ребром: пусть \(\xi\) и \(\eta\)~--- две случайные величины. Что такое \(\E{\xi \given \eta = y}\) и \(\E{\xi \given \eta}\)?

Давайте для начала посмотрим на самый простой случай~--- простые случайные величины. Как известно, для них
\[
	\E{\xi} = \sum_{i = 1}^{n} x_i\Pr{\xi = x_i}
\]

Было бы разумно ввести \(\E{\xi \given \eta = y}\) следующим образом:
\[
	\E{\xi \given \eta = y} = \sum_{i = 1}^{n} x_i\Pr{\xi = x_i \given \eta = y}
\]

Так и введём. Теперь посмотрим, что мы из этого имеем. Домножим на \(\Pr{\eta = y}\). Тогда
\begin{align*}
	\sum_{i = 1}^{n} x_i\Pr{\xi = x_i, \eta = y} = \E{\sum_{i = 1}^{n} x_i\mathbf{1}\{\xi = x_i\}\mathbf{1}\{\eta = y\}} = \E{\xi\mathbf{1}\{\eta = y\}}
\end{align*}

Однако, если положить \(\E{\xi \given \eta = y} = \phi(y)\), то
\[
	\phi(y)\Pr{\eta = y} = \E{\phi(y)\mathbf{1}\{\eta = y\}} = \begin{cases}
	\E{\xi\mathbf{1}\{\eta = y\}} \\
	\E{\phi(\eta)\mathbf{1}\{\eta = y\}}
	\end{cases}
\]

Как видно, выполнено следующее свойство: \(\E{\xi\mathbf{1}\{\eta = y\}} = \E{\E{\xi \given \eta}\mathbf{1}\{\eta = y\}}\). Теперь можно ввести определение для общего случая:

\begin{definition}
	Пусть \(\xi\)~--- это случайная величина, а \(\eta\)~--- это случайный вектор из \(\R^{m}\). Тогда \emph{условным математическим ожиданием} \(\xi\) относительно \(\eta\) называется случайная величина \(\E{\xi \given \eta}\), удовлетворяющая двум условиям:
	\begin{enumerate}
		\item \(\E{\xi \given \eta} = \phi(\eta)\), где \(\phi : \R^m \mapsto \R\)~--- некоторая борелевская функция (свойство измеримости).
		\item Для любого \(B \in \B(\R^m)\) \(\E{\xi\mathbf{1}_{B}(\eta)} = \E{\E{\xi \given \eta}\mathbf{1}_{B}(\eta)}\) (интегральное свойство)
	\end{enumerate}
\end{definition}

\begin{theorem}
	Если \(\E{\xi} < \infty\), то \(\E{\xi \given \eta}\) существует и единственно с точностью до равенства почти наверное.
\end{theorem}
\begin{proof}
	Как обычно, нам не хватает знаний в теории меры для того, чтобы доказать эту теорему.
\end{proof}

Неформально говоря, \(\E{\xi \given \eta}\)~--- это усреднение значений \(\xi\) по значениям \(\eta\).