\section{Лекция от 06.12.2016}

\subsection{Полезные формулы}
Пусть есть некоторый случайный вектор \(\xi = (\xi_1, \xi_2, \dots, \xi_n)\) с плотностью распределения \(p_{\xi}(x)\). Докажем следующие три утверждения:
\begin{theorem}
	Для любого \(B \in \B(\R^n)\) выполнено, что
	\[
	\Pr{\xi \in B} = \idotsint\limits_{B} p_{\xi}(\mathbf{x}) \diff\mathbf{x}.
	\]
\end{theorem}
\begin{proof}
	Введём функцию \(f(\mathbf{x}) = \mathbf{I}_{B}(\mathbf{x})\). Тогда
	\[
	\Pr{\xi \in B} =\E{f(\xi)} = \idotsint\limits_{\R^n} \mathbf{I}_{B}(\mathbf{x})p_{\xi}(\mathbf{x}) \diff\mathbf{x} = \idotsint\limits_{B} p_{\xi}(\mathbf{x}) \diff\mathbf{x}.\qedhere
	\]
\end{proof}

\begin{theorem}
	Для любой борелевской функции \(f : \R \to \R\) и любого \(i \in \{1, 2, \dots, n\}\) выполнено
	\[
	\E{f(\xi_i)} = \idotsint\limits_{\R^n} f(x_i)p_{\xi}(\mathbf{x}) \diff\mathbf{x}.
	\]
\end{theorem}
\begin{proof}
	Введём функцию \(g(\mathbf{x}) = f(x_i)\). Тогда
	\[
	\E{f(\xi_i)} = \E{g(\xi)} = \idotsint\limits_{\R^n} g(\mathbf{x})p_{\xi}(\mathbf{x}) \diff\mathbf{x} = \idotsint\limits_{\R^n} f(x_i)p_{\xi}(\mathbf{x}) \diff\mathbf{x}.\qedhere
	\]
\end{proof}
\begin{theorem}
	Для любой борелевской функции \(f : \R^n \to \R^n\) выполнено, что
	\[
	\Pr{f(\xi) \in B} = \idotsint\limits_{\mathbf{x}:\,f(\mathbf{x}) \in B} p_{\xi}(\mathbf{x}) \diff\mathbf{x}.
	\]
\end{theorem}
\begin{proof}
	Так как функция \(f\) борелевская, то для любого \(B \in \B(\R^n)\) \(f^{-1}(B)\) лежит в \(\B(\R^n)\). Тогда, пользуясь рассуждениями, аналогичными теоремам выше, получаем, что
	\[
	\Pr{f(\xi) \in B} = \E{\mathbf{I}_{f^{-1}(B)}(\xi)} =  \idotsint\limits_{f^{-1}(B)} p_{\xi}(\mathbf{x}) \diff\mathbf{x} = \idotsint\limits_{\mathbf{x}:\,f(\mathbf{x}) \in B} p_{\xi}(\mathbf{x}) \diff\mathbf{x}.\qedhere
	\]
\end{proof}

\subsection{Формула замены переменных в кратном интеграле}
Допустим, что у нас есть случайная величина \(\xi\). Рассмотрим следующую функцию: \(f(\xi) = (f_1(\xi), f_2(\xi), \dots, f_n(\xi))\). Как посчитать плотность полученного вектора?

Для ответа на этот вопрос нужно ввести формулу замены переменной в \(n\)-мерном интеграле. Сделаем это.
\begin{definition}
	\emph{Матрицей Якоби} \(\mathbf{J}_f\) для функции \(f : \R^n \to \R^m\) матрица частных производных
	\[
	\mathbf{J}_f = \mathbf{J}(f_1, \dots, f_m) =
	\begin{bmatrix}
	\dfrac{\strut\partial f_1}{\strut\partial x_1} & \dfrac{\strut\partial f_1}{\strut\partial x_2} & \ldots & \dfrac{\strut\partial f_1}{\strut\partial x_n} \\
	\dfrac{\strut\partial f_2}{\strut\partial x_1} & \dfrac{\strut\partial f_2}{\strut\partial x_2} & \ldots & \dfrac{\strut\partial f_2}{\strut\partial x_n} \\
	\vdots & \vdots & \ddots & \vdots \\[0.5em]
	\dfrac{\strut\partial f_m}{\strut\partial x_1} & \dfrac{\strut\partial f_m}{\strut\partial x_2} & \ldots & \dfrac{\strut\partial f_m}{\strut\partial x_n} \\
	\end{bmatrix}_{m \times n}
	\]
	
	\emph{Якобианом} называют называют определитель матрицы Якоби (если он существует).
\end{definition}
\begin{theorem}[Замена переменной]
	Пусть задано биективное отображение \(\psi : \R^n \to \R^n\), переводящее область \(D'\) в \(D\) по следующему правилу:
	\[ \mathbf{x} = \psi(\mathbf{y}) \iff
	\begin{cases}
	x_1 = \psi_1(y_1, \dots, y_n) \\
	x_2 = \psi_2(y_1, \dots, y_n) \\
	\dots \\
	x_n = \psi_n(y_1, \dots, y_n)
	\end{cases}
	\]
	где \(x_1, \dots, x_n\)~--- ``старые'' координаты, а \(y_1, \dots, y_n\)~--- ``новые'' координаты. Далее, предположим, что все функции, задающее отображение, гладкие (имеют непрерывные производные первого порядка) на \(D'\), а так же \(0 < |\det \mathbf{J}(\psi_1, \dots, \psi_n)| < \infty\). Тогда, если существует интеграл
	\[
	\idotsint\limits_{D} f(\mathbf{x})\diff\mathbf{x} = \idotsint\limits_{D} f(x_1, \dots, x_n)\diff x_1 \dots \diff x_n,
	\]
	то верна следующая формула
	\[
	\idotsint\limits_{D} f(\mathbf{x})\diff\mathbf{x} = \idotsint\limits_{D'} f(\psi(\mathbf{y}))|\det \mathbf{J}(\psi_1, \dots, \psi_n)|\diff\mathbf{y}.
	\]
\end{theorem}
 
Заметим, что \(f^{-1}(f(\mathbf{x})) = \mathbf{x}\), если функция \(f\) обратима. Тогда сделаем замену \(\mathbf{x} = f^{-1}(\mathbf{t})\). Тогда по формуле замены переменных
\[
\Pr{f(\xi) \in B} = \idotsint\limits_{\mathbf{x}:\,f(\mathbf{x}) \in B} p_{\xi}(\mathbf{x}) \diff\mathbf{t} = \idotsint\limits_{B} p_{\xi}(f^{-1}(\mathbf{t}))|\det \mathbf{J}(f^{-1}_1, \dots, f^{-1}_n)| \diff\mathbf{t}.
\]

Тогда \(p_{\xi}(f^{-1}(\mathbf{t}))|\det \mathbf{J}(f^{-1}_1, \dots, f^{-1}_n)|\) будет плотностью функции \(f(\xi)\) в точке \((t_1, \dots, t_n)\).

Докажем следующую формулу:
\begin{theorem}[Формула свёртки]
	Пусть \(\xi\) и \(\eta\)~--- независимые случайные величины с плотностями \(p_{\xi}\) и \(p_{\eta}\) соответственно. Тогда
	\[
	p_{\xi + \eta}(x) = \int\limits_{-\infty}^{+\infty} p_{\xi}(y)p_{\eta}(x - y)\diff y = \int\limits_{-\infty}^{+\infty} p_{\xi}(x - y)p_{\eta}(y)\diff y.
	\]
\end{theorem}
\begin{proof}
	Распишем функцию распределения суммы, пользуясь независимостью случайных величин \(\xi\) и \(\eta\): 
	\[
	\Pr{\xi + \eta \leq z} = \iint\limits_{(x, y):\,x + y \leq z} p_{\xi}(x)p_{\eta}(y)\diff x \diff y = \int\limits_{-\infty}^{z} p_{\xi + \eta}(y)\diff y.
	\]
	
	Сделаем замену переменных:
	\[
	\begin{cases}
	u = y \\
	v = x + y
	\end{cases}
	\iff
	\begin{cases}
	y = u \\
	x = v - u
	\end{cases}
	\]
	
	Получаем, что 
	\[
	\det \mathbf{J}(x, y) = 
	\begin{vmatrix}
	\dfrac{\strut\partial x}{\strut\partial u} & \dfrac{\strut\partial x}{\strut\partial v} \\
	\dfrac{\strut\partial y}{\strut\partial u} & \dfrac{\strut\partial y}{\strut\partial v}
	\end{vmatrix} =
	\begin{vmatrix}
	-1 & 1 \\
	1 & 0
	\end{vmatrix}
	= 1.
	\]
	
	Тогда
	\[
	\iint\limits_{(x, y):\,x + y \leq z} p_{\xi}(x)p_{\eta}(y)\diff x \diff y = \iint\limits_{(u, v):\,u \leq z} p_{\xi}(v)p_{\eta}(v - u)\diff u \diff v.
	\]
	
	Далее, по теореме Фубини
	\[
	\iint\limits_{(u, v):\,u \leq z} p_{\xi}(v)p_{\eta}(v - u)\diff u \diff v = \int\limits_{-\infty}^{z}\int\limits_{-\infty}^{+\infty} p_{\xi}(v)p_{\eta}(v - u)\diff v \diff u.
	\]
	
	Отсюда (по теореме Радона-Никодима) получаем, что почти везде выполнено, что
	\[
	p_{\xi + \eta}(x) = \int\limits_{-\infty}^{+\infty} p_{\xi}(y)p_{\eta}(x - y)\diff y.\qedhere
	\]
\end{proof}

\subsection{Дисперсия и ковариация.}
Пусть \((\Omega, \F, \Pr)\) --- вероятностное пространство и \(\xi\)~--- случайная величина, для которой определено математическое ожидание \(\E{\xi}\).
\begin{definition}
	\emph{Дисперсией} случайной величины \(\xi\) называется величина \[\D{\xi}=\E{(\xi - \E{\xi})^2}.\]
\end{definition}
\begin{definition}
	Пусть \((\xi, \eta)\) --- пара случайных величин. Их \emph{ковариацией} называется \[\cov(\xi, \eta) = \E{\left(\xi - \E{\xi}\right)\left(\eta - \E{\eta}\right)}.\]
\end{definition}
\begin{definition}
	Случайные величины \(\xi,\ \eta \) называются \emph{некоррелированными}, если их ковариация равна нулю: \(\cov(\xi, \eta) = 0\).
\end{definition}
\begin{definition}
	Если \(\D{\xi} > 0\), \(\D{\eta} > 0\), то величина \[\rho(\xi, \eta) = \frac{\cov(\xi, \eta)}{\sqrt{\D{\xi}\D{\eta}}}\] называется \emph{коэффициентом корреляции} случайных величин $ \xi $ и $ \eta $.
\end{definition}

\begin{lemma}[Свойства дисперсии и ковариации]
	Пусть \(\xi\), \(\eta\), \(\chi\)~--- некоторые случайные величины. Тогда верны следующие утверждения:
	\begin{enumerate}
		\item Ковариация билинейна: \(\cov(\xi, a\eta + b\chi) = a\cov(\xi, \eta) + b\cov(\xi, \chi)\);
		\item \(\D{\xi} = \cov(\xi, \xi)\);
		\item Для любого \(c \in \R\) верно, что \(\D{c\xi} = c^2\D{\xi}\) и \(\D{\xi + c} = \D{\xi}\);
		\item Связь с матожиданием: \(\D{\xi} = \E{\xi^2} - \left(\E{\xi}\right)^2\), \(\cov(\xi, \eta) = \E{\xi\eta} - \E{\xi}\E{\eta}\);
		\item Неравенство Коши-Буняковского \[\E{\xi\eta} \leq \sqrt{\E{\xi^2}\E{\eta^2}}.\]
		\item \(\left|\rho(\xi, \eta)\right| \leq 1\) и принимает значения \(\pm 1\) тогда и только тогда, когда \(\xi\) и \(\eta\) являются линейно зависимыми почти наверное.
	\end{enumerate}
\end{lemma}
\begin{proof}
	Первые четыре свойства доказываются точно так же, как и в дискретном случае, так что повторно доказывать мы их не будем. Докажем последние два:
	\begin{enumerate}
		\setcounter{enumi}{4}
		\item Посмотрим на следущее матожидание \(\E{(\xi + \lambda\eta)^2}. \) Заметим, что оно неотрицательно как матожидание от неотрицательной случайной величины. Распишем его, используя свойство линейности: \[0 \leq \E{(\xi + \lambda\eta)^2} = \E{\xi^2} + 2\lambda\E{\xi\eta} + \lambda^2\E{\eta^2}.\] Теперь рассморим его с точки зрения многочлена от \(\lambda\). Зная, что он неотрицателен, можно сказать, что дискриминант не превосходит нуля: \[ \frac{D}{4} = \left(\E{\xi\eta} \right)^2 - \E{\xi^2}\E{\eta^2} \leq 0. \] Откуда и получаем желаемое.
		
		\item \(\left|\rho(\xi, \eta)\right| \leq 1\) получаем, используя неравнство Коши-Буняковского к случайным величинам \(\alpha = \frac{\xi - \E{\xi}}{\sqrt{\D{\xi}}},\ \beta = \frac{\eta - \E{\eta}}{\sqrt{\D{\eta}}}\):
		\begin{align}
			\left|\rho(\xi, \eta)\right| &= \left|\frac{\cov(\xi, \eta)}{\sqrt{\D{\xi}\D{\eta}}}\right| = \left|\E{\left(\frac{\xi - \E{\xi}}{\sqrt{\D{\xi}}}\right)\left(\frac{\eta - \E{\eta}}{\sqrt{\D{\eta}}}\right)}\right| \leq \\
			&\leq \sqrt{\E{\left(\frac{\xi - \E{\xi}}{\sqrt{\D{\xi}}}\right)^2}\E{\left(\frac{\eta - \E{\eta}}{\sqrt{\D{\eta}}}\right)^2}} = 1.
		\end{align}
		Тогда следующие условия эквивалентны:
		\begin{itemize}
			\item \(\left|\rho(\xi, \eta) \right| = 1\);
			\item Дискриминант в неравенстве Коши-Буняковского равен нулю;
			\item Сущействует корень уравнения \(\E{(\alpha + \lambda\beta)^2} = 0\) (обозначим его за \(\lambda_0\));
			\item \(\alpha + \lambda_0\beta = 0\) почти наверное;
			\item \(\exists a,b \in \R : \xi = a\eta + b\) почти наверное. 
		\end{itemize}
	\end{enumerate}
\end{proof}
\begin{consequence}
	Если случайные величины \(\xi_1, \ldots, \xi_n \) являются попарно некоррелированными, то \[\D{\xi_1 + \ldots + \xi_n} = \D{\xi_1} + \ldots + \D{\xi_n}. \]
\end{consequence}
\begin{proof}
	Распишем дисперсию, как сумму ковариаций:
	\[
	\D{\xi_1 + \ldots + \xi_n} = \cov(\xi_1 + \ldots + \xi_n, \xi_1 + \ldots + \xi_n) = \sum\limits_{i, j = 1}^{n}\cov(\xi_i, \xi_j).
	\]
	Так как \(\xi_i\) и \(\xi_j\) некоррелированные при \(i \neq j\), то в полученной сумме ненулевыми могут быть только члены вида \(\cov(\xi_i, \xi_i)\). Тогда
	\[
	\D{\xi_1 + \ldots + \xi_n} = \sum\limits_{i = 1}^{n}\cov(\xi_i, \xi_i) = \D{\xi_1} + \ldots + \D{\xi_n}.\qedhere
	\]
\end{proof}
Следовательно, для независимых случайных величин это также выполняется.
		
\begin{definition}
	Пусть \(\xi = (\xi_1,\ldots,\xi_n)\) --- случайный вектор. Тогда математическим ожиданием случайного вектора \(\xi\) будем называть вектор, составленный из матожиданий компонент: \[\E{\xi} = \left(\E{\xi_1}, \ldots, \E{\xi_n} \right). \] Назовем \emph{матрицей ковариации} вектора матрицу порядка \(n \times n\) 
	\[
	\D{\xi} = \left\lVert\cov(\xi_i, \xi_j)\right\rVert_{i,\,j = 1}^{n}.
	\]
\end{definition}
		
\begin{point}
	Матрица ковариаций любого случайного вектора является \emph{симметрической} и \emph{неотрицательно определенной}.
\end{point}
\begin{proof}
	Исходя из того, что \(\cov(\xi_i, \xi_j) = \cov(\xi_j, \xi_i) \), матрица является симметрической.
	
	Теперь докажем второе свойство. Пусть \(\vec{x} = (x_1,\ldots,x_n) \in \R^n. \) Тогда \[\langle\D{\xi}\vec{x}, \vec{x}\rangle = \sum\limits_{i, j = 1}^{n}\cov(\xi_i, \xi_j)x_ix_j = \sum\limits_{i, j = 1}^{n}\cov(\xi_ix_i, \xi_jx_j).  \] Теперь распишем по линейности:
	\[\sum\limits_{i, j = 1}^{n} \cov(\xi_{i}x_{i}, \xi_{j}x_{j}) = \sum\limits_{j = 1}^{n} \cov\left(\sum\limits_{i = 1}^{n} x_{i}\xi_{i}, x_{j}\xi_{j}\right) = \cov\left(\sum\limits_{i=1}^{n}x_i\xi_i,\sum\limits_{j=1}^{n} x_j\xi_j\right) = \D{\sum\limits_{i=1}^{n}x_i\xi_i} \geq 0. \]
\end{proof}