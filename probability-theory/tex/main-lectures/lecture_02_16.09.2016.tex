\section{Лекция от 16.09.2016}
\subsection{Классические задачи теории вероятностей}
\begin{problem}[Задача о сумасшедшей старушке]
    Есть самолёт на \(n\) мест, в который садятся \(n\) пассажиров. Первой в него заходит безумная старушка, которая садится на случайное место. Каждый следующий пассажир действует по следующему правилу: садится на своё место, если оно свободно, и на случайное, если своё занято. С какой вероятностью
    \begin{enumerate}
        \item последний пассажир сядет на своё место?
        \item предпоследний пассажир сядет на своё место?
        \item и последний, и предпоследний пассажир сядут на свои места?
    \end{enumerate}
\end{problem}
\begin{proof}[Решение]
    У первого пункта есть элементарное решение. Пусть при некоторой рассадке пассажиров последний пассажир сел не на свое место (такую рассадку назовем неудачной). Тогда до прихода последнего пассажира его место было занято пассажиром A (A может быть и сумасшедшей старушкой). В момент прихода пассажира A перед ним стоит выбор~--- какое место занять. В рассматриваемой рассадке он занимает место последнего пассажира. Но с той же вероятностью он мог занять и место старушки, и в дальнейшем все пассажиры, включая последнего, займут свои собственные места. (Конечно, нужно еще пояснить, почему в момент прихода пассажира A старушкино место все еще свободно. Но это действительно так~--- нетрудно проследить, что пока старушкино место свободно, среди всех еще не вошедших пассажиров есть ровно один, чье место уже занято. Как только очередной пассажир занимает старушкино место, все остальные будут садиться только на свои места.) Таким образом, каждой неудачной рассадке соответствует удачная, которая может случиться с той же вероятностью. Это говорит о том, что ровно в половине случаев рассадка будет неудачной.

    Теперь рассмотрим формальное решение для первого пункта. Пусть \(A = \{\)последний сядет на своё место\(\}\). Если\footnote{Это уже вертолёт, скорее. (Д.А. Шабанов)} \(n = 2\), то \(\Pr(A) = \frac{1}{2}\). Теперь рассмотрим случай \(n = 3\). Пусть \(B_{i} = \{\)бабушка есла на \(i\)-е место\(\}\). По формуле полной вероятности \[\Pr(A) = \Pr(A \mid B_{1})\Pr(B_{1}) + \Pr(A \mid B_{2})\Pr(B_{2}) + \Pr(A \mid B_{3})\Pr(B_{3}) = \frac{1}{3}\left(1 + \frac{1}{2} + 0\right) = \frac{1}{2}.\]
    Намечается закономерность. Попробуем сформулировать гипотезу и доказать её:
    \begin{hypothesis}
        Для любого \(n\) вероятность того, что последний пассажир сядет на своё место, равна \(\frac{1}{2}\).
    \end{hypothesis}
    \begin{proof}
        По индукции. База \((n = 2)\) была доказана ранее. Теперь предположим, что утверждение верно для всех \(k < n\). Тогда докажем, что утверждение верно для \(k = n\). Опять же, распишем вероятность по формуле полной вероятности:
        \[\Pr(A) = \sum\limits_{i = 1}^{n} \Pr(A \mid B_i)\Pr(B_i).\]
        Заметим, что \(\Pr(B_i) = \frac{1}{n}\). Теперь посмотрим на значения условных вероятностей:
        \[\Pr(A \mid B_i) = \begin{cases}
        1, & i = 1 \\
        0, & i = n \\
        \frac{1}{2}, & 2 \leq i \leq n - 1
        \end{cases}\]
        Последнее условие на значение условной вероятности следует из шага индукции, так как \(i\)-й пассажир ``становится'' бабушкой. Тогда \(\Pr(A) = \frac{1}{n}\left(1 + \frac{1}{2} + \ldots + \frac{1}{2} + 0\right) = \frac{1}{2}\).
    \end{proof}
    Перейдём ко второму пункту задачи. Пусть \(C = \{\)предпоследний пассажир сел на своё место\(\}\). Тогда рассмотрим \(\Pr(C)\) в случае, когда \(n = 3\). Он сможет сесть на своё место тогда и только тогда, когда бабушка не села на его место. Тогда \(\Pr(C) = \frac{2}{3}\). Попробуем доказать гипотезу, аналогичную случаю с последним пассажиром.
    \begin{hypothesis}
        Для любого \(n\) вероятность того, что предпоследний пассажир сядет на своё место, равна \(\frac{2}{3}\).
    \end{hypothesis}
    \begin{proof}
        Доказательство практически такое же, как и для последнего пассажира, с тем отличием, что значения условной вероятности будут несколько другие:
        \[\Pr(C \mid B_i) = \begin{cases}
        1, & i = 1, n \\
        0, & i = n - 1 \\
        \frac{2}{3}, & 2 \leq i \leq n - 2
        \end{cases}\]
        Тогда \(\Pr(C) = \frac{1}{n}\left(1 + \frac{2}{3} + \ldots + \frac{2}{3} + 0 + 1\right) = \frac{2}{3}\)
    \end{proof}
    Аналогичными рассуждениями можно доказать, что вероятность того, что \(i\)-й с конца пассажир сядет на своё место, равна \(\frac{i}{i + 1}\).

    Теперь приступим к третьему пункту. Докажем следующее утверждение:
    \begin{hypothesis}
        Вероятность этого события (обозначим его за \(D\)) равна \(\frac{1}{3}\).
    \end{hypothesis}
    \begin{proof}
        Опять же, по индукции. Базой служит случай \(n = 3\). В таком случае условие выполнимо тогда и только тогда, когда бабушка сядет на своё место. Тогда вероятность равна \(\frac{1}{3}\) и база верна.

        Перейдём к шагу индукции. Рассуждаем абсолютно аналогично: по формуле полной вероятности \(\Pr(D) = \sum\limits_{i = 1}^{n} \Pr(D \mid B_i)\Pr(B_i)\). Значения условных вероятностей равны
        \[\Pr(D \mid B_i) = \begin{cases}
        1, & i = 1 \\
        0, & i = n - 1, n \\
        \frac{1}{3}, & 2 \leq i \leq n - 2
        \end{cases}\]
        и \(\Pr(D) = \frac{1}{n}\left(1 + \frac{1}{3} + \ldots + \frac{1}{3} + 0 + 0\right) = \frac{1}{3}\).
    \end{proof}
    Заметим, что \(\Pr(D) = \Pr(A)\Pr(C)\). Это \sout{счастливое совпадение} \emph{неспроста}.
\end{proof}

\begin{problem}[Задача об удачливом студенте]
    Студент начал готовиться к экзамену слишком поздно и выучил только \(k\) билетов из \(n\). Студент решил схитрить и выбрать место в очереди такое, чтобы вероятность получить выученный билет была максимальной. Какое место ему выбрать?
\end{problem}
\begin{proof}[Решение]
    Пусть \(A_{s} = \{\)студент вытащил хороший билет, если он встал \(s\)-тым в очереди\(\}\). Докажем следующую гипотезу:
    \begin{hypothesis}
        \(\Pr(A_{s}) = \frac{k}{n}\).
    \end{hypothesis}
    \begin{proof}
        Введём разбиение \(B_{0}, B_{1}, \ldots, B_{k}\) пространства \(\Omega\), где \(B_{i} = \{\)до студента взяли ровно \(i\) хороших билетов\(\}\). Тогда по формуле полной вероятности \[\Pr(A_{s}) = \sum\limits_{i = 0}^{k} \Pr(A_{s} \mid B_{i})\Pr(B_{i}).\]
        Посчитаем \(\Pr(B_{i})\). Для определения количества успешных исходов нужно выбрать \(i\) билетов из \(k\) хороших и \(s - i - 1\) из \(n - k\) плохих. Тогда \[\Pr(B_{i}) = \frac{\binom{k}{i}\binom{n - k}{s - i - 1}}{\binom{n}{s - 1}}.\]
        Теперь надо определить значение условной вероятности \(\Pr(A_{s} \mid B_{i})\). Так как есть \(n - s + 1\) невыбранный билет и \(k - i\) из них изучены, то \(\Pr(A_{s} \mid B_{i}) = \frac{k - i}{n - s + 1}\). Тогда
        \[\Pr(A) = \sum\limits_{i = 0}^{k}\frac{k - i}{n - s + 1}\frac{\binom{k}{i}\binom{n - k}{s - i - 1}}{\binom{n}{s - 1}} = \frac{k}{n}\sum\limits_{i = 0}^{k}\frac{\binom{k - 1}{i}\binom{n - k}{s - i - 1}}{\binom{n - 1}{s - 1}}.\]
        Теперь заметим, что сумма справа равна 1, так как она соответствует разбиению пространства в случае, когда всего задач \(n - 1\). Тогда \(\Pr(A_{s}) = \frac{k}{n}\).
    \end{proof}
    В итоге как ни вставай~--- всё равно никакой разницы не будет.\footnote{Ибо по закону подлости попадётся невыученный билет.}
\end{proof}

\subsection{Теорема Байеса}
\begin{theorem}[Байес]
    Пусть \(\{B_{1}, B_{2}, \ldots, B_{n}\}\) --- разбиение \(\Omega\), причём \(\Pr(B_i) > 0\) для всех \(i \in \{1, 2, \ldots, n\}\). Тогда для любого события \(A\) такого, что \(\Pr(A) > 0\), выполняется
    \[\Pr(B_i \mid A) = \frac{\Pr(A\mid B_i)\Pr(B_i)}{\sum\limits_{j = 1}^{n} \Pr(A\mid B_j)\Pr(B_j)}\]
\end{theorem}
\begin{proof}
    По определению условной вероятности \(\Pr(B_i\mid A) = \frac{\Pr(A\cap B_i)}{\Pr(A)} = \frac{\Pr(A\mid B_i)\Pr(B_i)}{\Pr(A)}\). Тогда, применяя формулу полной вероятности для \(\Pr(A)\), получаем желаемое.
\end{proof}

\subsection{Независимость событий}
\begin{definition}
    События \(A\) и \(B\) на вероятностном пространстве \((\Omega, \Pr)\) называются \emph{независимыми}, если \(\Pr(A \cap B) = \Pr(A)\Pr(B)\). Иногда используется обозначение \(A \independent B\).
\end{definition}
Рассмотрим некоторые примеры независимых событий:
\begin{enumerate}
    \item Задача про сумасшедшую бабушку: События \(A = \{\)последний сядет на своё место\(\}\) и \(B = \{\)предпоследний сядет на своё место\(\}\) независимы, как было доказано ранее.
    
    \item Бросок игральной кости. Пусть \(A = \{\)выпало чётное число очков\(\}\), а \(A = \{\)выпало число очков, кратное 3\(\}\). Докажем, что они независимы. Заметим, что \(A \cap B = \{\)число очков кратно 6\(\}\). Тогда \(\Pr(A \cap B) = \frac{1}{6} = \frac{1}{2} \cdot \frac{1}{3} = \Pr(A)\Pr(B)\).
\end{enumerate}
Данное определение работает только для двух событий. Можно ли как-то его обобщить? Попробуем ввести аналогично:
\begin{definition}
    Cобытия \(A_1, \ldots, A_n\) называются \emph{попарно независимыми}, если для любых \(i\) и \(j\) таких, что \(i \neq j\), \(A_i\) независимо от \(A_j\).
\end{definition}
Однако обобщение обычно вводят по другому:
\begin{definition}
    События \(A_1, a_2, \ldots, A_n\) называются \emph{независимыми по совокупности}, если для любого множества \(\{i_1, i_2, \ldots, i_k\} \subseteq \{1, 2, \ldots, n\}\) выполняется, что \(\Pr(A_{i_1}\cap\ldots\cap A_{i_k}) = \prod\limits_{j = 1}^{k} \Pr(A_{i_j})\).
\end{definition}
\begin{remark}
    Если говорят про независимость событий и не указывают тип, то обычно подразумевают независимость по совокупности.
\end{remark}

Стоит заметить, что попарная независимость~--- гораздо более слабое условие, чем независимость по совокупности. Приведём пример. Пусть есть тетраэдр, грани которого покрашены следующим образом: первая грань покрашена в красный, вторая~--- в синий, третья~--- в зелёный, а четвёртая~--- во все три цвета сразу. Его подбрасывают. Введём три события: \(A_{\text{К}} = \{\)на нижней грани есть красный цвет\(\}\), \(A_{\text{С}} = \{\)на нижней грани есть синий цвет\(\}\) и \(A_{\text{З}} = \{\)на нижней грани есть зелёный цвет\(\}\). Очевидно, что вероятность любого события~--- \(\frac{1}{2}\), а любой пары событий~--- \(\frac{1}{4}\). Однако вероятность объединения всех трёх событий равна \(\frac{1}{4}\), а не \(\frac{1}{8}\). Следовательно, события попарно независимы, но не независимы по совокупности.
\begin{exercise}
    Приведите пример \(n\) событий таких, что любой набор из \(n - 1\) события независим, а все \(n\) событий вместе зависимы.
\end{exercise}

Для независимости выполняются следующие свойства:
\begin{enumerate}
    \item \(A \independent A \iff \Pr(A) = 0\text{ или }\Pr(A) = 1 \iff \text{для любого события } B\ A \independent B\).
    \item \(A \independent B \implies \overline{A} \independent B\).
    \item Если \(A_1, A_2, \ldots, A_n\) независимы в совокупности, то любой набор \(B_1, B_2, \ldots, B_n\) такой, что \(B_i = A_i\) или \(B_i = \overline{A_i}\), тоже независим.
\end{enumerate}

\subsection{Случайные величины в дискретных вероятностных пространствах}
\begin{definition}
    Пусть \((\Omega, \Pr)\)~--- дискретное вероятностное пространство. Тогда любое отображение \(\xi : \Omega \to \R\) называется \emph{случайной величиной}.
\end{definition}
\begin{remark}
    В литературе случайную величину часто сокращают до с.в.
\end{remark}
Рассмотрим некоторые примеры случайных величин:
\begin{enumerate}
    \item Широко распространённым примером случайной величины служит индикатор какого-либо события. Введём определение.
    \begin{definition}
        Пусть \(A \in \Omega\)~--- событие. Тогда \emph{индикатором} события \(A\) называют называется случайная величина, равная \[I_A(\omega) = \begin{cases}
        1, w \in A; \\
        0, w \not\in A;
        \end{cases}\]
    \end{definition}
    
    \item Бросок игральной кости. Тогда случайной величиной будет число выпавших очков.
    
    \item Схема Бернулли: \(\Omega = \{\omega = (\omega_1 \ldots \omega_n),\; \omega_i \in \{0, 1\} \}\). В данном случае вводится случайная величина, равная \(\xi(\omega) = \sum\limits_{i=1}^{n}\omega_i\). Ещё её называют \emph{числом успехов в схеме Бернулли}.
\end{enumerate}
