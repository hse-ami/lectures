\section{Семинар от 30.09.2016}
Как обычно, начнём с разбора домашнего задания.
\begin{problem}
    Пусть \(\xi\)~--- некоторая случайная величина. При каком \(a \in \R\) достигается
    минимальное значение функции \(f(a) = \E[(\xi - a)^2]\)?
\end{problem}
\begin{proof}[Решение]
    Раскроем матожидание по линейности: \(\E[(\xi - a)^2] = \E[\xi^2] - 2a\E[\xi] + a^2\). Теперь добавим и вычтем \((\E[\xi])^2\). Тогда \(f(a) = (a - \E[\xi])^2 + \E[\xi^2] - (\E[\xi])^2 = (a - \E[\xi])^2 + \D[\xi]\). Так как \(\D[\xi]\) не зависит от \(a\), то минимум достигается при \((a - \E[\xi])^2 = 0 \iff a = \E[\xi]\).
\end{proof}

\begin{problem}
    Найдите \(\E[\xi]\), \(\D[\xi]\) и \(\E[3^{\xi}]\), если \(\xi\)~--- это а) пуассоновская случайная величина с параметром \(\lambda > 0\), б) геометрическая случайная величина с параметром \(p \in (0, 1)\).
\end{problem}
\begin{proof}[Решение]
    Начнём с пуассоновской величины. Как известно, множество её значений равно \(\N\), а \(\Pr(\xi = k) = \frac{\lambda^k}{k!}e^{-\lambda}\) для любого \(k \in \N\). Тогда матожидание \(\xi\) равно
    \[\E[\xi] = \sum_{k = 1}^{\infty} k\frac{\lambda^k}{k!}e^{-\lambda} = \frac{\lambda}{e^{\lambda}} \sum_{k = 1}^{\infty} \frac{\lambda^{k - 1}}{(k - 1)!} = \frac{\lambda}{e^{\lambda}}e^{\lambda} = \lambda.\]
    
    Перейдём к подсчёту дисперсии:
    \[\D[\xi] = \E[\xi^2] - (\E[\xi])^2 = \sum_{k = 1}^{\infty} k^2\frac{\lambda^k}{k!}e^{-\lambda} - \lambda^2 = \frac{1}{e^{\lambda}}\sum_{k = 1}^{\infty} \frac{k\lambda^k}{(k - 1)!} - \lambda^2.\]
    
    Поменяем индекс суммирования, уменьшив его на 1. Тогда дисперсия равна
    \[\frac{1}{e^{\lambda}}\sum_{k = 0}^{\infty} \frac{(k + 1)\lambda^{k + 1}}{k!} - \lambda^2 = \frac{\lambda^2}{e^{\lambda}}\sum_{k = 1}^{\infty} \frac{\lambda^{k - 1}}{(k - 1)!} + \frac{\lambda}{e^{\lambda}}\sum_{k = 0}^{\infty} \frac{\lambda^{k}}{k!} - \lambda^2 = \lambda.\]
    
    Теперь перейдём к подсчёту математического ожидания случайной величины \(3^{\xi}\):
    \[\E[3^{\xi}] = \sum_{k = 1}^{\infty} 3^k\frac{\lambda^k}{k!}e^{-\lambda} = e^{-\lambda}\sum_{k = 1}^{\infty}\frac{(3\lambda)^k}{k!} = e^{2\lambda}.\]
    
    Перейдём к геометрическому распределению. Напомню, что множество её значений равно \(\N\), а \(\Pr(\xi = k) = p(1 - p)^{k - 1}\) для любого \(k \in \N\). Тогда матожидание равно
    \[\E[\xi] = \sum_{k = 1}^{\infty} kp(1 - p)^{k - 1} = p\sum_{k = 1}^{\infty} k(1 - p)^{k - 1}.\]
    
    Как посчитать этот ряд? Возьмём производную от функции \(f(x) = \sum\limits_{k = 1}^{\infty} x^{k}, |x| < 1\). Легко увидеть, что она совпадает с этим рядом. Тогда
    \[\sum_{k = 1}^{\infty} kx^{k - 1} = \left(\frac{1}{1 - x}\right)' = \frac{1}{(1 - x)^2}.\]
    
    Отсюда сразу получаем, что \(\E[\xi] = \dfrac{1}{p}\).
    
    Перейдём к дисперсии: \(\D[\xi] = \E[\xi^2] - (\E[\xi])^2 = \E[\xi^2] - \frac{1}{p^2}\). Рассмотрим \(\E[\xi^2]\) отдельно:
    \[\E[\xi^2] = p\sum_{k = 1}^{\infty} k^2(1 - p)^{k - 1}.\]
    
    С таким рядом простой метод не пройдёт. Но можно добавить и вычесть \(\E[\xi]\). Тогда
    \[\E[\xi^2] = p\sum_{k = 1}^{\infty} k(k - 1)(1 - p)^{k - 1} +\E[\xi] = p(1 - p)\sum_{k = 1}^{\infty} k(k - 1)(1 - p)^{k - 2} + \frac{1}{p}.\]
    
    Заметим, что ряд равен \(\left.\left(\dfrac{1}{1 - x}\right)''\right|_{x = 1 - p} = \left.\dfrac{2}{(1 - x)^3}\right|_{x = 1 - p} = \dfrac{2}{p^3}\). Тогда \[\D[\xi] = \dfrac{2(1 - p)}{p^2} + \frac{1}{p} - \frac{1}{p^2} = \frac{1 - p}{p^2}.\]
    
    Перейдём к \(\E[3^{\xi}]\):
    \[\E[3^{\xi}] = 3p\sum_{k = 1}^{\infty} 3^{k - 1}(1 - p)^{k - 1}.\]
    
    В данном случае внутри ряда геомертическая прогрессия с знаменателем \(3(1 - p)\). Для сходимости ряда необходимо, чтобы \(3(1 - p) < 1 \implies p > 2/3\). Если это не так, то \(\E[3^{\xi}] = +\infty\). Иначе же \[\E[3^{\xi}] = \frac{3p}{1 - 3(1 - p)}.\qedhere\]
\end{proof}

\begin{problem}
    На скамейке сидят \(n\) человек. Каждый из них независимо бросает игральную шестигранную кость. Случайная величина \(X\) равна количеству людей, у которых у хотя бы одного соседа выпало то же число, что и у него самого. Найдите \(\E[X]\) и \(\D[X]\).
\end{problem}
\begin{proof}[Решение]
    Введём событие \(A_i = \{\)у \(i\)-го человека на скамейке число совпало хотя бы с одним из соседей\(\}\). Легко понять, что \(X = \sum\limits_{i = 1}^{n} I_{A_i}\). Тогда \(\E[X] = \sum\limits_{i = 1}^{n} \Pr(A_i)\). Чему равно \(\Pr(A_i)\)? Если \(i = 1\) или \(i = n\), то число у человека должно совпасть с числом единственного соседа и вероятность равна \(1/6\). Если же \(1 < i < n\), то покажем, что вероятность равна \(11/36\). Пусть мы зафиксировали число у человека посередине. Тогда либо у человека слева совпадает, а у человека справа нет (что даёт 5 вариантов), либо симметрично (ещё 5), либо у обоих сразу (ещё 1). Всего исходов же 36. Отсюда получаем, что \[\E[X] = \dfrac{11(n - 2)}{36} + \dfrac{1}{3} = \dfrac{11n - 10}{36}.\]
    
    Приступим к подсчёту дисперсии.\footnote{Готовьтесь, будет больно.} Распишем дисперсию через сумму ковариаций:
    \[\D[X] = \cov(X, X) = \sum_{i, j = 1}^{n} \cov(I_{A_i}, I_{A_j}) = \sum_{i, j = 1}^{n} \left(\Pr(A_i \cap A_j) - \Pr(A)\Pr(B)\right).\]
    
    Есть 4 случая:
    \begin{enumerate}
        \item \(i = j\). Тогда \(\cov(I_{A_i}, I_{A_j}) = \frac{11}{6^2} - \frac{11 \cdot 11}{6^4} = \frac{11 \cdot 25}{6^4}\) для \(1 < i < n\) и \(\frac{1}{6} - \frac{1}{36} = \frac{5}{36}\) для \(i = 1\) или \(i = n\).
        \item \(|i - j| = 1\). Тогда подойдут ситуации \(aabb\) и \(baac\) (\(a, b, c\)~--- выпавшие числа). Отсюда получаем, что \(\Pr(A_i \cap A_j) - \Pr(A)\Pr(B) = \frac{1}{6} + \frac{6 \cdot 5}{6^4} - \frac{11^2}{36}\) для \(1 < i,j < n\). Если, например, \(i = 1\), то подойдёт только ситуация \(aac\). Тогда \(\Pr(A_i \cap A_j) - \Pr(A)\Pr(B) = \frac{1}{36} - \frac{11}{6^3}\).
        \item \(|i - j| = 2\). Тогда подходят ситуации \(baaac\) или \(bbcaa\). Тогда \(\Pr(A_i \cap A_j) = \frac{1}{6^3} + \frac{5 \cdot 4}{6^4}\). В случае \(i = 1\) допустимы лишь ситуация \(aaac\) или \(aabb\). Отсюда получаем, что \(\Pr(A_i \cap A_j) = \frac{1}{6^3} + \frac{30}{6^4}\).
        \item \(|i - j| > 2\). Тогда они никак не влияют на друга и \(\cov(I_{A_i}, I_{A_j}) = 0\).
    \end{enumerate}
    
    Просуммировав всё это, получаем значение дисперсии.
\end{proof}

\begin{problem}
    Случайный граф \(G(n, p)\) получается случайным и независимым удалением ребер из полного графа на \(n\) вершинах \(K_n\): любое ребро остается в \(G(n, p)\) независимо от других с с вероятностью \(p\). Пусть \(X_n\)~--- число треугольников в \(G(n, p)\). Вычислите \(\E[X_n]\) и \(\D[X_n]\).
\end{problem}
\begin{proof}
    Заметим, что всего треугольников в графе на \(n\) вершинах можно составить \(\binom{n}{3}\). При этом вероятность того, что какие-то три фиксированных ребра будут в графе, равна \(p^3\). Отсюда сразу получаем, что \(\E[X_n] = \binom{n}{3}p^3\).
    
    Перейдём к подсчёту дисперсии. Введём событие \(A_i = \{i\)-й треугольник вошёл в случайный граф\(\}\) (для этого каким-либо образом пронумеруем все треугольники). Тогда выразим дисперсию через сумму ковариаций индикаторов:
    \[\D[X_n] = \sum_{i, j = 1}^{\binom{n}{3}}\cov(I_{A_i}, I_{A_j}).\]
    Есть три случая:
    \begin{itemize}
        \item \(\bigtriangleup_i\) и \(\bigtriangleup_j\) не имеют общих рёбер. Тогда события \(A_i\) и \(A_j\) независимы и \(\cov(I_{A_i}, I_{A_j}) = 0\).
        \item \(\bigtriangleup_i\) и \(\bigtriangleup_j\) имеют одно общее ребро. Тогда \(\cov(I_{A_i}, I_{A_j}) = \Pr(A_i \cap A_j) - \Pr(A_i)\Pr(A_j) = p^5 - p^6\).
        \item \(\bigtriangleup_i\) и \(\bigtriangleup_j\) имеют два общих ребра. Тогда эти треугольники совпадают и \(\cov(I_{A_i}, I_{A_j}) = \Pr(A_i \cap A_j) - \Pr(A_i)\Pr(A_j) = p^3 - p^6\)
    \end{itemize}
    Теперь посчитаем число вторых и третьих случаев. Очевидно, что выбрать 4 вершины из \(n\) можно \(\binom{n}{4}\) способами. Но при этом эти 4 вершины будут задавать два разных варианта таких треугольников (в зависимости от положения общего ребра). Тогда
    \[\D[X_n] = 2\binom{n}{4}(p^5 - p^6) + \binom{n}{3}(p^3 - p^6).\]
\end{proof}

Порой в некоторых задачах нужно доказать, что вероятность того, что случайная величина будет равна нулю, стремится к 0 или к 1. Для упрощения жизни можно доказать два простых и полезных правила:

\textbf{Метод первого момента:} пусть \(\{x_n\}_{n = 1}^{\infty}\)~--- последовательность случайных величин с значеними в \(\Z_+\). Тогда \[\Pr(x_n > 0) = \Pr(x_n \geq 1) \leq \{\text{по неравенству Маркова}\} \leq \E[x_n].\]

Если \(\E[x_n] \to 0\) при \(n \to \infty\), то \(\Pr(x_n = 0) \to 1\).

\textbf{Метод второго момента:} пусть \(\{x_n\}_{n = 1}^{\infty}\)~--- последовательность случайных величин. Тогда
\[\Pr(x_n = 0) \leq \Pr(x_n \geq 0) = \Pr(|x_n - \E[x_n]| \geq \E[x_n]) \leq \{\textrm{по неравенству Чебышёва}\} \leq \frac{\D[x_n]}{(\E[x_n])^2}.\]

Если \(\D[x_n] = o((\E[x_n])^2)\), то \(\Pr(x_n = 0) \to 0\) при \(n \to \infty\).
\begin{problem}
    Случайный граф \(G(n, p)\) получается случайным и независимым удалением ребер из полного графа на \(n\) вершинах \(K_n\): любое ребро остается в \(G(n, p)\) независимо от других с с вероятностью \(p\). Пусть \(x_n\)~--- число треугольников в \(G(n, p)\). Докажите, что
    \[\Pr(x_n = 0) \to \begin{cases}
    1, &\text{если } np(n) \to 0, \\
    0, &\text{если } np(n) \to \infty.
    \end{cases}\]
\end{problem}
\begin{proof}
    Воспользуемся результатами задачи 4. Тогда \(\E[x_n] = \frac{1}{6}(np)((n - 1)p)((n - 2)p)\). Пусть \(np(n) \to 0\). Тогда \(\E[x_n] \to 0\). Следовательно, по правилу первого момента \(\Pr(x_n = 0) \to 1\).
    
    Теперь предположим, что \(np(n) \to \infty\). Тогда \(\D[x_n] = O(n^4p^6)\), а \((\E[x_n])^2 = O(n^6p^6)\). Тогда \(\D[x_n] = o((\E[x_n])^2)\) и по правилу второго момента \(\Pr(x_n = 0) \to 0\) при \(n \to \infty\).
\end{proof}

\begin{remark}
    Теперь предположим, что \(np(n) \to c, c > 0\). Тогда легко понять, что \(\E[x_n] \to \frac{c^3}{6}\). Вообще, при достаточно больших \(n\) \(x_n\) будет иметь распределение, близкое к пуассоновскому:
    \[\E[x_n^k] \to \E[\xi^k], \xi \sim \mathrm{Pois}\left(\frac{c^3}{6}\right).\]
\end{remark}

Введём несколько различных определений, которые будут полезны в дальнейшем.
\begin{definition}
    \emph{Моментом порядка \(k\)} для случайной величины \(\xi\) называют \(\E[\xi^k]\).
\end{definition}

\begin{definition}
    \emph{Центральным моментом порядка \(k\)} для случайной величины \(\xi\) называют \(\E[(\xi - \E[\xi])^k]\).
\end{definition}

\begin{definition}
    \emph{Факториальным моментом порядка \(k\)} для случайной величины \(\xi\) называют \(\E[\xi(\xi - 1)\ldots(\xi - k + 1)]\).
\end{definition}

\begin{remark}
    Обычный и факториальный моменты первого порядка совпадают с матожиданием, центральный момент первого порядка всегда равен 0, а второй центральный момент совпадает с дисперсией.
\end{remark}

\begin{problem}
    Пусть \(\xi_1\) и \(\xi_2\)~--- независимые случайные величины, причём \(\xi_i \sim \mathrm{Bin}(n_i, p)\). Какое распределение у случайной величины \(\xi_1 + \xi_2\)?
\end{problem}
\begin{proof}[Решение]
    Из определения биномиального распределения получаем, что
    \[\begin{aligned}
    \Pr(\xi_1 = k) =& \binom{n_1}{k}p^{k}(1 - p)^{n_1 - k}, \\
    \Pr(\xi_2 = n - k) =& \binom{n_2}{n - k}p^{n - k}(1 - p)^{n_2 - n + k}.
    \end{aligned}\]
    
    Посчитаем \(\Pr(\xi_1 + \xi_2 = n)\). Представим \(n\) в виде \(k + (n - k)\). Теперь поставим ограничения на \(k\):
    \[
    \begin{cases}
    0 \leq k \leq n_1 \\
    0 \leq n - k \leq n_2
    \end{cases}
    \implies
    \begin{cases}
    0 \leq k \leq n_1 \\
    n - n_2 \leq k \leq n
    \end{cases}
    \]
    
    Тогда
    \[\begin{aligned}
        \Pr(\xi_1 + \xi_2 = n) &= \sum_{k = \max(0, n - n_2)}^{\min(n, n_1)} \Pr(\xi_1 = k, \xi_2 = n - k) \\
        &= \sum_{k = \max(0, n - n_2)}^{\min(n, n_1)} \Pr(\xi_1 = k)\Pr(\xi_2 = n - k) \\
        &= \sum_{k = \max(0, n - n_2)}^{\min(n, n_1)} \binom{n_1}{k}\binom{n_2}{n - k}p^{n}(1 - p)^{n_1 + n_2 - n} \\
        &= p^{n}(1 - p)^{n_1 + n_2 - n}\sum_{k = \max(0, n - n_2)}^{\min(n, n_1)} \binom{n_1}{k}\binom{n_2}{n - k}
    \end{aligned}\]
    Теперь заметим, что \(\sum\limits_{k = \max(0, n - n_2)}^{\min(n, n_1)} \binom{n_1}{k}\binom{n_2}{n - k} = \binom{n_1 + n_2}{n}\). Рассуждения для доказательства точно те же, что и для \(\sum\limits_{k = 0}^{n} \binom{n}{k}^2 = \binom{2n}{n}\). Тогда
    \[\Pr(\xi_1 + \xi_2 = n) = \binom{n_1 + n_2}{n}p^{n}(1 - p)^{n_1 + n_2 - n} \implies \xi_1 + \xi_2 \sim \mathrm{Bin}(n_1 + n_2, p).\qedhere\]
\end{proof}

\begin{remark}
    Предположим, что у нас есть \(n\) случайных величин, подчиняющихся бернуллиевскому распределению с параметром \(p\). Тогда их сумма будет иметь биномиальное распределение с параметрами \(n\) и \(p\). Это ещё раз доказывает, что в случае броска \(n\) монет сумма количества выпавших орлов будет подчиняться биномиальному распределению.
\end{remark}

\begin{problem}
    Пусть \(\xi_1\) и \(\xi_2\)~--- независимые случайные величины, причём \(\xi_i \sim \mathrm{Pois}(\lambda_i)\). Какое распределение у случайной величины \(\xi_1 + \xi_2\)?
\end{problem}
\begin{proof}[Решение]
    По определению пуассоновского распределения
    \[
    \Pr(\xi_1 = k) = \frac{\lambda_1^k}{k!}e^{-\lambda_1},\quad
    \Pr(\xi_2 = n - k) = \frac{\lambda_2^{n - k}}{(n - k)!}e^{-\lambda_2}.
    \]
    Тогда 
    \[\begin{aligned}
    \Pr(\xi_1 + \xi_2 = n) &= \sum_{k = 0}^{n} \Pr(\xi_1 = k, \xi_2 = n - k) = \sum_{k = 0}^{n} \Pr(\xi_1 = k)\Pr(\xi_2 = n - k) \\
    &= \sum_{k = 0}^{n} \frac{\lambda_1^k}{k!}e^{-\lambda_1}\frac{\lambda_2^{n - k}}{(n - k)!}e^{-\lambda_2} = \frac{e^{-(\lambda_1 + \lambda_2)}}{n!} \sum_{k = 0}^{n} \binom{n}{k}\lambda_1^{k}\lambda_2^{n - k} \\
    &= \frac{(\lambda_1 + \lambda_2)^n}{n!}e^{-(\lambda_1 + \lambda_2)}.
    \end{aligned}\]
    Тогда \(\xi_1 + \xi_2 \sim \mathrm{Pois}(\lambda_1 + \lambda_2)\).
\end{proof}

\begin{problem}
    Пусть есть \(n\) случайных независимых величин \(\xi_1, \xi_2, \ldots, \xi_n\), причём \(\xi_i \sim \mathrm{Geom}(p)\). Какое распределение у \(\xi_1 + \xi_2 + \ldots + \xi_n\)?
\end{problem}
\begin{proof}[Решение]
    Напомню, что для геометрического распределения \(\Pr(\xi_i = k_i) = p(1 - p)^{k_i}\). Тогда 
    \[\Pr(\xi_1 + \xi_2 + \ldots + \xi_n = k) = \sum_{k_1 + k_2 + \ldots + k_n = k}p^{n}(1 - p)^{k} = \binom{n + k - 1}{n}p^{n}(1 - p)^{k}.\]
    Данное распределение называется \emph{отрицательным биномиальным}.
\end{proof}