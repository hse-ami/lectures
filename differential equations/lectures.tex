\documentclass[12pt,a4paper]{article}
\usepackage[cp1251]{inputenc}
\usepackage[russian]{babel}


%\documentclass[12pt]{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{latexsym}
\usepackage{srcltx}

\newtheorem{theorem}{Теорема}
\newtheorem{prop}{Предложение}
\newtheorem{lemma}{Лемма}
\newtheorem{cor}{Следствие}
\newtheorem{remark}{Замечание}
\newtheorem{ex}{Пример}
\newtheorem{defi}{Определение}

\newcommand{\co}{{\mathbb C}}
\newcommand{\rd}{{\mathbb R^d}}
\newcommand{\re}{{\mathbb R}}
\newcommand{\ren}{{\re^{n\times n}}}
\newcommand{\real}{{\rm Re\, }}
\newcommand{\imag}{{\rm Im \, }}
\newcommand{\n}{{\mathbb N}}
\newcommand{\T}{{\mathbb T}}
\newcommand{\z}{{\mathbb Z}}
\newcommand{\cA}{{\cal{A}}}
\newcommand{\cJ}{{\cal{J}}}
\newcommand{\cO}{{\cal{O}}}
\newcommand{\cD}{{\cal{D}}}
\newcommand{\cF}{{\cal{F}}}
\newcommand{\cV}{{\cal{V}}}
\newcommand{\cL}{{\cal{L}}}
\newcommand{\cW}{{\cal{W}}}
\newcommand{\cK}{{\cal{K}}}
\newcommand{\cB}{{\cal{B}}}
\newcommand{\cH}{{\cal{H}}}
\newcommand{\cS}{{\cal{S}}}
\newcommand{\cQ}{{\cal{Q}}}
\newcommand{\cP}{{\cal{P}}}
\newcommand{\cM}{{\cal{M}}}
\newcommand{\cN}{{\cal{N}}}
\newcommand{\hr}{{\hat \rho}}
\newcommand{\F}{{\cal{F}}}
\newcommand{\Ima}{{{\rm Im\, }}}
\newcommand{\Rea}{{{\rm Re \, }}}
\newcommand{\bp}{{\mathbf{p}}}
\newcommand{\bb}{{\mathbf{b}}}
\newcommand{\bc}{{\mathbf{c}}}
\newcommand{\be}{{\mathbf{e}}}
\newcommand{\ba}{{\mathbf{a}}}
\newcommand{\conv}{{{\rm Conv}\, }}
\newcommand{\cho}{{\rm ch}\, }

\textwidth=165mm \oddsidemargin=5mm \topmargin=-5mm
\textheight=22cm

\date{}


\begin{document}

\begin{titlepage}

\vspace{2cm}


\begin{center}
{\textsc{В.Ю.Протасов}}
\end{center}
\vspace{1cm}

\begin{center}
\Large{\textsc{Дифференциальные уравнения}}
\end{center}
\vspace{1cm}

\begin{center}
\textbf{курс лекций} \\
\vspace{1cm}

НИУ Высшая Школа Экономики, \\
Факультет компьютерных наук, 2017 г. \\
\end{center}

\bigskip
 \end{titlepage}
\newpage






\begin{center}
 \scshape СОДЕРЖАНИЕ
\end{center}
$$
\begin{array}{llll}
 \mathbf{1.} & {} & \mbox{{Что такое дифференциальные уравнения ?}} & \qquad 2\\
{} & \mathbf{1.1.} & \mbox{\footnotesize{Основные понятия}} & \qquad 2 \\
{} & \mathbf{1.2.} & \mbox{\footnotesize{Примеры}} & \qquad 3\\
{} & {} & {} & {} \\
 \mathbf{2.} & {} & \mbox{{Общая теория дифференциальных уравнений}} & \qquad 6\\
{} & \mathbf{2.1.} & \mbox{\footnotesize{Задача Коши}} & \qquad 6 \\
{} & \mathbf{2.2.} & \mbox{\footnotesize{Существование и единственность решения. Принцип сжимающих отображений }} & \qquad 6 \\
{} & \mathbf{2.3.} & \mbox{\footnotesize{Продолженрие решений}} & \qquad 8 \\
{} & \mathbf{2.4.} & \mbox{\footnotesize{Метод последовательных приближений}} & \qquad 10 \\
{} & \mathbf{2.5.} & \mbox{\footnotesize{Геометрическая интерпретация. Ломаные Эйлера}} & \qquad 10 \\
{} & \mathbf{2.6.} & \mbox{\footnotesize{Доказательство существования решений методом Пеано}} & \qquad 13 \\
{} & \mathbf{2.7.} & \mbox{\footnotesize{Теорема Осгуда о единственности}} & \qquad 15 \\
{} & \mathbf{2.8.} & \mbox{\footnotesize{Гладкость решений}} & \qquad 16 \\
{} & \mathbf{2.9.} & \mbox{\footnotesize{Зависимость  решений от правой части, от начальных условий и от параметров}} & \qquad 17\\
{} & {} & {} & {} \\
 \mathbf{3.} & {} & \mbox{{Примеры и приложения}} & \qquad 19\\
{} & \mathbf{3.1.} & \mbox{\footnotesize{Математическая экологии и популяционная динамика.}} & {} \\
{} & {} & \mbox{\footnotesize{Модели роста популяции}} & \qquad 19 \\
{} & \mathbf{3.2.} & \mbox{\footnotesize{Модель Лотки-Вольтерры экологического равновесия}} & {} \\
{} & {} & \mbox{\footnotesize{(``модель хищник-жертва'')}} & \qquad 21 \\
{} & \mathbf{3.3.} & \mbox{\footnotesize{Уравнение химическо реакции}} & \qquad 24\\
{} & \mathbf{3.4.} & \mbox{\footnotesize{Приложения к математической экономике. Влияние рекламы}} & \qquad 24 \\
{} & \mathbf{3.5.} & \mbox{\footnotesize{Модель Нерлофа-Эрроу расходов на рекламу}} & \qquad 25 \\
{} & {} & {} & {} \\
\mathbf{4.} & {} & \mbox{Линейные уравнения и системы} & \qquad 27\\
{} & \mathbf{4.1.} & \mbox{\footnotesize{Линейные системы с переменными коэффициентами}} & \qquad 27 \\
{} & \mathbf{4.2.} & \mbox{\footnotesize{Линейные системы с ппостоянными коэффициентами}} & \qquad 33\\
{} & \mathbf{4.3.} & \mbox{\footnotesize{Неоднородные системы с постоянными коэффициентами}} & \qquad 39 \\
{} & \mathbf{4.4.} & \mbox{\footnotesize{Линейные уравнения высших порядков с постоянными коэффициентами}} & \qquad 40 \\
{} & {} & {} & {} \\
\mathbf{5.} & {} & \mbox{Асимптотика и устойчивость} & \qquad 42\\
{} & \mathbf{5.1.} & \mbox{\footnotesize{Асимптотика решений линейных уравнений при $t \to \infty$. Резонанс.}} & \qquad 42 \\
{} & \mathbf{5.2.} & \mbox{\footnotesize{Устойчивость}} & \qquad 45\\
{} & {} & {} & {} \\
\mathbf{6.} & {} & \mbox{Приложения к вариационнному исчислению} & \qquad 48\\
{} & \mathbf{6.1.} & \mbox{\footnotesize{Простейшая задача вариационного исчисления. Уравнения Эйлера-Лагранжа}} & \qquad 48 \\
{} & \mathbf{6.2.} & \mbox{\footnotesize{Задача Больца. Условия трансверсальности }} & \qquad 53 \\
{} & \mathbf{6.3.} & \mbox{\footnotesize{Изопериметрическая задача }} & \qquad 54 \\
{} & \mathbf{6.4.} & \mbox{\footnotesize{Условия второго порядка в простейшей задаче. Уравнение Якоби}} & \qquad 56  \\
{} & {} & {} & {} \\
{} & {} & \mbox{Литература} & \qquad 63
\end{array}
$$



\newpage

\begin{center}
\large{\textbf{1. Что такое дифференциальные уравнения ?}}
\end{center}

\bigskip

\begin{center}
\textbf{1.1. Основные понятия}
\end{center}
\medskip


Обыкновенным дифференциальным уравнением порядка $n \ge 1$ называется уравнение на вектор-функцию
$x(t) = \bigl(x_1(t), \ldots , x_d(t) \bigr)$ одного переменного $t \in \re$ и первые $n$ ее производных. Пусть дана область (произвольное открытое множество)
$V \subset \re\times [\, \re^{d}\, ]^{n+1}$ и отображение  $F: V \to \re^k$. Тогда дифференциальное уравнение на функцию
$x: (a, b) \to \re^d$ порядка $n$ -- это уравнение (точнее, система из $k$ уравнений)
$$
F\, \bigl(t, x(t), \dot x(t), \ldots , x^{(n)}(t)\bigr) \, = \, 0 \qquad \Leftrightarrow \qquad
F_i\, \bigl(t, x(t), \dot x(t), \ldots , x^{(n)}(t)\bigr) \, = \, 0\, , \ i = 1, \ldots , k,
$$
 выполненное в любой точке области $V$.

Сделав замену переменной $y_0 = x, y_1  = \dot x, \ldots , y_{n-1} = x^{(n-1)}$, мы приходим к системе уравнений
первого порядка
$$
\dot y_0(t) - y_1(t) = 0, \ldots , \dot y_{n-2}(t) - y_{n-1}(t) = 0, \ F\, \bigl(t, y_0(t), \ldots , y_{n-1}(t), \dot y_{n-1}(t)\, \bigr) \, = \, 0
$$
 на вектор-функцию $y(t) = (y_0(t), \ldots , y_n(t))$. Вектор-функция $y$ отображает промежуток $(a,b)$
в пространство $\re^{\, d \, n}$. Система состоит из $n+k-1$ уравнений и действительно является уравнением первого порядка, поскольку содержит только первую производную функции $y$. Таким образом, дифференциальное уравнение любого порядка сводится к уравнению первого порядка. Впредь мы не будем переходить к функции $y$, а будем считать, что исходное уравнение на функцию $x$
имеет порядок $n=1$, т.е., имеет вид $F(t, x, \dot x) = 0$ (напомним, что это не одно уравнение, а система из $k$ уравнений).

Наконец, сделаем еще одно допущение. Если не оговорено обратное,
мы будем предполагать, что уравнение $F(t, x, \dot x) = 0$ можно разрешить относительно производной, т.е.,
из этого уравнения можно явно выразить $\dot x$ и получить уравнение вида
\begin{equation}\label{expl}
\dot x \ = \ f(t, x)\ , \qquad (t, x) \in \Omega\, ,
\end{equation}
где $\Omega \subset \re^{d+1}$ -- произвольная область (открытое множество), $f: \Omega \to \re^d$ -- произвольная функция.
Заметим, что в силу теоремы о неявной функции, если $F$ непрерывно дифференцируема в окрестности точки
$(t_0, x_0, x_1) \in V$ и невырождена, т.е., $(k\times d)$ -- матрица $F_{x_1} = \bigl( \frac{\partial F_i}{\partial (x_1)^j}\bigr)_{ij}$
имеет в этой точке полный ранг $k$, то существует окрестность этой точки, в которой $\dot x$ можно явно выразить через
остальные переменные из уравнения $F(t, x, \dot x) = 0$. Таким образом, это уравнение действительно приводится к виду
(\ref{expl}), по крайней мере, локально в окрестности каждой невырожденной точки области $V$.
\smallskip
\newpage

\begin{center}
\textbf{1.2. Примеры}
\end{center}
\medskip

\begin{ex}\label{ex.firm}\textbf{(Динамика развития фирмы)}.
{\em Одна из экономических моделей развития состоит в том, что скорость изменения 
капитала фирмы $x(t)$ в момент времени $t$ подчиняется уравнению: 
$$
\dot x \ = \  \psi(x) \ - \ \alpha \, x  \ - \ p(t, x)\ ,\qquad x(0) = x_0\, .  
$$  
$x_0$ -- начальный капитал фирмы, $\alpha$ -- инфляционный фактор (обесценивание денег с течением времени, 
износ основных фондов, и т.д.), 
$p(t)$ -- зарплата сотрудников, $\psi(ч)$ -- функция полезности (utility function). 
Функцией полезности называется скорость притока суммарной прибыли при 
вложении капитала $x$. Обычно функция полезности предполагается непрерывной, вогнутой, 
и удовлетворяющей условиям $\psi(0) = 0, \ \dot \psi(0) = +\infty , \  
\lim\limits_{x\to +\infty}\psi(x) = +\infty$. 
Вогнутость следует из того, что, как правило, размер прибыли не пропорционален вложениям. 
Увеличив вдвое капитал фирмы, мы получим рост прибыли меньше, чем в два раза. 
Происходит это по многим причинам: увеличение издержек, ужесточение конкуренции, и т.д.
 Мы рассмотрим пример 
функции полезности $\psi(x) = \sqrt{x}$. Она удовлетворяет всем условиям. 
Инфляционный фактор $\alpha$ будем считать равным $0.1$, его мы контролировать не можем. 
Зато можно управлять функцией $p(x, t)$ -- затратами на оплату труда. Мы рассмотрим две 
простейшие стратегии на эту функцию.  
\smallskip 

\textbf{Случай 1.} $\ p(t, x) \, = \, k x$: оплата труда пропорциональна 
капиталу фирмы. Получаем уравнение:   
 $$
\dot x \ = \  \sqrt{x} \ - \ (\alpha \, + \, k)\, x \,  ,\qquad x(0) = x_0\, .  
$$
 Это -- } уравнение c разделяющимися переменными. {\em Оно приводится к виду 
 $a(x)\dot x \, = \, b(t)$. Для решения  нужно умножить обе части 
 на $dt$, получив $a(x)dx \, = \, b(t)dt$, а затем взять первообразную от обеих частей. 
 Итак, 
$$
\frac{\dot x }{\sqrt{x} \ - \ (\alpha \, + \, k)\, x}\ = \ 1\quad \Leftrightarrow \quad 
  \frac{ d\, x }{\sqrt{x} \ - \ (\alpha \, + \, k)\, x}\ = \ d\, t\, . 
$$  
Берем первообразную от обеих частей: 
$$
\int \, \frac{d \, \sqrt{x} }{\frac12 \ - \ \, \frac{\alpha + k}{2}\sqrt{x}\, }\ = \ t + C\quad \Leftrightarrow \quad 
  - \frac{2}{\alpha + k}\, \ln \, 
  \Bigl( \frac12 \, - \, \frac{\alpha + k}{2}\, \sqrt{x}\, \Bigr)\ = \ t\, + \, C\, . 
$$
Откуда 
$$
x\ = \ \frac{1}{(\alpha + k)^2} \, \left( 1 \, - \, C_0 e^{- \frac{\alpha + k}{2}\, t} \right)^2 \, , 
$$
где $C_0$ -- некоторая константа. 
Подставляя начальное условие $x(0) = x_0$, получаем решение: 
\begin{equation}\label{eq.firm}
x(t)\ = \ \frac{1}{(\alpha + k)^2} \, \left( 1 \, - \, \bigl(\, 
1 \, - \, \sqrt{x_0}\, (\alpha + k) \, \bigr) \, e^{- \frac{\alpha + k}{2}\, t} \right)^2 
\end{equation}
Теперь у нас есть полная картина происходящего, и мы можем исследовать 
судьбу фирмы, в зависимости от размера ее стартового капитала~$x_0$. 
При $x_0 = \frac{1}{(\alpha + k)^2}$ решение $x(t)$
является  тождественной константой. Таким образом, $x \equiv x_s = \frac{1}{(\alpha + k)^2}$ -- 
стационарное значение. При любом другом $x_0\ne x_s$ решение непостоянно, но монотонно стремится к $x_s$
при $t \to +\infty$. Итак, в случае $p(t) = k x$ фирма не разорится ни при каком начальном значении стартового капитала. При любом $x_0$ капитал фирмы стабилизируется со временем на предельном значении 
$x_s$ (не зависящем от $x_0$!). При $x_0 < x_s$ фирма монотонно богатеет. При 
$x_0 < x_s$ -- монотонно беднеет, что плохо.

 Таким образом, устанавливать зарплату пропорционально капиталу фирмы разумно при малом значении стартового капитала~$x_0$. С ростом капитала фонд зарплаты становится слишком большим, что тормозит дальнейшее развитие. Доходы перестают расти из за  непомерных трат.  Пусть капитал измеряется в миллионах 
 \$ \$, а время -- в годах. Тогда при $\alpha = 0.1, k  = 0.4, x_0 = 1$ получаем 
$$
x(t)\quad = \quad  \bigl( 2 \, - \, e^{\, - t/4}\bigr)^2\, . 
$$ 
Капитал монотонно возрастает. До стационарного 
 значения $x_s = \frac{1}{(0.1 + 0.4)^2} = 4$. Этого значения он никогда не достигнет.
Растет и фонд зарплаты: вначале  $p(0) = 0.4$, через год $p(1) = 0.6$, далее 
$p(2) = 0.78, \ p(3) =  0.93 ,\ p(4) = 1.06$, а через $10$ лет будет $p(10) = 1.47$, 
что уже близко с стационарному значению $p(+\infty) = 1.6$.     
 
 
 Повысить прибыльность фирмы можно, используя другую стратегию расходов на зарплату,
 которая представлена в следующем случае.     
\smallskip 


\textbf{Случай 2.} $p(t) \equiv p$, Затраты фирмы на оплату труда постоянны. Получаем уравнение:   
 $$
\dot x \ = \  \sqrt{x} \ - \ \alpha \, x \, - \, p  ,\qquad x(0) = x_0\, .  
$$
Это -- также уравнение с разделяющимися переменными, и его можно явно решить. Однако, формула  будет громоздкой, а самое главное --  уже не удастся 
явно выразить $x$ через $t$. Поэтому, попробуем качественно исследовать решение, не находя его. 
\smallskip 

Положим $z = \sqrt{x}$ и обозначим через $f(z)$ правую часть уравнения. 
Решение будет тождественной константой, если $f(s) = 0$. 
Дискриминант этого квадратного уравнения относительно 
$\sqrt{x}$ равен $1 - 4\alpha p$. Возможны случаи: 
\smallskip 

1)  $\, p \, > \, \frac{1}{4\alpha}$. В этом случае, $f(s)$ -- парабола с ветвями вниз, не пересекающая ось абсцисс. Поэтому $f(s) < - q$, где $q > 0$ -- константа. 
Значит, за время, не превосходящее $t = x_0/q$ величина $x$ станет равной нулю. Фирма разорится за конечное время.
\smallskip 

2)  $\, p \, \le  \, \frac{1}{4\alpha}$.  Решая квадратное уравнение, находим два корня: 
$$
x_1 \ = \ \left(\frac{1 - \sqrt{1 - 4\alpha p}}{2\alpha} \right)^2\ ; \qquad 
x_2 \ = \ \left(\frac{1 + \sqrt{1 - 4\alpha p}}{2\alpha} \right)^2
$$ 
Дифференциальное уравнение~(\ref{eq.firm}) имеет два стационарных решения $x(t)\equiv x_1$ и 
$x(t)\equiv x_2$. Правая часть уравнения 
отрицательна при $x \ne [x_1, x_2]$ и положительна при 
$x \in (x_1, x_2)$. Поэтому, при $x_0 < x_1$ имеем $\dot x(t) < 0$ при всех $t$. 
Более того, правая часть уравнения убывает от $t$, а значит $\dot x(t) \, < \, \dot x(0) \, < \, 0$
 при всех $t$. 
Следовательно, $x(t)$ убывает до нуля за конечное время. Итак, при $x_0 < x_1$ фирма разоряется за конечное время.  

При $x_0 \in (x_1, x_2)$ функция $x(t)$ возрастает, но не достигает значения $x_2$. 
В самом деле, если, от противного $x(\bar t) = x_2$, то через точку $(\bar t, x_2)$ проходят два разных решения уравнения: данное решение $x(t)$ и стационарное решение $x(t)\equiv x_2$. 
А это невозможно в силу теоремы о единственности решений (теорема~\ref{th.eu-1}), которую мы докажем 
в \S 2.2. Следовательно, $x(t)$ возрастает на всей полупрямой $[0, +\infty)$. 
Значит, существует предел $x_{\infty} = \lim_{t \to + \infty} x(t)$. Следовательно $\lim_{t \to + \infty} 
\dot x(t) \, = \, 0$, а значит $x_{\infty} = x_2$. 
Итак, при $x_0 \in (x_1, x_2)$ решение $x(t)$ монотонно возрастает и стремится к $x_2$ при $t \to +\infty$. 
\smallskip 

Так же показываем, что при  $x_0 > x_2$ решение $x(t)$ монотонно убывает и стремится к $x_2$  при $t \to +\infty$. 

Подведем итоги. Если  $p \equiv \rm {\rm const}$, то при $p > 1/4\alpha$ фирма разорится за конечное время. 
При $p \le 1/4\alpha$ все зависит от $x_0$. Если $x_0 < x_1$, то фирма снова разорится. Если 
$x_0 = x_1$, то $x(t) \equiv x_1$. Если же $x_0 > x_1$, то $x(t)$ монотонно стремится ко второму стационарному решению $x_2$ при $t \to \infty$.  То есть, при любом $x_0 > x_1$ фирма не разоряется и ее решение 
устойчиво стабилизируется при $t \to +\infty$.   

Пусть опять $\alpha = 0.1$, а уровень оплаты возьмем $p = 0.9$, как после трех   
лет работы фирмы в предыдущем случае. Тогда $x_2 = 81$. Заметим, что $x_1 = 1$.  Таким образом, 
при любом $x_0 > 1$ капитал фирмы монотонно возрастает до предельного значения $81$
(более, чем в $20$ раз больше, чем в случае $p = kx$!). При $x_0 = 1$ он остается все время равным $1$ 
При любом $x_0 < 1$ фирма разоряется за конечное время. 
}
\end{ex}
\begin{ex}\label{ex.warm}
\textbf{(Задача о гусенице)}. На ростке бамбука длиной 1 м, в самом низу сидит гусеница. Бамбук растет со скоростью 1 метр в день, а гусеница ползет по нему вверх со скоростью (относительно бамбука) 1 мм в день. Доползет ли гусеница до вершины?


\textbf{Решение.} {\em Пусть время измеряется в днях, а длина -- в метрах. Через время $t$ длина бамбука равна $t+1$.
Пусть $x$ -- высота, на которой к этому времени оказалась гусеница. Ясно, что $x(0) = 0$ и что $x(t) \le t+1$ при любом
$t \ge 0$. Надо понять, наступит ли момент времени, когда $x(t) = t+1$.

Скорость гусеницы складывается из двух частей: ее скорости относительно бамбука, равной $a = 10^{-3}$, и
скорости точки ствола бамбука, в которой она оказалась. Последняя равна $1\cdot \frac{x}{t+1}$. В самом деле,
 ствол растет равномерно по всей длине, его верхушка отдаляется от земли со скоростью $1$ метр в день,
 а точка, делящая его в отношении
$x: (t+1-x)$, растет со скоростью $1\cdot \frac{x}{t+1}$ метров в день. Получаем уравнение на функцию $x$:
\begin{equation}\label{warm1}
\dot x \ = \ \frac{x}{t+1}\ + \ a \, .
\end{equation}
Функция $x$ удовлетворяет ему в каждой точке $t > 0$. А в точке $t=0$ выполнено {\em начальное условие} $x(0) = 0$.
Получили дифференциальное уравнение первого порядка. Для того, чтобы его решить, мы решим сначала
более простое {\em однородное уравнение}  $\dot x  =  \frac{x}{t+1}$. Разделив обе части на $x$, получим
$\frac{\dot x}{x}  =  \frac{1}{t+1}$. Взяв первообразную от обеих частей и учитывая, что $\dot x \, d t \, = \, d x$, получим
$$
\int \frac{d x}{x} \ = \ \int \frac{d t}{t+1} \quad \Rightarrow \quad \ln x \ = \ \ln (t+1) \ + \ C_0,
$$
где $C_0$ -- константа. Таким образом, $x(t) = C(t+1)$, где $C = e^{C_0}$.  Итак, при любом $C$ функция
$C(t+1)$ является решением однородного уравнения $\dot x  =  \frac{x}{t+1}$. Для отыскания решения уравнения
(\ref{warm1}) мы используем  прием, называемый {\em вариацией постоянной}. Представим решение в виде
$x(t) = C(t) (t+1)$. Заметим, что при этом мы ничего не теряем, поскольку любую функцию можно представить в таком виде,
положив просто $C(t) = x(t)/(t+1)$. Подставляя $C(t)(t+1)$ в уравнение~(\ref{warm1}), и учитывая, что
$\bigl(C(t)(t+1) \Bigr)' \, = \, \dot C(t)\, (t+1) + C$, получаем
$$
\dot C(t)\, (t+1) + C  \quad = \quad \frac{C(t+1)}{t+1} \ + \ a\, .
$$
величина $C$ сократится  в обеих частях. Это происходит потому, что $C(t+1)$ -- решение однородного уравнения.
Остается  $\dot C(t)\, (t+1) = a$. Таким образом, $\dot C(t) = \frac{a}{t+1}$, откуда
$C(t) = a\ln (t+1) + C_1$. В итоге получаем $x(t) \, = \, a (t+1)\, \ln (t+1) \, + \, C_1(t+1)$.
Поскольку $x(0) = 0$, имеем $C_1 = 0$, откуда окончательно
$$
x(t) \ = \ a\, (t+1)\, \ln (t+1)
$$
 Момент, когда $x(t) = t+1$ наступит при $a\ln (t+1) = 1$, т.е., при $t = e^{1/a} -1$. Вспоминая, что $a=10^{-3}$,
 получаем, что гусеница доползет до верхушки через $e^{1000}-1$ дней, т.е., примерно, $5\cdot 10^{431}$ лет. Бамбук при этом достигнет высоты $10^{431}$ км.
}
\end{ex}


\bigskip 

\begin{center}
\large{\textbf{2. Общая теория дифференциальных уравнений}}
\end{center}

\bigskip


\begin{center}
\textbf{2.1. Задача Коши}
\end{center}
\medskip

Мы будем решать уравнение~(\ref{expl}) на неизвестную вектор-функцию $x: \, (a, b) \to \re^d$.
Любое решение этого уравнения называется{\em интегральной кривой}. Таким образом, интегральная кривая --
это функция $x$, определенная на некотором интервале $(a, b)$, для которой $(t, x(t)) \in \Omega$ и
$\dot x (t) = f(t, x(t))$ при любом $t \in (a, b)$.


Отыскание решения, проходящего через заданную точку области $(t_0, x_0)\in \Omega $, называется
{\em задачей Коши}:
\begin{equation}\label{Cauchy}\left\{
\begin{array}{l}
\dot x \ = \ f(t, x)\ , \qquad (t, x) \in \Omega\, ,\\
x(t_0)\, = \, x_0\, .
\end{array}
\right.
\end{equation}
Условие $x(t_0)\, = \, x_0$ называется {\em начальным условием}. Как правило,
решение ищется на некотором отрезке $[t_0, t_1]$ (с начальной точкой $t_0$), мы будем придерживаться чуть более общей постановки, и искать решение на некотором интервале $(a, b)$ содержащем точку $t_0$. Если найдется такой интервал, на  котором
существует решение~(\ref{Cauchy}), то мы будем говорить, что через точку $(t_0, x_0)$ проходит интегральная кривая.
Сейчас мы докажем, что если функция $f$ липшицева по переменной $x$, то через каждую точку области проходит единственная интегральная кривая.
\medskip

\begin{center}
\textbf{2.2. Существование и единственность решения. \\
Принцип сжимающих отображений}
\end{center}
\medskip

Предположим, что функция $f$ является липшицевой по переменной $x$ с некоторой константой $K$. Это значит, что
$$
\|f(t, x_1) \, - \, f(t, x_2)\| \ \le \ K \, \|x_1 \, - \, x_2\|\ \, , \qquad (t, x_1), (t, x_2) \in \Omega\, .
$$
Таким образом, для всех  $t$ функции $f(t, \cdot)$ являются липшицевыми с одной и той же константой~$K$.
\begin{theorem}\label{th.eu-1}
Если функция $f$ липшицева по $x$, то для каждой точки $(t_0, x_0) \in \Omega$ найдется
интервал, содержащий $t_0$, на котором уравнение $\dot x = f(t, x)$ имеет единственное решение,
 удовлетворяющее условию $x(t_0) = x_0$.
\end{theorem}
Итак, задача Коши с липшицевой по $x$ правой частью всегда имеет единственное решение на некотором интервале
(т.е., локально). Доказательство основано на принципе сжимающих отображений:
\begin{theorem}\label{th.contr}
Пусть $M$ -- полное метрическое пространство, $F : M \to M$ -- сжимающее отображение, т.е.,
существует $q< 1$ такое, что $\rho(Fx_1, Fx_2) \, < \, q\, \rho(x_1, x_2)$ для всех $x_1, x_2 \in M$.
Тогда $F$ имеет единственную неподвижную точку: $F(z) = z$.
\end{theorem}
{\tt Доказательство}. Для произвольной точки $z_0\in M$ рассмотрим последовательность $z_{k+1} = F(z_k), \, k \ge 0$.
Поскольку $\rho(z_{k}, z_{k+1}) = \rho(F(z_{k-1}), F(z_{k}))\,  < \, q\, \rho(z_{k-1}, z_{k})$, получаем:
$\rho(z_{k}, z_{k+1}) < q^k \rho(z_0, z_1)$. Следовательно, для любых чисел $m < n$ имеем
$\rho(z_m, z_n) \le \rho(z_m, z_{m+1})+ \ldots + \rho(z_{n-1}, z_n) \, \le \,
(q^m + q^{m+1} + \cdots + q^{n-1})\rho(z_0, z_1) \, \le \, q^m C$, где константа $C$ не зависит от
$m$ и $n$. Следовательно, это -- фундаментальная последовательность, значит она имеет предел $z \in M$.
Перейдя к пределу при $k \to \infty$ в обеих частях равенства $F(z_k) = z_{k+1}$, получаем, что
$F(z) = z$. Докажем единственность. Если кроме $z$ есть еще неподвижная точка $y$, то $\rho(x, y) = \rho(Fz, Fy) < q \rho(x, y) $,  откуда $1 < q$, что противоречит предположению.

   {\hfill $\Box$}
\medskip

{\tt Доказательство теоремы~\ref{th.eu-1}}.
Проинтегрировав равенство $\dot x = f(t, x)$ по отрезку $[t_0, t]$, получим равносильное
{\em интегральное уравнение}:
\begin{equation}\label{integr}
x(t) \ = \ x_0 \, + \, \int_{t_0}^t f(\tau, x(\tau))\, d \, \tau\ , \qquad t \in (a, b)\, .
\end{equation}
Заметим, что любое решение этого уравнения удовлетворяет краевому условию $x(t_0)\, = \, x_0$.
В случае $t < t_0$ в интеграле выражение $d\tau$ берется со знаком минус.
Для малого $\delta > 0$, значение которого определим позже, рассмотрим интегральный оператор
$F: C[t_0-\delta, t_0  +\delta]$, заданный формулой:
 $$
 [F\varphi](t) \ = \  x_0 \, + \, \int_{t_0}^t f(\tau, \varphi(\tau))\, d \, \tau\, , \qquad \tau \in [t_0-\delta, t_0  +\delta]\, .
 $$
Из точки $(t_0, x_0)$ проведем всевозможные прямые, образующие с горизонталью угол, тангенс которого равен  $2\|f(t_0, x_0)\|$.
Обозначим через $P$ фигуру из двух конусов, образуемых этими прямыми и вертикальными гиперплоскостями
$t = t_0 \pm \delta$. Уменьшая, если надо, $\delta$, будем предполагать, что
$\|f\|_{P} \, < \, 2 \|f(t_0, x_0)\|$. Обозначим, наконец, через $\Phi(P)$ множество непрерывных функций,
график которых лежит в $P$. Для любой функции $\varphi \in \Phi(P)$ имеем:
$$
\bigl\|[F\varphi](t) \, - \, [F(\varphi)](0)\bigr\|\  = \
\bigl\|[F\varphi](t) \, - \, x_0\bigr\|\  \le \ 2\bigl\|f(t_0, x_0) (t-t_0)\bigr\|
$$ для всех $t$, поэтому $F\varphi \in \Phi(P)$.
Итак, $F$ переводит $\Phi(P)$ в себя. Для любых $\varphi_1, \varphi_2 \in \Phi(P)$ имеем
$$
\Bigl|[F\varphi_1](t) - [F\varphi_2](t)\Bigr| \ = \
\Bigl| \int_{t_0}^t \Bigl( f(\tau, \varphi_1(\tau)) - f(\tau, \varphi_2(\tau))\Bigr)\, d \, \tau\, \Bigr|\ \le \
K \|\varphi_1 - \varphi_2\| |t_0 - t| \ \le \ K\delta \|\varphi_1 - \varphi_2\|\, .
$$
Таким образом, если $K \delta < 1$, то отображение $F$ сжимающее на $\Phi(P)$, следовательно оно имеет единственную неподвижную точку.


   {\hfill $\Box$}
\medskip

\begin{cor}\label{c.lip1}
Если частная производная $f_x$ ограничена на~$\Omega$, то для каждой точки $(t_0, x_0) \in \Omega$ найдется
интервал, содержащий $t_0$, на котором уравнение $\dot x = f(t, x)$ имеет единственное решение,
 удовлетворяющее условию $x(t_0) = x_0$.
\end{cor}
\begin{cor}\label{c.lip2}
Если в уравнении $n$-ного порядка $x^{(n)} = f(t, x, \dot x, \ldots , x^{(n-1)})$
все частные производные $f_{x^{(k)}}, \, k = 0, \ldots , n-1$ ограничены в некоторой области, то для каждой точки
$(t_0, x_0, \ldots , x_0^{(n-1)})$ найдется
интервал, содержащий $t_0$, на котором это уравнение имеет единственное решение с данными начальными условиями.
\end{cor}
\begin{remark}\label{r.non-unique}
{\em Как мы увидим в дальнейшем, для локального существования решения свойство липшицевости не нужно, достаточно непрерывности функции $f$. Но для единственности решения это условие опустить нельзя, как показывает следующий пример:
\begin{ex}\label{ex.non-unique}
{\em Уравнение $\dot x(t) \, = \, \sqrt{|x(t)|}$ с условием $x(0) = 0$ в области
$$
\Omega =
\bigl\{ (t, x) \in \re^2 \ \bigl| \ -1 \le t \le 1\, \bigr\}
$$
 имеет два решения: первое $x(t) \equiv 0$ и второе:
$$
x(t) \ = \
\left\{
\begin{array}{lll}
{}&  \frac{1}{4}\, t^2\, , & t \ge 0;\\
- &\frac{1}{4}\, t^2\,  , & t < 0\, .
\end{array}
\right.
$$
При этом точка $(0, 0)$ лежит внутри области, функция $f(x) = \sqrt{x}$ непрерывна, но не липшицева
(липшицевость нарушается только в этой точке)}.
\end{ex}}
\end{remark}

\medskip
\begin{center}
\textbf{2.3. Продолжение решений}
\end{center}
\medskip

\begin{theorem}\label{th.contin}
Если функция $f$ ограничена и липшицева по $x$ в области~$\Omega$,
то каждая интегральная кривая уравнения $\dot x = f(t, x)$
однозначно продолжается в каждую сторону до границы области~$\Omega$ или до бесконечности.
\end{theorem}
{\tt Доказательство}. Фиксируем произвольную точку $(t_0, x_0) \in \Omega$ и обозначим
через $\bar t$ супремум чисел $t > 0$, для которых уравнение имеет единственное решение на промежутке $[t_0, t]$.

Если $\bigl(\bar t, x(\bar t)\bigr) \in \Omega$, то для достаточно малого $\delta > 0$ (не превосходящего $\delta$ из доказательства теоремы~\ref{th.eu-1}), прямоугольник с центром в этой точке и со сторонами $2\delta$ и $4\delta \|f\|$ лежит в $\Omega$. Так как $f$ ограничена, то $x$ липшицева, и следовательно $x(\bar t) = \lim_{t \to \bar t} x(t)$. Возьмем $t_1 \in [t_0, \bar t)$ достаточно близко к точке $\bar t$ так, чтобы $t_1 > \bar t - \delta$ и чтобы данный прямоугольник с центром в точке $(t_1, x(t_1))$ по-прежнему лежал в $\Omega$.
Возьмем единственное решение $\varphi(t)$ на отрезке $[t_0, t_1]$. Тогда на промежутке $(t_1-\delta, t_1+\delta)$
это уравнение имеет решение $x$ с условием $x(t_1) = \varphi(t_1)$ (см. доказательство теоремы~\ref{th.eu-1}).
В силу единственности, это решение на промежутке $(t_1 - \delta, t_1]$ совпадает с $\varphi$.
Следовательно, $\varphi$ продолжается до точки $t_1 + \delta > \bar t$, что противоречит определению  $\bar t$.

Итак, точка $(\bar t , x(\bar t))$ находится на границе $\Omega$. Тогда для любых точек $t_1 < t_2 < \bar t$
решения, соответствующие промежуткам $[t_0, t_1)$ и $[t_0, t_2)$ совпадают, в силу единственности на множестве $[t_0, t_1)$.
Поэтому, решение продолжается на весь отрезок $[t_0, \bar t]$.


   {\hfill $\Box$}
\medskip


Таким образом, если правая часть дифференциального уравнения липшицева по~$x$, то все область~$\Omega$ покрывается
полем интегральных кривых. Каждая из них -- график решения уравнения. Через каждую точку области проходит единственная интегральная кривая и никакие две кривые не пересекаются.
\begin{cor}\label{c.lip3}
Если $f$ и $f_x$ непрерывны на некотором компакте, то
то каждая интегральная кривая уравнения $\dot x = f(t, x)$
однозначно продолжается в каждую сторону до границы этого компакта.
\end{cor}

Заметим, что продолжаемость до границы области не означает того, что каждое решение продолжается
до конца отрезка или на всю прямую. Рассмотрим следующий пример.
\begin{ex}\label{ex.lip1}
{\em Рассмотрим уравнение $\dot x = -x^2$ на области $\Omega = \{(t, x )\in \re^2 \ | \ -2< t < 2\}$. Его общее решение $x(t) = \frac{1}{t+C}$ и $x\equiv 0$.
Решение $x(t) = \frac{1}{t+1}$, проходящее через точку $(0,1)$, уходит в бесконечность при
$t \to -1$ и не доходит до левого конца отрезка $[-2,2]$. Таким образом, на весь отрезок это решение не продолжается.}
\end{ex}
\begin{ex}\label{ex.lip2}
{\em Уравнение $t^4\dot x^2 + x^2 = 1$ в полуплоскости $t> 0$
имеет решение $x(t) = \sin\, \frac{1}{t}$.
При $t\to +0$ оно приближается к границе области, но не ``упирается'' в нее, т.к., не сходится. Частичные пределы
этого решения при $t \to +0$ заполняют отрезок $[-1,1]$ оси $OX$. }
\end{ex}

На какой отрезок можно гарантированно продолжить решение?   Следующая теорема дает количественный вариант локальной теоремы существования (теоремы~\ref{th.eu-1}).
\begin{theorem}\label{th.segment}
Пусть область $\Omega$ содержит цилиндр $P = [t_0, t_0+a]\, \times \, \bigl\{x \in \re^d \ | \ \|x-x_0\|\le b\bigr\}$,
пусть также $m = \max_{(t, x) \in P}\|f(t, x)\|$ и $f$ имеет на $P$ непрерывную частную производную по~$x$. Тогда
уравнение $\dot x = f(t, x)$ имеет единственное решение на отрезке $[t_0, t_0 + c]$ с начальным условием
$x(t_0) = x_0$, где $c = \min\, \{a, b/m\}$. График этого решения лежит в~$P$.
\end{theorem}
{\tt Доказательство.} Если $K$ -- максимум нормы частной производной $\|f_x\|$ на $P$, то $f$ липшицева по~$x$
с константой~$K$ на этом множестве. Поэтому, в силу теорем~\ref{th.eu-1} и \ref{th.contin}, уравнение
с данным начальным условием имеет решение
на некотором отрезке $[t_0, t+\delta]$, которое продолжается до пересечения с границей~$P$.
Если точка пересечения лежит на противоположном основании цилиндра, то решение продолжается на отрезок
$[t_0, t_0 + a]$, и все доказано. Если же она лежит на боковой поверхности цилиндра~$P$, т.е., имеет вид $(t_1, x)$,
где $t_1 < t_0 + a$ и $\|x-x_0\| = b$, то, в силу теоремы о промежуточном значении,
на отрезке $[t_0, t_1]$ найдется точка $\bar t$, в которой $\|\dot x(\bar t)\| \ge \|x-x_0\|/(t_1 - t_0) =
\frac{b}{t_1 - t_0}$. Следовательно, $\frac{b}{t_1 - t_0} \le m$, откуда $t_1 \ge t_0 + \frac{b}{m}$.


   {\hfill $\Box$}
\medskip
\newpage

\medskip
\begin{center}
\textbf{2.4. Метод последовательных приближений}
\end{center}
\medskip

Назовем функцию $\varphi(t)$ определенную  на некотором отрезке $[a,b]$, допустимой, если
ее график лежит в области $\Omega$.
В доказательстве теоремы~\ref{th.eu-1} мы установили, что интегральный оператор $F$ является сжимающим на любом отрезке
длины меньше $1/K$, где $K$ -- константа Липшица функции $f$ по переменной~$x$. Именно, если обе функции $\varphi_1, \varphi_2$ и их образы $F\varphi_1, F\varphi_2$
допустимы, то  $\|F\varphi_1 - F\varphi_2 \|\, \le \, K(b-a)\|\varphi_1 - \varphi_2\|$.
Поэтому, если $K(b-a) < 1$, то оператор $F$ сжимающий с коэффициентом $q = K(b-a)$. Таким образом, мы доказали
\begin{theorem}\label{th.secv}
Если $q = K(b-a) < 1$   и  все функции $\varphi, F\varphi, F^2\varphi, \ldots$ допустимы, то последовательность
$F^k \varphi$ сходится к решению $x$ уравнения $\dot x = f(t, x) $ на отрезке $[a, b]$. При этом
$$
\|F^k \varphi - x\|\, \le \, q^k \|\varphi - x\|\, , \ k \in \n\, .
$$
\end{theorem}
Эта теорема служит основой метода последовательных приближений. Решение дифференциального уравнения
приближается с помощью итераций интегрального оператора~$F$, примененного к произвольной допустимой функции.
 Если отрезок $[a, b]$ достаточно мал, так, что $K(b-a) < 1$, то
метод сходится к решению уравнения с экспоненциальной  скоростью.
\begin{ex}\label{ex.secv}
{\em Простейшее дифференциальное уравнение $\dot x = x$ с начальным условием $x(0)=1$, как мы знаем, имеет решение
$x(t) = e^t$. Посмотрим, как работает метод последовательных приближений. Функция $f(t, x) = x$ липшицева с константой
 $K=1$. Возьмем отрезок $[0, b]$ и интегральный
оператор на пространстве $C[0, b]$,  действующий по формуле
$[F\varphi] (t) \, = \, 1\, +\, \int_0^t f(\tau, \varphi(\tau))\, d \tau \, = \, 1\, + \,
\int_0^t \varphi(\tau)\, d \tau\, , \
t \in [0, b]$.
В качестве начальной функции можно взять любую функцию $\varphi$, удовлетворяющую начальному условию $\varphi(0)=1$.
Возьмем, например $\varphi \equiv 1$. Тогда $F\varphi = t+1, \, F^2\varphi = \frac{1}{2}t^2 + t+ 1$ и т.д.
$[F^k\varphi ](t) = \sum_{m=0}^k \frac{t^k}{k!}$. Если $K(b-a) < 1$, т.е., если $b < 1$, то $F^k \varphi$ равномерно сходится
к~$e^t$ на отрезке $[0, b]$. Таким образом, мы получили еще одно доказательство разложение экспоненты в степенной ряд.
На самом деле, как мы знаем, он равномерно сходится на всей прямой, а не только при $b<1$.
}
\end{ex}

\medskip
\begin{center}
\textbf{2.5. Геометрическая интерпретация. Ломаные Эйлера}
\end{center}
\medskip

Предположим, что мы хотим решить задачу Коши на заданной области $\Omega$ с заданной функцией
$f(t, x)$, но не получается найти
явной формулы решения. Тогда нужно решать численно, т.е, приближенно. Наиболее естественный и практичный способ --
построить ломаную, приближающую интегральную кривую. Такая ломаная строится последовательно:
сначала из точки $A_0 = (t_0, x_0)$ выпускам луч в направлении вектора $(d t , d x)$, т.е., в направлении вектора
$(1, \dot x(t_0)) = (1, f(t_0, x_0))$. В случае $d=1$, это будет вектор на плоскости, составляющий угол
 $\alpha = {\rm arctg}\, \dot x(t_0)$ с горизонталью. Этот вектор касается интегральной кривой в точке $A_1$.
 Далее выбираем на этом луче точку $A_1 = (t_1, x_1)$ и следующий луч, в направлении вектора
 $(1, \dot x(t_1))$, выпускаем из этой точки. Это -- касательная к интегральной кривой, проходящей через точку
 $A_1$. Далее выбираем точку $A_2=  (t_2, x_2)$ на нем, и т.д. В результате получаем {\em ломаную Эйлера}.
 \begin{defi}\label{d.euler}
 Ломаной Эйлера называется ломаная $A_0 \ldots A_N$ в области $\Omega$, $\ A_i = (t_i, x_i)$
 такая, что отрезок $A_iA_{i+1}$ сонаправлен вектору $(1, \dot x(t_i))$ для каждого $\, i = 0, \ldots , N-1$.
 Кусочно-линейная функция $\varphi(t)$ на отрезке $[t_0, t_N]$, график которой совпадает с данной ломаной также называется ломаной Эйлера.  Для этой функции $\varphi(t_i) = x_i, \, \dot \varphi(t_i) = f(t_i, x_i)\, , \, i = 0, \ldots , N-1$.
 \end{defi}
Максимальным шагом ломаной Эйлера называется длина максимального из отрезков разбиения $[t_i, t_{i+1}]$.
Если все эти отрезки равны, то такую ломаную и соответствующее  разбиение назовем равномерным, а длину отрезка разбиения назовем шагом.
\begin{theorem}\label{th.euler}
Если последовательность ломаных Эйлера, построенных на отрезке $[a, b]$, с максимальным шагом стремящимся к нулю,
равномерно сходится к некоторой функции, график которой лежит в~$\Omega$, то эта функция является интегральной кривой.
\end{theorem}
Это значит, что если для последовательности ломаных Эйлера $\{\varphi_k\}_{k \in \n}$ максимальный шаг стремится к нулю
и $\|\varphi_k - \varphi\|_{[a, b]}\to 0$ при $k \to \infty$, то $x = \varphi$ -- решение уравнения $\dot x = f(t, x)$.

\noindent {\tt Доказательство.} Возьмем $r> 0$
и рассмотрим множество $Q_r = \{(t, x) \in \re^{d+1} \ | \ \|x - \varphi (t)\| \le r,\ t \in [a,b]\}$.
Считаем $r$ достаточно малым так, чтобы $Q_r \subset \Omega$, а $k$ достаточно большим так, чтобы
график $\varphi_k$ содержался в $Q_r$. Пусть $M$ -- максимум функции $f$ на  $Q_r$. Фиксируем $\varepsilon > 0$. Поскольку $f$ равномерно непрерывна
на компакте $Q_r$, найдется число $\rho$ такое, что $\|f(t_1, x_1) - f(t_2, x_2)\| < \varepsilon$
для любых точек $(t_1, x_1), (t_2, x_2) \in Q_r$, расстояние между которыми меньше $\rho$.
Возьмем на отрезке $[a, b]$ произвольные точки $\alpha < \beta$. Производная функции $\varphi_k$
в каждой точке отрезка $[\alpha, \beta]$, за исключением узловых точек (вершин ломаной), не превосходит $M$, поэтому
$\|\varphi_k(t) - \varphi_k(\alpha)\| \le M (\beta - \alpha)$ для всех $t \in [\alpha, \beta]$.
Кроме того, $\|\varphi_k(\alpha) - \varphi(\alpha)\| < \rho/2$ при больших $k$. Поэтому, если
$\beta - \alpha < \rho/2M$, то график функции $\varphi_k$ на отрезке $[\alpha, \beta]$ целиком лежит в $\rho$-окрестности точки
$(\alpha, \varphi(\alpha))$. Значит, в  любой точке этого графика значение функции $f$
отличается не более, чем на $\varepsilon$ от ее значения в точке $(\alpha, \varphi(\alpha))$.
Пусть $t_m, \ldots , t_{m+N}$ -- узловые точки функции $\varphi_k$, лежащие на отрезке $[\alpha, \beta]$. Тогда
$$
\varphi_k(t_{m+N}) \, - \, \varphi_k(t_{m}) \ = \ \int_{t_m}^{t_{m+N}}\, \dot \varphi_k dt \ = \
\sum_{i=m}^{m+N-1} (t_{i+1} - t_i)f(t_i, \varphi_k(t_i)) \ = \
$$
$$
\sum_{i=m}^{m+N-1} (t_{i+1} - t_i)f(\alpha, \varphi (\alpha)) \ + \
\sum_{i=m}^{m+N-1} (t_{i+1} - t_i)\Bigl[f(t_i, \varphi_k(t_i)) \, - \, f(\alpha, \varphi (\alpha))\Bigr]
$$
  Первая сумма равна $(t_{m+N} - t_m)f(\alpha, \varphi (\alpha))$, а вторая не превосходит по норме
 величины  $(t_{m+N} - t_m)\varepsilon$. Следовательно,
\begin{equation}\label{eq.aa}
\left\|\, \frac{\varphi_k(t_{m+N}) \ - \ \varphi_k(t_{m})}{t_{m+N} \, - \, t_m} \ - \
f(\alpha, \varphi(\alpha)) \right\| \quad < \quad \varepsilon \, .
\end{equation}
Поскольку максимальный шаг ломаной Эйлера стремится к нулю, имеем $t_{m} \to \alpha$ и $t_{m+N} \to \beta$
при $k \to \infty$. Кроме того, $\|\varphi_k - \varphi\| \to 0$.
Следовательно, переходя в неравенстве~(\ref{eq.aa}) к пределу при $k \to \infty$, получаем
$$
\left\|\, \frac{\varphi(\beta) \ - \ \varphi(\alpha)}{\beta \, - \, \alpha} \ - \
f(\alpha, \varphi(\alpha)) \right\| \quad < \quad \varepsilon \, .
$$
При $\beta \to \alpha$ мы получаем $\|\dot \varphi(\alpha) - f(\alpha, \varphi(\alpha))\| <  \varepsilon$.
Так как мы выбирали $\varepsilon >0$ произвольно, при
$\varepsilon \to 0$, получаем
$\dot \varphi(\alpha) = f(\alpha, \varphi(\alpha))$. Следовательно, $\varphi$ -- интегральная кривая.


   {\hfill $\Box$}
\medskip

Для дальнейших рассуждений нам понадобится следующий классический результат, принадлежащий
итальянским математикам Чезаре Арцела (1847 -- 1912) и Джулио Асколи (1843 -- 1896).
\begin{theorem}\label{th.aa}[C.Arzel\'а, G.Ascoli, 1883--1884].
Подмножество пространства $C([a, b], \re^d)$ компактно тогда и только тогда когда оно ограничено, замкнуто и  равностепенно непрерывно.
\end{theorem}
Напомним, что семейство функций $\cM \subset C([a, b], \re^d)$ является равностепенно непрерывным если для любого $\varepsilon> 0$ существует
$\delta > 0$, при котором $\|\varphi(t_1) - \varphi(t_2)\| < \varepsilon$ для любых точек $t_1, t_2 \in [a, b]$ таких, что $|t_1 - t_2| < \delta$ и для любой функции $\varphi \in \cM$.

В доказательстве теоремы Арцела-Асколи используется понятие внешней $\varepsilon$-сети.
\begin{defi}\label{d.eps}
Если полное метрическое пространство $M$ вложено в некоторое метрическое пространстве $S$,
то конечное множество $S = \{s_1, \ldots , s_N\}$ является внешней $\varepsilon$-сетью для $M$,
если для любой точки  $x \in M$ найдется $s_i$, для которого $\rho(x, s_i) < \varepsilon$.
\end{defi}
\begin{lemma}\label{l.eps}
Полное метрическое пространство $M$, вложенное в  метрическое пространстве $S$, является компактным
тогда и только тогда когда для любого $\varepsilon > 0$ оно имеет внешнюю $\varepsilon$-сеть.
\end{lemma}
{\tt Доказательство.} (Необходимость). Для произвольного $\varepsilon > 0$ рассмотрим множество
всех шаров  радиусом
$\varepsilon$ с центрами  во всевозможных  точках множества~$M$. Это множество, очевидно, образует покрытие~$M$.
Если $M$ компактно, то из этого покрытия  можно выбрать конечное подпокрытие. Центры шаров этого подпокрытия образуют $\varepsilon$-сеть.

(Достаточность). Достаточно доказать, что из каждой последовательности точек $\{x_i\}_{i\in \n}$
можно выбрать сходящуюся подпоследовательность. Для $\varepsilon = \frac12$ существует внешняя $\varepsilon$-сеть.
Шары радиусом $\varepsilon$ с центрами в точках этой сети покрывают все множество~$M$. Значит, один из
этих шаров содержит бесконечную подпоследовательность. Оставим эту подпоследовательность, убрав все остальные члены последовательности $\{x_i\}_{i\in \n}$. Пусть $n_1$ -- наименьший из индексов оставшихся точек. Теперь рассмотрим $\varepsilon$-сеть для $\varepsilon = \frac14$. Снова переходим к подпоследовательности точек,
лежащих в одном шаре радиусом $\frac14$ и обозначаем $n_2$ -- наименьший из индексов оставшихся точек,
 больший~$n_1$. Далее поступаем так же. На $k$-том шаге получаем бесконечную подпоследовательность,
 лежащую в шаре радиусом~$2^{-k}$ и выбираем из нее точку $x_{n_k}$ с наименьшим индексом, большим
  $n_{k-1}$. Так получаем последовательность $\{x_{n_k}\}_{k \in \n}$. Для каждого $k \in \n$
  все точки этой последовательности, начиная
  с $x_{n_k}$, лежат в шаре радиусом $2^{-k}$. Поэтому, данная последовательность
  фундаментальна. А так как~$M$ -- полное пространство, она имеет предел в~$M$.

   {\hfill $\Box$}
\medskip

{\tt Доказательство теоремы~\ref{th.aa}}. Достаточно доказать теорему для $d=1$. В самом деле, если мы докажем, что из последовательности функций $x_k: [a, b] \to \re^d$ можно выбрать подпоследовательность, у которой сходится
первая компонента: $x_k^{(1)}$ при $k \to \infty$, то затем из нее так же выберем подпоследовательность со сходящейся второй компонентой $x_k^{(2)}$, и т.д. В результате получим подпоследовательность вектор-функций, у которых сходятся все
$d$ координат, т.е.,  сходящуюся подпоследовательность. Итак, дано множество функций $x_k: [a, b] \to \re$, которое ограничено и равностепенно непрерывно. Будем считать, что $[a, b] = [0,1]$. Ограниченность означает, что области значений всех функций лежат на некотором отрезке
$[-R, R]$. Для любого $n \in \n$ существует $m \in \n$ такое, что если $|t_1 - t_2| < 1/m$, то
$|x(t_1) - x(t_2)| < R/n$ для любой функции $x$ из нашего множества. Это следствие равностепенной непрерывности.
Тогда множество кусочно-постоянных функций с узлами в точках $\frac{k}{m}, \, k = 0, 1, \ldots , m$, принимающих
значения $\frac{jR}{n}, \, j = -n, \ldots , n$, образуют внешнюю $\varepsilon$-сеть при $\varepsilon = \frac{2R}{n}$.
В самом деле, для любой функции $x$ нашего семейства и для любого $k$ имеем
$\bigl|x\bigl(\frac{k}{m}\bigr) - x(t)\bigr| < R/n$ при всех  $t \in \bigl[\frac{k}{m},  \frac{k+1}{m} \bigr]$.
Поэтому, взяв кусочно-постоянную функцию $y$, принимающую в каждой точке $\frac{k}{m}$ значение вида $\frac{jR}{n}$,
ближайшее к числу $x\bigl(\frac{k}{m}\bigr)$, получаем $\|x - y\| < \frac{2}{n}$. Итак, наше семейство для сколь угодно малых $\varepsilon$ имеет $\varepsilon$-сеть, значит оно компактно.

   {\hfill $\Box$}
\medskip

\begin{cor}\label{c.euler-aa}
Любая последовательность ломаных Эйлера на отрезке $[a, b]$,
лежащих в некотором компакте $Q \subset \Omega$, максимальный шаг которых
стремится к нулю, содержит подпоследовательность, сходящуюся к интегральной кривой.
\end{cor}
{\tt Доказательство.} Если $K$ -- максимум нормы функции $f$ на множестве $Q$, то
все ломаные Эйлера, лежащие в $Q$ липшицевы с константой  $K$. Следовательно, данная последовательность
ломаных Эйлера равностепенно непрерывна, а значит, в силу теоремы Арцела-Асколи, содержит сходящуюся подпоследовательность. По теореме~\ref{th.euler}, ее предельная функция является интегральной кривой.

   {\hfill $\Box$}
\medskip

\begin{center}
\textbf{2.6. Доказательство существования решений методом Пеано}
\end{center}
\medskip

В качестве второго следствия теорем~\ref{th.euler} и \ref{th.aa} мы получаем локальную теорему существования решений,
в которой от функции $f$ не требуется ничего, кроме непрерывности.
\begin{theorem}\label{th.exist}
Для каждой точки $(t_0, x_0) \in \Omega$ найдется
интервал, содержащий $t_0$, на котором уравнение $\dot x = f(t, x)$ имеет решение,
 удовлетворяющее условию $x(t_0) = x_0$.
\end{theorem}
{\tt Доказательство.} Выбираем достаточно маленькое $\delta > 0$ так, чтобы
максимум функции $\|f(t, x)\|$ на множестве
$$
Q \ = \ [t_0 - \delta, t_0 + \delta]\, \times \, \bigl\{x_0 + y \, | \, \|y\| \, \le \, \delta\, 2\|f(t_0, x_0)\| \bigr\}
$$
не превосходил $2\|f(t_0, x_0)\|$. Тогда любая ломаная Эйлера на отрезке $[t_0 - \delta, t_0 + \delta]$, выходящая из точки   $(t_0, x_0)$, является липшицевой с константой $2\|f(t_0, x_0)\|$ и лежит в $Q$. Следовательно, любая последовательность таких ломаных с максимальным шагом, стремящимся к нулю, имеет подпоследовательность, сходящуюся к интегральной кривой.

   {\hfill $\Box$}
\medskip

Метод доказательства существования решений с помощью ломаных Эйлера принадлежит итальянскому математику
Джузеппе Пеано (Giuseppe Peano, 1858--1932).
Таким образом, теорема~\ref{th.exist} обобщает теорему~\ref{th.eu-1} в части существования. Для локального существования
решения задачи Коши о функции $f$ не требуется ничего, кроме непрерывности. Однако, это решение может быть  не единственно, как показывает пример~\ref{ex.non-unique}. Теорема Осгуда, обобщающая теорему~\ref{th.eu-1} в части единственности, будет доказана позднее. Пока же мы установим еще одно следствие теорем~\ref{th.euler} и \ref{th.aa}, которая лежит в основе метода Эйлера численного решения задачи Коши.
\begin{theorem}\label{th.euler-numer}
Если через точку $(t_0, x_0)$ проходит единственная интегральная кривая на отрезке $[a, b]$, то
любая последовательность ломаных Эйлера, выходящих из этой точки, максимальный шаг которых стремится к нулю,
сходится к интегральной кривой.
\end{theorem}
{\tt Доказательство.} От противного: пусть  существует $\varepsilon > 0$ и бесконечное число данных
ломаных Эйлера, расстояние от которых до интегральной кривой~$x$ больше $\varepsilon$.
 Согласно следствию~\ref{c.euler-aa}, существует подпоследовательность этих ломаных, сходящихся к интегральной кривой,
 расстояние от которой до $x$ также не меньше $\varepsilon$. Это противоречит единственности интегральной кривой.

   {\hfill $\Box$}
\medskip


Таким образом, если мы знаем, что решение дифференциального уравнения единственно, то метод Эйлера приближения
решений сходится к настоящему решению при измельчении разбиения.

\begin{ex}\label{ex.circle}
{\em Рассмотрим уравнение $\dot x \, = \, - \frac{t}{x}$ в области $\Omega = \{(t, x) \in \re^2 \ | \ x > 0\}$.
Все интегральные кривые -- окружности  $t^2 + x^2 = C$, поэтому через каждую точку области проходит единственное решение.
Согласно теореме~\ref{th.euler-numer}, ломаные Эйлера должны сходиться к нему с измельчением разбиения.

В каждой точке $(t, x)$ имеем $t dt + x d x = 0$, а значит вектор $(d t , d x)$ перпендикулярен радиус-вектору
$(t, x)$. Поэтому, процесс построения ломаной Эйлера выглядит так: из начальной точки $(t_0, x_0)$ (пусть для определенности $t_0^2 + x_0^2 = 1$) идем в направлении
перпендикулярном радиус-вектору до дочки $(t_1, x_1)$, далее идем в направлении, перпендикулярном
радиус-вектору $(t_1, x_1)$ до точки $(t_2, x_2)$, и т.д. Согласно теореме~\ref{th.euler-numer}
полученная ломаная сходится к дуге окружности $t^2 + x^2 = 1$, выходящей из точки $(t_0, x_0)$.

}
\end{ex}
\begin{ex}\label{ex.sinus}
{\em Рассмотрим уравнение $\dot x \, = \, \sqrt{1-x^2}$ в области
$\Omega = \{(t, x) \in \re^2 \ | \ x \in [-1, 1], \ t \in \re\, \}$.
Через точку $(0, 0)$ проходит единственная интегральная кривая $x(t) = \sin t$.
Тогда все ломаные Эйлера, проходящие через эту точку, стремятся к данной кривой при
уменьшении шага разбиения. Таким образом, ломаные Эйлера, проведенные через точку $(0, 0)$
сходятся к синусоиде. С другой стороны, легко убедиться, что все ломаные Эйлера, проходящие через точку
$(0, -1)$, идут по  горизонтальной прямой $x = -1$. Поэтому ломаные Эйлера не будут стремится
к интегральной кривой $x(t) = -\cos t$, проходящей через точку $(0, -1)$. Объясняется это тем, что
через точку $(0, -1)$ проходит не единственная интегральная кривая. В этой точке нарушается условие Липшица по~$x$
функции $f(x) = \sqrt{1-x^2}$.

}
\end{ex}
\bigskip


\medskip

\begin{center}
\textbf{2.7. Теорема Осгуда о единственности}
\end{center}
\medskip

Следующий результат ослабляет условия липшицевости функции (теорема~\ref{th.eu-1}) для единственности решения.
Он получен американским математиком Вильямом Фоггом Осгудом  (1864--1943).
\begin{theorem}\label{th.osgud}[W.F.Osgood]
Если непрерывная функция $f: \Omega \to \re^d\, $ обладает свойством
 $\, \|f(t, x_1) - f(t, x_2)\| \, \le \, p(\|x_1 - x_2\|)$
для любых $(t, x_1), (t, x_2) \in \Omega$, где $p: \re_+ \to \re_+$ --  непрерывная возрастающая функция
такая, что $\int_{0}^{c} \frac{1}{p(s)}\, d s \, = \, \infty$, то через каждую точку области $\Omega$ проходит единственная интегральная кривая.
\end{theorem}
Если $f$ липшицева по $x$ то $p(s) = Ks$. Ясно, что в этом случае $\int_{0}^{c} \frac{1}{p(s)}\, d s \, = \, \infty$.
Однако, этим свойством могут обладать и функции, растущие в окрестности нуля медленнее  линейной, например $p(s) = K s |\ln s|, \ p(s) = K s \, |\ln s|\, |\ln \, \ln s|$, и т.д. Мы докажем эту теорему в случае $d=1$.
\smallskip

{\tt Доказательство.} От противного: пусть  через точку $(t_0, x_0)$ проходят две различные интегральные кривые
$x_1(t)$ и $x_2(t)$. Не ограничивая общности, считаем, что $x_0 = 0$ и что $
x_1(b) > x_2 (b)$ для некоторого $b > t_0$. Положим $z (t) = x_1(t) - x_2(t)$. Таким образом,
 $z(t_0) = 0, z(b)  > 0$.
 Обозначим через $a$ наибольшее число из отрезка $[t_0, b]$, для которого $z(a) = 0$.
 Ясно, что $t_0\le a < b$ и что $z(t) > 0$ на промежутке $(a, b]$.
 После переноса начала координат, можно считать, что $a=0$. Таким образом, $z(t)>0$
 на промежутке $(0, b]$. Поскольку  $\dot z = f(t, x_1(t)) - f(f, x_2(t))$, на отрезке $[0, b]$
 имеем $\dot z \le p(z)$. Следовательно, $\frac{dz}{p(z)} \le dt$, поэтому
 $\int_{z(t)}^{z(b)} \frac{d z}{p(z)}\, \le \, b - t $ при любом $t \in [0, b]$.
  Однако, при $t \to +0$ правая часть этого равенства стремится к $+\infty$, в то время как
  левая остается ограниченной. Противоречие.

   {\hfill $\Box$}
\medskip


\newpage

\medskip
\begin{center}
\textbf{2.8. Гладкость решений}
\end{center}
\medskip

Гладкостью функции называется ее способность локально приближаться алгебраическим полиномом в каждой точке.
Приближающий полином при этом зависит от точки, а приближение становится сколь угодно точным с уменьшением окрестности.
 Чем выше степень приближающих полиномов и чем меньше ошибка приближения, тем выше гладкость функции.

 Непрерывные функции -- это ровно те, которые приближаются в окрестности каждой точки тождественной константой.
 Приближение в окрестности точки $t$ можно записать следующим образом: $\varphi(t+h) = c + o(1), \ h \to 0$,
 где $o(h)$ -- ``о малое'' от единицы, т.е., величина, стремящаяся к нулю.
 Действительно, при $h=0$ получаем $c  = \varphi(t)$. Таким образом, $\varphi(t+h) = \varphi (t)  + o(1), \ h \to 0$,
 что совпадает с определением непрерывной функции.

Пространство $C^1$ непрерывно-дифференцируемых функций состоит из функций с непрерывной производной,
т.е., $\dot \varphi(t+h) = \dot \varphi (t)  + o(1), \ h \to 0$. Если взять первообразную и учесть начальное условие
в точке $t$, получим ${\varphi (t+h) = \varphi(t) + \dot \varphi(t)h + o(h), \ h \to 0}$.
Итак, непрерывно-дифференцируемые функции -- это те, которые в каждой точке приближаются линейной функцией
с погрешностью $o(h)$.

Причем, как мы видим, эта линейная функция обязана совпадать с разложением в ряд Тейлора в точке $t$
до первого члена. Обратно, если функция $\varphi$ приближается в каждой точке
линейной функцией, т.е.,  $\varphi(t+h) = a + bh + o(h), \, h\to 0$, то $a = \varphi (t), \, b = \dot \varphi(t)$.
А если $o(h)$ равномерна по $t$ (т.е., стремление к нулю выражения $o(h)/h$ равномерно по всем $t$), то
$\dot \varphi(t)$ непрерывна, т.е., $\varphi \in C^1$.  Аналогично, действуя по индукции, мы приходим к выводу:
\smallskip

{\em Пространство $k$ раз непрерывно-дифференцируемых функций $C^k$
состоит из функций, которые в $\varepsilon$-окрестности каждой точки приближаются многочленом степени~$k$
с точностью $o(\varepsilon^k)$.
В этом случае многочлен совпадает с многочленом Тейлора в данной точке.}
\smallskip

Самые гладкие функции -- аналитические, они в окрестности каждой точки раскладываются в сумму бесконечного степенного ряда. Точность приближения полиномами можно измерять по-разному, из чего получаются различные пространства функций:
Соболева, Бесова, Никольского и т.д. Неизменным остается то, что {\em гладкость -- это мера приближаемости полиномами}.
В качестве приближающих функций можно использовать не только алгебраические полиномы, но и тригонометрические
(получим приближения рядами Фурье), кусочно-постоянные функции (системы Хаара, Радемахера и Уолша) и т.д.
Во всех системах приближения эффект гладкости одинаковый: {\em чем больше гладкость функции, тем лучше она приближается
заданной системой функций, тем быстрее сходится приближающий е\"е ряд}. Например, чтобы приблизить $2\pi$-периодическую функцию $\varphi\in C^1\, , \ \|\varphi'\| \le 1$ рядом Фурье с точностью $\varepsilon = 10^{-3}$ понадобится порядка тысячи коэффициентов (т.е., первых членов ряда), в то время как для функции $\varphi\in C^3\, , \ \|\varphi^{(3)}\|\le 1$,
 хватит и десяти коэффициентов. Результатами о способах и точности приближения гладких функций занимается теория
 приближений.

 Как мы видим, гладкость функций исключительно важна в приложениях. Поэтому возникает естественный
 вопрос о том, насколько гладким будет решение дифференциального уравнения. Причем желательно иметь ответ на этот вопрос даже в том случае, если решение нам не известно.
 \begin{theorem}\label{th.smooth-sol}
 Если $f \in C^k(\Omega)$ при некотором $k \ge 0$, то решение уравнения $\dot x = f(t, x)$
 принадлежит $C^{k+1}[a, b]$.
 \end{theorem}
{\tt Доказательство.} Индукция по $k$. При $k=0$ получаем, что функция $f$ непрерывна по $t$ и $x$.
Функция $x(t)$ дифференцируема, а значит и непрерывна.
Но тогда и функция $\dot x (t) = f(t, x(t))$ непрерывна по $t$ как композиция непрерывных функций $x$ и $f$.
Поэтому $\dot x \in C$, и значит $x \in C^1$. Пусть теперь утверждение верно для некоторого
$k = j \ge 0$. Тогда $f \in C^j$ и $x \in C^j$, а значит $\dot x = f(t, x(t)) \, \in C^j$.
Следовательно, $x \in C^{j+1}$.

   {\hfill $\Box$}
\medskip


\medskip
\begin{center}
\textbf{2.9. Зависимость  решений от правой части, \\
от начальных условий и от параметров}
\end{center}
\medskip


\begin{ex}\label{ex.discont}
{\em Дифференциальное уравнение $\dot x = \sqrt{x}$ с начальным условием $x(0) = 0$
имеет очевидное решение $x\equiv 0$. Предположим, мы сделали ошибку и незначительно изменили правую часть:
вместо данного уравнения стали решать уравнение $\dot x = \sqrt{x + \varepsilon}$, где $\varepsilon > 0$ -- маленькое число. Такие ситуации встречаются в приложениях постоянно, поскольку численные ошибки неизбежны даже в самых точных
расчетах. Можно ли надеяться, что решение $x_{\varepsilon}$ будет  близко к решению $x$ исходного уравнения~?
 Решая уравнение с разделяющимися переменным, получаем, что при каждом $\varepsilon > 0$ решение единственно:
 $x_{\varepsilon}(t) = \frac{1}{4}\, t^2 \, +\, \sqrt{\varepsilon}\, t$. При $\varepsilon \to 0$ оно стремится к
 $x = \frac{1}{4}\, t^2$, а вовсе не к $x = 0$. Таким образом, даже если $\varepsilon > 0$ пренебрежимо мало,
 например $\varepsilon = 10^{-30}$, полученное решение $x_{\varepsilon}(t) = \frac{1}{4}\, t^2 \, +\, 10^{-15}\, t$
 не будет иметь ничего общего с $x = 0$. Итак, ошибка в тридцатом знаке привела к кардинальному изменению решения.
 Для данного уравнения ситуация естественна, оно имеет не единственное решение. Функции $x_{\varepsilon}$
 сходятся к другому решению $x = t^2/4$.}
\end{ex}
А если дополнительно потребовать, что уравнение имеет единственное решение, можно ли утверждать, что оно непрерывно
зависит от начальных условий и от правой части? Это значит, что для любого $\varepsilon > 0$ должно найтись такое
$\delta > 0$, что для любой непрерывной функции $\tilde f : \Omega \to \re$ и точки $(\tilde t_{0}, x_{0}) \in\
\Omega$ таких, что $\|\tilde f - f\|_{\Omega} < \delta$ и $|\tilde t_{0} - t_0| < \delta$ и
$\|x_{\delta , 0} - x_0\| < \delta$
имеем $\|\tilde x  - x\|_{[a, b]} < \varepsilon$. Утвердительный ответ дает следующая
\begin{theorem}\label{th.cont-cond}
Если через каждую точку области $\Omega$ проходит единственная интегральная кривая, то она непрерывно зависит от правой части $f$ и от начальной точки~$(t_0, x_0)$.
\end{theorem}
{\tt Доказательство.} Обозначим $M = \|f(t_0, x_0)\|$ и
возьмем отрезок $[\alpha , \beta]$ с центром $t_0$ столь малой длины $2\delta$, что
максимум величины $\|f(t, x)\|$ на цилиндре  $P = [\alpha , \beta ]\times \{x \in \re^d \ | \
\|x - x_0\| < 2 \delta M\}$ меньше $2M$. Докажем сначала теорему для отрезка $[\alpha, \beta]$.
От противного: пусть  нашлась последовательность функций $f_k$ и точек $(t_{0, k}, x_{0, k})$,
стремящихся к $f$ и $(t_0, x_0)$ соответственно, для которых $\|x_k - x\|_{[\alpha, \beta]}\,  > \, \varepsilon$ при всех $k$,
где $x_k$ -- решение уравнения $\dot x = f_k(t, x)$ на $[\alpha, \beta]$ с начальным  условием $x(t_{0, k}) = x_{0, k}$.
Для достаточно больших $k$, скажем, $k \ge N$ для некоторого $N$,
  имеем $\max_{(t, x) \in Q} \|f_k(t, x)\| < 2M$. По теореме~\ref{th.segment},
функция $x_k$ определена на всем отрезке $[\alpha, \beta]$ и ее график лежит в $P$.
Более того, поскольку $x_k$ липшицева с константой $2M$ на множестве $P$, все семейство $\{x_k\}_{k \ge N}$
равностепенно непрерывно на $[\alpha, \beta]$, а значит, по теореме~\ref{th.aa}, компактно.
Следовательно, оно содержит подпоследовательность, сходящуюся к некоторой функции $\bar x$.
Переходя в уравнении $x_k(t) = x_{0, k} + \int_{t_{0, k}}^t f_k(s, x_k(s))\, ds$
к пределу по этой подпоследовательности, получим, что $\bar x$ -- решение уравнения
$x(t) = x_0 + \int_{t_0}^t f(s, x(s))ds$, т.е., является интегральной кривой.
Значит, $\bar x = x$, что противоречит предположению.

Итак, решение $x$ непрерывно зависит от правой части и от начальных условий  на отрезке $[\alpha, \beta]$.
Заметим, что длина этого отрезка $2\delta$ может быть взята одной и той же для всех точек отрезка $t_0 \in [a, b]$.
Разбив весь отрезок $[a, b]$ на отрезки длиной меньше $2\delta$, получим, что оно непрерывно зависит и на всем отрезке $[a, b]$.



   {\hfill $\Box$}
\medskip

Предположим теперь, что правая часть уравнения непрерывно зависит от нескольких параметров $\mu_1, \ldots , \mu_m$,
т.е.,  $f = f(t, x, \mu)$, где $\mu = (\mu_1, \ldots , \mu_m) \in \re^m$. По теореме~\ref{th.cont-cond}
решение уравнения непрерывно зависит от $\mu$. Оказывается, что если $f$ дифференцируема по этому параметру, то
то же верно и для решения.
\begin{theorem}\label{th.diff-param}
Если функция $f(t, x, \mu)$ зависит от параметра $\mu = (\mu_1, \ldots , \mu_m) \in \re^m$ и непрерывно дифференцируема по нему $p$ раз,
причем все ее производные липшицевы по $x$ то решение $x = f(t, \mu)$ дифференцируемо столько же раз
по $\mu$, причем каждая ее частная производная $x_{\mu_{j_1}\, \ldots \, \mu_{j_q}}$ порядка  $q \le p$ по параметру $\mu$ -- есть решение уравнения $\dot x  = f_{\mu_{j_1} \, \ldots \, \mu_{j_q}}(t, x)$.
\end{theorem}


\bigskip

\newpage

\begin{center}
\large{\textbf{3. Примеры и приложения}}
\end{center}

\bigskip

\medskip
\begin{center}
\textbf{3.1. Математическая экологии и популяционная динамика.\\
Модели роста популяции}
\end{center}
\medskip



\textbf{1. Рост популяции при обычных условиях.} В простейшей модели роста популяции биологического вида, число особей $x$ в момент времени $t$
подчиняется закону: $\dot x  = \alpha x$, где $\alpha > 0$ -- коэффициент воспроизводства вида (рождаемость минус смертность).
Тогда численность популяции дается формулой: $x(t) = x_0 e^{\alpha (t-t_0)}$. На практике, однако, бесконтрольному увеличению численности мешает конкуренция с другими видами и ограниченность ресурсов (например, пищи). Поэтому правая часть уравнения $\alpha x$
умножается на некоторую убывающую функцию от~$x$. В простейшей модели мы считаем эту функцию аффинной со свободным коэффициентом~$1$ (этот коэффициент может корректироваться выбором~$\alpha$). Таким образом,
\begin{equation}\label{eq.popul1}
\dot x \ = \ \alpha \, x\, (1 \, - \, k\, x)\, , \qquad x(0) = x_0\, .
\end{equation}
Это -- уравнение с разделяющимися переменными:
$$
\frac{dx}{\alpha \, x\, (1 \, - \, k\, x)} \ = \ d\, t \ \Rightarrow \ \int \frac{dx}{x\, (1 \, - \, k\, x)} \ = \ \alpha \, (t\, + \, C)\, ,
$$
откуда, учитывая начальное условие, получаем:
\begin{equation}\label{eq.sol-popul1}
x(t)\ = \ \frac{x_0}{kx_0 \, + \, (1-kx_0)\, e^{-\alpha t}}\, .
\end{equation}
Если $x_0 < 1/k$, то численность популяции растет монотонно от $x_0$ и приближается асимптотически к
величине $1/k$. Если же $x_0 > 1/k$, то численность популяции монотонно убывает от $x_0$ и приближается асимптотически к
величине $1/k$ при $t \to \infty$. При $x_0 = 1/k$ численность постоянна: $x(t) \equiv x_0$. Таким образом,
уравнение имеет устойчивое стационарное значение $x = 1/k$. Оно тем больше, чем меньше коэффициент $k$.

\begin{defi}\label{d.stability}
Стационарной точкой уравнения $\dot x = f(t, x)$ называется тождественная константа $x(t)\equiv \bar x$, являющаяся его решением. Она называется (локально) {\em асимптотически устойчивой}, если существует $\varepsilon > 0$ такое, что
$x(t) \to \bar x$ при $t \to \infty$, если $\|x(t_0) - \bar x\| \to 0$. Она называется {\em глобально асимптотически устойчивой}
если для любого решения $x(t) \to \bar x$ при $t \to \infty$.
\end{defi}

Наконец, если речь идет о промысловом звере (или рыбе), то на него устанавливается квота отстрела (или отлова) $q$ особей за единицу времени. Тогда уравнение численности популяции принимает вид:
\begin{equation}\label{eq.popul2}
\dot x \ = \ \alpha \, x\, (1 \, - \, k\, x)\ - \ q\, , \qquad x(0) = x_0\, .
\end{equation}
Это уравнение также имеет разделяющиеся переменные:
$$
\frac{dx}{\alpha \, x \, - \, \alpha k\, x^2 \, - \, q} \ = \ d\, t \qquad  \Rightarrow \qquad
\int \frac{dx}{x \, - \, k x^2\, - \, q/\alpha} \ = \ \alpha (t\, + \, C)\, ,
$$
{\em Случай 1}. $q \le  \frac{\alpha}{4k}$. В этом случае уравнение  $x \, - \, k x^2\, - \, q/\alpha$
имеет два действительных корня $x_1, x_2$. Пусть $x_2 \ge x_1$. Тогда
\begin{equation}\label{eq.sol-popul2}
\frac{1}{k(x_2-x_1)}\, \ln \, \left|\frac{x - x_1}{x_2-x} \right|\ = \ \alpha (t\, + \, C)\, ,
\end{equation}
где $C$ находится из начального условия.  При $t\to \infty$ получаем $x(t)\to x_2$.
Таким образом, данное уравнение также имеет стационарную предельную точку
$x_2 \, = \, \frac{1 + \sqrt{1 - \frac{4kq}{\alpha}}}{2k}$. Если отлова не происходит ($q=0$), мы получаем
$x_2 = \frac1k$, т.е., численность популяции, стремится к $\frac1k$. При увеличении квоты $x_2$ уменьшается до $\frac{1}{2k}$. При при максимальной квоте
$q = \frac{\alpha}{4k}$ дискриминант обращается в ноль,  и предельная численность популяции
равна $\frac{1}{2k}$, т.е., вдвое меньше, чем без отлова.
\smallskip

{\em Случай 2}. $q > \frac{\alpha}{4k}$. В этом случае существует положительная константа $C$, для которой
$x \, - \, x^2\, - \, q/\alpha \, < \, - C$ при всех $x$. Поэтому $\dot x (t) < - \alpha C$ при всех $t$.
Следовательно, за время, не превосходящее $T = \frac{x_0}{\alpha C}$ популяция полностью исчезнет. Итак,
\smallskip

{\em при отсутствии отлова численность популяции всегда стремится к $1/k$. Таким образом $x=1/k$ -- асимптотически устойчивая стационарная точка. При наличии квоты отлова $q \le  \frac{\alpha}{4k}$ численность популяции стремится к
$\frac{1 + \sqrt{1 - \frac{4kq}{\alpha}}}{2k}$. Максимальная квота равна $q = \frac{\alpha}{4k}$, при ней
численность популяции стремится к $1/2k$. Если квота превосходит это число, популяция исчезает за конечное время.}
\medskip

\textbf{2. Рост популяции, не образующей устойчивых пар (семей).} Если
животные определенного вида не образуют устойчивых пар, то
рождаемость пропорциональна не численности популяции, а числу встреч разнополых
особей, т.е., квадрату численности популяции. Таким образом, рождаемость за единицу времени равна $Cx^2$,
где $C$ -- положительная и, как правило, маленькая константа. Смертность же по-прежнему пропорциональна $x$ и
равна $\beta x$ за единицу времени.
Мы также предполагаем, что
популяция малочисленна и ореол ее обитания велик, поэтому другие факторы (наличие пищи и т.д.)
  не оказывают существенного влияния на численность.
Таким образом, получаем уравнение $\dot x = Cx^2 - \beta x$. Оно хорошо описывает динамику редкой популяции в случае когда особи не образуют устойчивых пар (``семей''). Так происходит, например, у китов, некоторых видов полярных животных, и т.д.
 Для того, чтобы провести аналогии с предыдущей моделью, мы
введем обозначения $k = C/\beta, \, \alpha = \beta$. Тогда
\begin{equation}\label{eq.popul3}
\dot x \ = \ - \, \alpha \, x\, (1 \, - \, k\, x)\, , \qquad x(0) = x_0\, .
\end{equation}
Это уравнение отличается от~(\ref{eq.popul1}) лишь знаком правой части. Поэтому и решение его выглядит аналогично,
с заменой $t$ на $-t$:
\begin{equation}\label{eq.sol-popul3}
x(t)\ = \ \frac{x_0}{kx_0 \, + \, (1-kx_0)\, e^{\alpha t}}\,
\end{equation}
Мы видим, что вновь уравнение имеет стационарную точку $x = 1/k$. Если $x_0 = 1/k$, то
функция $x(t)$ постоянна. Однако, в отличие от уравнения~(\ref{eq.popul1}) эта точка неустойчива.
При любом $x_0 < 1/k$ численность популяции стремится к нулю. Причем происходит это достаточно быстро.
Если, например, коэффициент смертности $\alpha = 0.03$ (т.е., за год три особи из ста умирают. Как правило, этот  коэффициент выше), то даже при
малом отклонении от стационарного значения $x_0 = 0.999\, \frac1k$ мы получим
$x(400) < 0.01 x_0$. Таким образом, за 400 лет численность популяции уменьшится в сто раз,
что на практике означает вымирание.

Если же $x_0 > 1/k$, то при $t \, = \, \frac{1}{\alpha}\, \ln \frac{kx_0}{kx_0 - 1}$
знаменатель дроби обратится в ноль, т.е., произойдет демографический взрыв: численность популяции вырастет до $+\infty$ за конечное время. Например, при $\alpha = 0.03, \,  x_0 = 1.001\, \frac1k$ демографический взрыв произойдет через
$t = 230$ лет.
\smallskip

Таким образом, {\em биологические виды, не образующие устойчивых пар, имеют нестабильную динамику численности.
Они подвержены рискам демографического взрыва или быстрого вымирания.}
\bigskip

\begin{center}
\textbf{3.2. Модель Лотки-Вольтерры экологического равновесия \\
(``модель хищник-жертва'')}
\end{center}
\medskip

Конечно, биологические виды существует не сами по себе, они постоянно взаимодействуют друг с другом.
Одна из первых моделей сосуществования  нескольких видов была предложена в 1925 г. Вито Волтеррой (Vito Volterra, 1860--1940, Италия) и в 1926 г.
Альфредом Лоткой (Alfred Lotka, 1880--1949, Польша-США). Ее используют не только в математической экологии и
популяционной динамике, но и в математической экономике (изучение конкуренции) и даже в химии
(уравнения некоторых химических реакций).

Мы предполагаем, что в экологической системе живет один вид хищных и один вид травоядных животных.
В момент времени $t$ количество травоядных равно $x(t)$, а хищных равно $y(t)$.
Если бы не было хищников, то травоядные животные размножались бы по закону
$\dot x \, = \, \alpha x$, $\alpha > 0$. Присутствие хищников уменьшает их число пропорционально
числу встреч между хищниками и жертвами, т.е., пропорционально произведению $xy$.
Таким образом, $\dot x = \alpha x - \beta xy$.

Если бы не было травоядных, то хищники вымерли бы по закону
$\dot y \, = \, - \gamma x$, $\gamma > 0$. Присутствие травоядных  увеличивает их число пропорционально
числу встреч, т.е., пропорционально произведению $xy$.
Таким образом, $\dot y = - \gamma y + \delta xy$.

Итак, получаем уравнение экологического балланса Лотки-Вольтерры:
\begin{equation}\label{eq.lv}
\left\{
\begin{array}{l}
\dot x \ = \ \ \alpha x \, - \, \beta xy\\
\dot y \ = \ - \gamma y \, + \, \delta xy
\end{array}
\right.
\end{equation}
где $\alpha, \beta, \gamma, \delta$ -- положительные параметры.

Для решения сделаем замену $p = \ln x, \, q = \ln y$, возможную в силу положительности
$x$ и $y$. Разделив первое уравнение на $x$ и заметив, что $\frac{\dot x}{x} = \dot p$,
получаем $\dot p = \alpha - \beta e^q$. Преобразовав аналогично второе уравнение, приходим к системе
  $$
\left\{
\begin{array}{l}
\dot p \ = \  \ \alpha \,  - \, \beta e^q\\
\dot q \ = \ - \gamma  \, + \, \delta e^p\, .
\end{array}
\right.
$$
Дифференцируем первое уравнение: $\ddot p \, = \, - \dot q \, \beta e^q$.
Подставив $\beta e^q = \alpha - \dot p$ из первого уравнения и
$\dot q \, = \, - \gamma   +  \delta e^p$ из второго, приходим к уравнению:
$$
\ddot p \ = \ (\gamma   -  \delta e^p)\, (\alpha - \dot p)\, .
$$
Это уравнение автономно (не зависит явно от $t$) поэтому заменяем $v(p) = \dot p $.
Тогда $\ddot p \, = \, v_p \dot p \, = \, \dot v \, v$, где мы обозначаем $\dot v = v_p$.
Итак,
$$
\frac{v\, d v}{\alpha - v}\ = \ \bigl(\gamma - \delta e^p \bigr) \, d p\, .
$$
Интегрируем: $ - v \, - \, \alpha\, \ln |\alpha - v|\ = \ \gamma p \, - \, \delta e^p \, + \, C$.
Подставляя $p = \dot x, \, v = \frac{\dot x}{x}$, получаем
$$
\frac{\dot x}{x} \ + \ \, \alpha\, \ln \, \left| \alpha \, - \, \frac{\dot x}{x}\right| \quad = \quad
\delta x\, - \, \gamma \, \ln x \, - \, C.
$$
Таким образом, уравнение Лотки-Вольтерры сводится к уравнению первого порядка. По теоремы о неявной функции,
данное уравнение можно локально разрешить относительно~$\dot x$, т.е., $\dot x = f(x)$, что приводит к уравнению с разделяющимися переменными. Обозначим $ f(\dot x) \, = \, \dot p \, + \, \alpha\, \ln (\alpha - \dot p)$ и
$g(x) \, = \, \delta x\, - \, \gamma \, \ln x \, - \, C.$ Получаем уравнение $f(\dot p) = g(x)$.
Заметим, что $f$ -- строго убывающая функция, поэтому определена обратная функция $f^{-1}$. Таким образом,
$$
\frac{\dot x}{x}\, = \, \dot p\ = \ f^{-1} \bigl(g(x)\bigr),
$$
откуда
$$
\frac{d x}{x \, f^{-1} \bigl(g(x)\bigr)}\ = \ dt,
$$
откуда получаем решение.
\medskip

Мы вывели явную форму решения уравнения
Лотки-Вольтерры. Из нее, однако, трудно углядеть структуру решения. Будут ли $x(t)$ и $y(t)$ неограниченно расти при $t \to \infty$ ?  Или, напротив, при каких-то начальных условиях, обе эти величины стремятся к нулю, т.е.,
обе популяции вымирают~? Или эти функции периодические, т.е., система из двух биологических видов развивается циклически?
Для ответа на эти вопросы, сделаем несколько несложных наблюдений.

Во-первых, система~(\ref{eq.lv}) имеет очевидную стационарную точку $(\bar x, \bar y) = \bigl(\frac{\gamma}{\delta}\, , \, \frac{\alpha}{\beta} \bigr)$. В самом деле, пара функций $x(t)\equiv \bar x, \, y(t) \equiv \bar y$ является решением системы. Является ли оно глобально асимптотически устойчивым, т.е., верно ли, что любое решение стремится к стационарному при $t \to \infty$ ?
Вполне естественно перенести начало координат в стационарную точку. Положим $x = \bar x + x_1, \, y = \bar y + y_1$.
Подставляя в уравнение~(\ref{eq.lv}), получим:
$$
\left\{
\begin{array}{cclcl}
\dot x_1 & = & \ \alpha \, \bigl(\frac{\gamma}{\delta} + x_1\bigr) \ - \ \beta \,
\bigl(\frac{\gamma}{\delta} + x_1\bigr)\, \bigl(\frac{\alpha}{\beta} + y_1\bigr) & = &
- \frac{\beta \gamma}{\delta}\, y_1  \, - \, \beta x_1 y_1\\
\dot y_1 & = & - \gamma \, \bigl(\frac{\alpha}{\beta} + y_1\bigr) \ + \ \delta \,
\bigl(\frac{\gamma}{\delta} + x_1\bigr)\, \bigl(\frac{\alpha}{\beta} + y_1\bigr) & = &
 \frac{\alpha \delta}{\beta}\, x_1  \, - \, \delta x_1 y_1
\end{array}
\right.
$$
Полученная система уравнений
$$
\left\{
\begin{array}{ccl}
\dot x_1 & = &  - \frac{\beta \gamma}{\delta}\, y_1  \, - \, \beta x_1 y_1\\
\dot y_1 & = & \ \frac{\alpha \delta}{\beta}\, x_1  \, - \, \delta x_1 y_1
\end{array}
\right.
$$
 равносильна исходной системе~(\ref{eq.lv}) и ничуть не проще ее.
Поэтому мы сделаем {\em линеаризацию}, т.е., заменим систему на близкую линейную систему.
Если $x_1$ и $y_1$ малы, то $x_1y_1 = o(x_1) = o(y_1)$. Поэтому, при $x_1 , y_1 \to 0$, наша система близка к линейной
системе:
\begin{equation}\label{eq.lv-lin}
\left\{
\begin{array}{ccl}
\dot x_1 & = & - \frac{\beta \gamma }{\delta}\, y_1\\
\dot y_1 & = &  \ \ \frac{\alpha \delta }{\beta}\, x_1
\end{array}
\right.
\end{equation}
Имеем $\ddot x - \alpha \gamma \, x$, откуда $\, x_1 = A \cos \bigl(\omega t + \varphi \bigr)$ и
следовательно $y_1 = \frac{\sqrt{\alpha}\delta}{\sqrt{\gamma} \beta}\, A \sin \bigl(\omega t + \varphi \bigr)$,
где $\omega = \sqrt{\alpha \gamma}$. Обозначив $A = C\frac{\beta}{\sqrt{\alpha}}$, приходим к общему виду решения линеаризованной системы:
\begin{equation}\label{eq.lv-lin-sol}
\left\{
\begin{array}{ccl}
x_1 & = & C\frac{\beta}{\sqrt{\alpha}}\, \cos \, \bigl(\omega \, t \, + \, \varphi \bigr)\\
y_1 & = &  C\frac{\delta}{\sqrt{\gamma}}\, \sin \, \bigl(\omega \, t \, + \, \varphi \bigr)
\end{array}
\right.
\end{equation}
Таким, образом, решение $(x_1, y_1)$ описывает эллипс $\frac{\alpha x_1^2}{\beta^2} + \frac{\gamma x_1^2}{\delta^2} = C$
с полуосями $C\frac{\beta}{\sqrt{\alpha}}$ и $C\frac{\delta}{\sqrt{\gamma}}$.
При различных $C \in [0, +\infty)$ получаем множество соосных эллипсов с общим центром в стационарной
точке $(\bar x, \bar y)$. Итак, решения линеаризованной системы являются периодическими с периодом
$T = \frac{2\pi}{\sqrt{\alpha \gamma}}$. Решение не стремится к стационарной точке, но и не уходит от нее далеко,
двигаясь циклически вокруг по эллипсу. Малое шевеление такой системы может привести как к системе, у которой траектория  стремиться к стационарной точке, так и системе с траекториями, стремящимися к бесконечности. Поэтому, из нашей линеаризации, к сожалению, нельзя сделать выводов о том, как ведет себя решение
исходной системы.

Поступим по-другому. Разделив первое уравнение системы~(\ref{eq.lv}) на второе, получим:
$$
\frac{d x}{d y}\ =  \  \frac{x (\alpha - \beta y)}{y (-\gamma + \delta x)}\, .
$$
Разделяем переменные:
$$
\frac{(-\gamma + \delta x)\, d x}{x}\ =  \  \frac{ (\alpha - \beta y)\, d y}{y}\,
$$
и интегрируем:
$$
\delta x \, - \,  \gamma \ln x \ = \ - \beta y \, +\, \alpha \ln y \ + \ C\, .
$$
Положив $p(x) = \delta x \, - \,  \gamma \ln x$ и $q(y) = \beta y \, -\, \alpha \ln y$,
получим окончательно $p(x) + q(y) = C$. Заметим, что обе функции $p, q$ -- выпуклы и стремятся к $+\infty$ на концах
промежутка $(0, +\infty)$, поэтому и
функция $p(x) + q(y)$ выпукла и стремится к $+\infty$ на границе положительного квадранта. Значит, все е\"е линии уровня
 $p(x) + q(y) = C$ -- ограниченные выпуклые замкнутые кривые. Итак, {\em все решения уравнения Лотки-Вольтерры периодичны. Эволюция биологической системы происходит по циклу вокруг стационарной точки. Длина цикла (период) зависит от
 параметров системы. Если амлитуда цикла мала, т.е., траектория вращается близко к стационарной точке,
 то период близок к $T = \frac{2\pi}{\sqrt{\alpha \gamma}}$}.

\bigskip

\begin{center}
\textbf{3.3. Уравнение химической реакции}
\end{center}
\smallskip

Уравнение химических реакций имеют много общего с уравнениями популяционной динамики, поскольку описывают взаимодействие
молекул различных веществ, по аналогии с особями различных  видов. Предположим, мы перемешали два вещества, которые начали взаимодействие друг с другом. $x(t)$ и $y(t)$ -- количества этих веществ (измеряемое числом молей) к моменту времени~$t$.
При взаимодействии образуются другие вещества, поэтому $x(t)$ и $y(t)$ убывают. Скорость реакции
пропорциональна числу ``встреч'' молекул разных веществ, т.е., произведению $xy$.
Получаем уравнение реакции:
\begin{equation}\label{eq.chem1}
\left\{
\begin{array}{ccc}
\dot x \ & = & \ - a \, xy \\
\dot x \ & = & \ - b \, xy
\end{array}
\ , \qquad x(0) = x_0, \, y(0) = y_0\, .
\right.
\end{equation}
Коэффициенты $a, b > 0$ зависит от числа молекул данных веществ, вступающих в реакцию.
Соотношение $a/b$ легко находится из химической формулы реакции.
Умножив первое уравнение на $b$,  второе на $a$ и произведя вычитание,
получим $b \dot x \, - \, a \dot y \, = \, 0$, или $bx(t) - ay(t) = C$,
причем константа $C$ находится из начальных условий: $C = bx_0 - ay_0$.
Мы предполагаем, что $C \ge 0$.
Выражая $y(t) = \frac{b x (t) - C}{a}$ и подставляя в первое уравнение системы~(\ref{eq.chem1}),
получаем
$$
\dot x \ = \ C x \, - \, b x^2\, , \qquad x(0) = x_0\, .
$$
Уравнение совпадает с уравнением численности популяции~(\ref{eq.popul1}) с параметрами
$\alpha = C, k = b/C$ ! Воспользуемся полученной формулой решения~(\ref{eq.sol-popul1}):
\begin{equation}\label{eq.sol-chem1}
\left\{
\begin{array}{l}
x(t)\ = \ \frac{x_0C}{bx_0 \, - \, ay_0\, e^{-\alpha t}}\, \\
y(t)\ = \ \frac{y_0Ce^{-\alpha t}}{bx_0 \, - \, ax_0\, e^{-\alpha t}}
\end{array}
\right.
\end{equation}
\bigskip

\begin{center}
\textbf{3.4. Приложения к математической экономике. Влияние рекламы}
\end{center}

\bigskip

Распространение знания о товаре среди потребителей описывается следующим дифференциальным уравнением.
Если $N$ -- общее число потребителей, и $x(t)$ -- количество тех из них, которые знают о товаре,
то $\dot x$ пропорционально количеству встреч знающих с незнающими. Таким образом,
\begin{equation}\label{eq.rec1}
\dot x \ = \ a \, x\, (N \, - \,  x)\, , \qquad x(0) = x_0\, .
\end{equation}
Заменив $aN = \alpha, \, k = \frac{1}{N}$, приходим к уравнению~(\ref{eq.popul1}).
Воспользовавшись формулой решения~(\ref{eq.sol-popul1}), получаем
\begin{equation}\label{eq.sol-rec1}
x(t)\ = \ \frac{x_0N}{x_0 \, + \, (N-x_0)\, e^{- a N t}}\, .
\end{equation}
Таким образом, $x(t) \to N$ при любых начальных условиях. На практике влияние рекламы
уменьшается со временем, так как потребитель о ней забывает. Поэтому влияние рекламы
измеряется величиной $e^{-\beta t}x(t)$. Продать товар нужно, когда это влияние максимально.
Из условия максимума: $(e^{-\beta t} x)' = 0\ \Leftrightarrow \
\dot x \, - \, \beta x = 0$. Поскольку $\dot x = a x(N-x)$,
получаем $\beta x \, = \,  a x(N-x)$, откуда $x(t) = N - \frac{\beta}{a}$.
Подставляя $x(t)$ из уравнения~(\ref{eq.sol-rec1}), получаем момент $t$ продажи товара.

\bigskip

\begin{center}
\textbf{3.5. Модель Нерлофа-Эрроу расходов на рекламу}
\end{center}

\bigskip


Эта модель является одной из наиболее популярных в настоящее время. Она предложена в 1962 г.
американскими экономистами М.Нерловом (Mark Leon Nerlove, 1933) и К.Д.Эрроу (Kenneth Joseph Arrow, 1921).
Е\"е главная идея заключается в том, что
предполагается существование некоторого виртуального капитала, измеряемого в рублях, и называемого ``покупательской расположенностью''
(consumer's goodwill). Чем он больше, тем больше продажи товара. Увеличить его можно с помощью рекламы: сколько денег вложено в рекламу, но столько возрастает и расположенность. С другой стороны, капитал расположенности обесценивается со временем, поскольку покупатель забывает о данном товаре. Таким образом, величина покупательской
расположенности $x(t)$ удовлетворяет уравнению:
\begin{equation}\label{eq.ner}
\dot x(t)\ = \ u(t) \ - \  \beta \, x(t)\, ,
\end{equation}
где $\beta$ -- фактор обесценивания, $u(t)$ -- количество средств, вложенных в рекламу в момент времени $t$
(мгновенная скорость вложения средств). Прибыть от продажи  товара вычисляется с помощью
функции полезности (utulity function) $\psi(x)$ -- прибыли, полученной от продажи товара при начении покупательской расположенности~$x$. Как правило, предполагается, что функции полезности
дифференцируемая, возрастающая, $\psi (0) = 0$ (эти предположения вполне естественны),
кроме того $\psi$ -- вогнутая. Последнее предположение объясняется тем, что вложив вдвое больше средств,
мы не получим вдвое больше прибыли, т.к., возрастают расходы и фактор конкуренции.  Таким образом, функция полезности имеет рост меньше линейной функции, т.е.,
производная функции полезности уменьшается с возрастанием~$x$, поэтому $\psi$ -- вогнутая функция.
Иногда делается дополнительное предположение $\dot \psi(0) = +\infty$.

Функция полезности предполагается известной. Тогда прибыль $P$ за время от нуля до $T$ дается формулой:
\begin{equation}\label{eq.ner-profit}
P(T) \ = \ \int_{0}^T \, \left( \, \psi (x(t))\,  \, - \, u(t) \, \right)\, d \, t \
\end{equation}
(из чистой прибыли $\int_{0}^T \,  \psi (x(t))\, d t$ вычитаем расходы а на рекламу
$\int_{0}^T \,  u(t) \, d t$). Функцию расположенности $x(t)$ можно найти из функции вложений~$u(t)$, решив
линейное дифференциальное уравнение~(\ref{eq.ner}) методом вариации постоянной.
Для однородного уравнения $\dot x = - \beta \, x$ имеем $x(t) = C e^{\, -\beta t}$.
После вариации постоянной $C = C(t)$, подставляя в уравнение,получим:
$\dot C \, = \, e^{\,\beta t} u(t)$, откуда
$$
x(t)\ = \ e^{\,- \beta t} \, \left(\, x_0 \ + \ \int_{0}^t u(\tau) e^{\, \beta \tau} \, d\tau \right)  .
$$
Подставляя это выражение для $x(t)$ в формулу прибыли, получаем выражение для прибыли $P$
от функции вложений в рекламу $u(t)$. Множество задач состоит  в нахождении оптимальной стратегии вложений,
т.е., функции $u(t)$, для которой при данных условиях прибыль~$P(T)$ максимальна.




\bigskip

\newpage

\begin{center}
\large{\textbf{4. Линейные уравнения и системы}}
\end{center}

\bigskip

\begin{center}
\textbf{4.1. Линейные системы с переменными коэффициентами}
\end{center}

\bigskip

Дифференциальное уравнение называется линейным, если правая часть является линейной
(точнее, аффинной) функцией переменного $x$, т.е.,  $f(x, t) = A(t)x + g(t)$,
где $A(t)$ -- $d\times d$ матрица, непрерывно зависящая от~$t$, $g: [a, b]\to \re^d$ -- непрерывная функция.
Итак,
\begin{equation}\label{eq.lin}
\dot x(t) \ = \ A(t)x(t) \ + \ g(t)\, .
\end{equation}
Изучение линейных уравнений начнем с нескольких несложных наблюдений, но сначала условимся в обозначениях.
Координаты вектора $x$ будем обозначать $x^1, \ldots , x^d$ (верхние  индексы). Разные векторы
будeм обозначать нижними  индексами: $x_1, \ldots , x_s \in \re^d$. Через $(x_1, \ldots , x_d)$ обозначаем матрицу, составленную из столбцов $x_1, \ldots , x_d$. Как всегда, $\det A$ -- определитель матрицы
$A = (a_{ij})$, ${\rm tr}\, A$ -- ее след, т.е., сумма диагональных элементов, или сумма собственных значений,
${\rm sp} (A) = \{\lambda_1, \ldots , \lambda_d\}$ -- спектр матрицы, т.е., множество собственных значений, перечисленных
с учетом кратностей.

Так как $\|A(x_1  -x_2)\| \le \|A\| \, \|x_1 - x_2\|$, то на любом компакте $Q\subset \re^{d+1}$
функция $f(t, x)$ липшицева по  $x$ с константой $K = \max\limits_{(t, x)\in Q} \|A(t)\|$. Поэтому, {\em для линейного
уравнения выполнены все теоремы о существовании, единственности и продолжения решений.}

Если $x_1(t)$ и $x_2(t)$ -- решения уравнения~(\ref{eq.lin}), то их разность $x = x_1 - x_2$ -- решение
соответствующего {\em однородного линейного уравнения}:
\begin{equation}\label{eq.linh}
\dot x(t) \ = \ A(t)x(t) \, .
\end{equation}
Обратно, если $y$ -- решение уравнения~~(\ref{eq.lin}), то для любого решения
$x$ однородного уравнения, сумма $y+x$ -- решение уравнения~(\ref{eq.lin}).
Таким образом, все решения неоднородного линейного уравнения имеют вид суммы одного
частного решения и произвольного решения однородного уравнения. Поэтому мы начнем с изучения
однородных уравнений~(\ref{eq.linh}).

Если $x_1(t), x_2(t)$ -- два решения однородного уравнения, то для любых чисел $\alpha_1, \alpha_2$
функция $\alpha_1 x_1 (t) + \alpha_2 x_2 (t)$ -- также решение. Следовательно,
{\em решения однородного линейного уравнения образуют линейное пространство}.
Для того, чтобы выяснить размерность и свойства этого пространства нам понадобится
понятие определителя Вронского.
\begin{defi}\label{d.wron}
Для любых $d$ решений $x_1, \ldots, x_d$ уравнения $\dot x = A(t) x$
величина  \linebreak $W(t) \, = \, \det \, \bigl(x_1(t), \ldots , x_d(t)\bigr)$ называется
определителем Вронского, или вронскианом.
\end{defi}
Определитель матрицы, составленной из решений линейного дифференциального уравнения
был впервые исследован в 1812 г.
польско-французским математиком и философом-мистиком Юзефом Мария Вронским (J\'ozef Maria Hoene-Wro\'nski, 1776 -- 1853).
\begin{theorem}\label{th.w}
Для любого $t$ имеем $\, W(t) \ = \ W(t_0) \ e^{\, \int_{t_0}^t \, {\rm tr}(A(\tau))\, d \tau}$.
\end{theorem}
Для доказательства нужно дифференцировать определитель матрицы.
\begin{lemma}\label{l.ddet}
Для любой матрицы $X(t) = (x_{ij})$, дифференцируемой по~$t$,  имеем
\begin{equation}\label{eq.ddet}
\frac{d}{dt}\, \det X \ = \
\left|
\begin{array}{ccc}
\dot x_{11} & \cdots & \dot x_{1d}\\
 x_{21} & \cdots &  x_{2d}\\
\vdots  &    \cdots & \vdots \\
 x_{d1} & \cdots &  x_{dd}
\end{array}
\right| \ + \
\left|
\begin{array}{ccc}
x_{11} & \cdots & x_{1d}\\
\dot x_{21} & \cdots &  \dot x_{2d}\\
\vdots  &    \cdots & \vdots \\
 x_{d1} & \cdots &  x_{dd}
\end{array}
\right|
 \ + \ \cdots \ + \
\left|
\begin{array}{ccc}
x_{11} & \cdots & x_{1d}\\
x_{21} & \cdots &  x_{2d}\\
\vdots  &    \cdots & \vdots \\
\dot x_{d1} & \cdots &  \dot x_{dd}
\end{array}
\right|
\end{equation}
\end{lemma}
{\tt Доказательство леммы~\ref{l.ddet}.}
По формуле определителя $\det X \, = \, \sum_{\sigma} (-1)^{\varepsilon (\sigma)} x_{1 \sigma_1}
\cdots x_{d \sigma_d}$, где сумма взята по всем перестановкам $\sigma$ набора $(1 \ldots d)$,
 $\varepsilon (\sigma)$ -- четность перестановки. По формуле Лейбница дифференцирования произведения:
 $$
 \frac{d}{dt} \, (x_{1 \sigma_1} x_{2 \sigma_2}
\cdots x_{d \sigma_d}) \ = \ \dot x_{1 \sigma_1} x_{2 \sigma_2}
\cdots x_{d \sigma_d} \ + \ x_{1 \sigma_1} \dot x_{2 \sigma_2}
\cdots x_{d \sigma_d} \ + \ \cdots \ + \ x_{1 \sigma_1} x_{2 \sigma_2}
\cdots \dot x_{d \sigma_d}\, .
 $$
Просуммировав первое слагаемое $\dot x_{1 \sigma_1} x_{2 \sigma_2}
\cdots x_{d \sigma_d}$, умноженное на $(-1)^{\varepsilon (\sigma)}$, по всем перестановкам $\sigma$,
получим первое слагаемое в сумме~(\ref{eq.ddet}), аналогично -- с остальными $d -1$ слагаемыми.

   {\hfill $\Box$}
\medskip

{\tt Доказательство теоремы~\ref{th.w}.} Согласно лемме~\ref{l.ddet}
$$
\frac{d}{dt} \, W(t)\ = \
\left|
\begin{array}{ccc}
\dot x_{1}^1 & \cdots & \dot x_{d}^1\\
 x_{1}^2 & \cdots &  x_{d}^2\\
\vdots  &    \cdots & \vdots \\
 x_{1}^d & \cdots &  x_{d}^d
\end{array}
\right| \ + \
\left|
\begin{array}{ccc}
x_{1}^1 & \cdots & x_{d}^1\\
\dot x_{1}^2 & \cdots &  \dot x_{d}^2\\
\vdots  &    \cdots & \vdots \\
 x_{1}^d & \cdots &  x_{d}^d
\end{array}
\right|
 \ + \ \cdots \ + \
\left|
\begin{array}{ccc}
x_{1}^1 & \cdots & x_{d}^1\\
x_{1}^2 & \cdots &  x_{d}^2\\
\vdots  &    \cdots & \vdots \\
\dot x_{1}^d & \cdots &  \dot x_{d}^d
\end{array}
\right|
$$
Рассмотрим первое слагаемое, обозначим соответствующую матрицу через~$X$.
Поскольку для любого $i=1, \ldots , d$ имеем $\dot x_i^1\, = \, \sum_{s=1}^d a_{1s}x_i^s$,
для строк первой матрицы выполнено соотношение $\dot X^1 \, = \, \sum_{s=1}^d a_{1s}X^s$.
Пользуясь линейностью определителя по первой строке и тем, что определитель матрицы с
двумя равными строками равен нулю, получаем
$$
\det X \ = \ \sum_{s=1}^d a_{1s}\det (X^s, X^2, \ldots , X^d) \ = \  a_{11}\det (X^1, X^2, \ldots , X^d) \ = \ a_{11} W\, .
$$
Сложив по всем $d$ слагаемым, получаем
$$
\frac{d}{dt} \, W(t)\ = \ \left( \sum_{i=1}^d a_{ii}(t)\, \right) \, W(t) \ = \ \left( {\rm tr}\, A(t)\, \right) \, W(t)\, .
$$
Таким образом $\dot W (t)\, = \, ({\rm tr}\, A(t)) \, W(t)$. Решив данное дифференциальное уравнение, получаем формулу для $W(t)$.

   {\hfill $\Box$}
\medskip

\begin{cor}\label{c.indep}
Если $d$ решений $x_1(t), \ldots , x_d(t)$ уравнения~(\ref{eq.linh}) линейно независимы в некоторой точке $\bar t \in \re$,
то они линейно независимы в любой точке~$t \in [a, b]$.
\end{cor}
{\tt Доказательство.} По теореме~\ref{th.w}, если функция $W(t)$ обращается в ноль хотя бы в одной точке,
то она равна нулю тождественно. Следовательно, если $W(\bar t) \ne 0$, то $W(t)\ne 0$ в любой точке~$t$.

   {\hfill $\Box$}
\medskip

Таким образом, $d$ решений однородной линейной системы линейно независимы, если они линейно независимы
хотя бы в одной точке. Оказывается, в этом случае они составляют бызис пространства решений.
\begin{defi}\label{d.lin-f}
Система из  $d$ решений $x_1, \ldots, x_d$ уравнения $\dot x \, = \, A(t) \, x$
называется фундаментальной, если эти решения независимы и порождают своими линейными комбинациями все решения.
\end{defi}
Таким образом, фундаментальная система это базис пространства решений, состоящий  из $d$ функций.  Если такая системы существует, 
то размерность пространства решений равна~$d$.
\begin{theorem}\label{th.lin-f}
Пространство решений уравнения $\dot x(t) = A(t)x(t)$ имеет размерность~$d$.
Любая система решений $\{x_k\}_{k=1}^d$, для которой векторы $\{x_k(t_0)\}_{k=1}^d$
линейно независимы в некоторой точке~$t_0$, является фундаментальной.
\end{theorem}
{\tt Доказательство.} По теореме о существовании решений, для любого вектора
$x_0\in \re^d$  существует решение уравнения $\dot x = Ax$ с начальными условиями
$x(t_0) = x_0$. Поэтому, для любых  $d$ вектров $\{x_{0, k}\}_{k=1}^d$
существуют решения $\{x_{k}\}_{k=1}^d$ с начальными условиями $x_k(t_0) = x_{0, k}, \, k = 1, \ldots , d$.
Если эти векторы независимы, то и решения будут независимы (следствие~\ref{c.indep}).
Для любого решения $x(t)$ этого уравнения, вектор $x(t_0)$ выражается
как линейная комбинация векторов $\{x_{0, k}\}_{k=1}^d$. Линейная комбинация функций
 $\{x_k\}_{k=1}^d$ с теми же коэффициентами также является решением, причем с теми же начальными условиями,
   что и решение $x$. Поэтому, в силу теорему о единственности решения, $x$ тождественно равно данной линейной
   комбинации. Следовательно, данные $d$ решений порождают любое решение.

   {\hfill $\Box$}
\medskip

Далее будет удобно использовать матричные обозначения. Матрица $X(t)$ составлена из
столбцов фундаментальных решений $x_1(t), \ldots, x_d(t)$. Эта матрица удовлетворяет
уравнению $\dot X \, = \, A X, \,  X(t_0) = X_0$.

Итак, каждой невырожденной матрице $X_0$ однозначно соответствует фундаментальная система
решений $\{x_k\}_{k=1}^d$, для которой $X(t_0) = X_0$, т.е., $x_k(t_0)$ равен $k$ -- тому столбцу матрицы~$X_0$.
Чтобы получить фундаментальную систему, можно взять например
$x_k^i(t_0) = \delta_{ki}$ (символ Кронекера, $\delta_{ki} = 1$ при $k=i$ и $\delta_{ki}=0$ при $k\ne i$).
В этом случае $X_0 = I$ (единичная матрица).
\smallskip

Верно и обратное: любая система из $d$ функций, линейно независимых в каждой точке, является фундаментальной системой решений линейного однородного уравнения.
\begin{theorem}\label{th.syst-lin}
Всякая система из $d$ непрерывно-дифференцируемых функций $\{x_k\}_{k=1}^d$ такая, что векторы $\{x_k(t)\}_{k=1}^d$
линейно независимы в каждой точке $t\in [a,b]$, является фундаментальной системой решений линейного однородного уравнения, которое определяется однозначно. Матрица этой системы равна $A(t)\, = \, \dot X (t)\, \bigl[ X(t)\bigr]^{-1}$.
\end{theorem}
{\tt Доказательство.}
Если матрица $X(t)$ невырождена в каждой точке~$t$, то для матрицы $A(t) = \dot X (t)\, X^{-1}(t)$
 выполнено $\dot X = AX$, т.е. столбцы $x_i$ матрицы $X$ являются решениями уравнения $\dot x = Ax$.
 Обратно, если функции $x_i$ составляют фундаментальную систему решений уравнения
  $\dot x = Ax$, то $\dot X = AX$, а значит  $A(t) = \dot X (t)\, X^{-1}(t)$.
Таким образом, матрица $A$ однозначно определяется матрицей~$X(t)$.

   {\hfill $\Box$}
\medskip



Cистему с матрицей $A = \dot X \, X^{-1}$ можно записать $d$ уравнениями
\begin{equation}\label{eq.syst-lin}
\det \ \left(
\begin{array}{cccc}
x^1 & x_{1}^1 & \cdots & x_{d}^1\\
x^2 & x_{1}^2 & \cdots &  x_{d}^2\\
\vdots  &  \cdots &   \cdots & \vdots \\
x^d &  x_{1}^d & \cdots &  x_{d}^d\\
\dot x^i & \dot x_{1}^i & \cdots &  \dot x_{d}^i
\end{array}
\right)\quad = \quad 0\ , \qquad i = 1, \ldots , d
\end{equation}
Разложив определитель по первому столбцу,
получим $W(t) \dot x^i(t)  \, = \, \sum_{k=1}^d W_{k, i} (t) x^k(t)$,
где $W(t)$ -- определитель Вронского системы функций $x_1, \ldots , x_d$,
и $W_{k, i}(t)$ -- непрерывные функции, равные соответствующим алгебраическим дополнениям матрицы~(\ref{eq.syst-lin}).
Поскольку, система функций $\{x_k\}_{i=1}^d$ независима в каждой точке, имеем $W(t)\ne 0$, а значит
наша система приобретает нужный вид:
$$
 \dot x^i  \  = \  \sum_{k=1}^d \frac{W_{k, i}(t)}{W(t)}  \, x^k(t)\ , \qquad i = 1, \ldots , d\, .
$$

\begin{ex}\label{ex.syst-lin1}
{\em Для системы функций
$x_1(t) = \bigl(\cos t\, , \, \sin t \bigr), \, x_2(t) = \bigl(-\sin t\, , \, \cos t \bigr) $
имеем $W(t) = \cos^2 t + \sin^2 t \equiv  1$, следовательно, эта система является
фундаментальной системой решений некоторого уравнения, определенного на всей прямой~$\re$.
Это уравнение можно получить из формулы~(\ref{eq.syst-lin}), а можно и легко угадать:
его матрица $A = \bigl({\, 0\,  -1\atop 1\, \  \ 0 }\bigr)$.

 Для системы функций
 $x_1(t) = \bigl(\cos t\, , \, \sin t \bigr), \, x_2(t) = \bigl(\sin t\, , \, \cos t \bigr) $
 ситуация иная: вронскиан $W(t) = \cos 2t$ обращается в ноль в точках $\frac{\pi}{4} + \frac{\pi k}{2}, \ k \in \z$.
 Поэтому, она не является фундаментальной системой линейной однородной системы ни на каком отрезке,
 содержащем какую-либо из этих точек. Но на любом промежутке, не содержащим их, согласно
теореме~\ref{th.syst-lin}, такая система найдется. Например, на интервале $\bigl(-\frac{\pi}{4}, \frac{\pi}{4} \bigr)$.
 Угадать ее уже не так просто,
поэтому воспользуемся формулой~(\ref{eq.syst-lin}). Получаем:
$$
A(t) \ = \
\left(\,
\begin{array}{cc}
- {\rm tg}\, 2t\, & \, \frac{1}{\cos 2t}\\
\, \frac{1}{\cos 2t} & - {\rm tg}\, 2t\,
\end{array}
\right)
$$
}
\end{ex}
\bigskip

Обратимся теперь к отысканию частных решения неоднородного уравнения
$\dot x = A x + g$. Для этого  можно применить метод вариации постоянной.
Каждое решение однородного уравнения $\dot x = Ax$ есть линейная комбинация фундаментальных решений~$x_i$, т.е., дается формулой
$x = \sum_{i=1}^d c_i x_i \, = \, X\, c$,
где $c = (c_1, \ldots , c_d)$ -- вектор констант.
Если теперь допустить, что этот вектор зависит от $t$, т.е., $c = c(t)$, то
$$
(Xc)' \ =\ \dot X \, c \ + \ X \, \dot c\, \ = \ A(Xc)\ + \ g\, .
$$
Так как $\dot X c = AXc$, то $X \dot c = g$, откуда $\dot c = X^{-1}g$.
Таким образом, доказана
\begin{theorem}\label{th.lin-nonh}
Частное решение неоднородной сиcтемы $\dot x = Ax + g$ можно найти по формуле
$x = Xc$, где $X$ -- матрица фундаментальных решений однородной системы, а $c(t)$ -- вектор-функция, которая
находится  из уравнения $\dot c = X^{-1}g$, или, что то же, из системы $X \, \dot c \, = \, g$.
\end{theorem}
Получим теперь следствия для линейного уравнения $d$-того порядка. Линейным уравнением порядка
$d$ называется уравнение на функцию $x(t)$:
\begin{equation}\label{eq.l-order-d}
x^{(d)}(t) \ = \ \sum_{k=0}^{d-1} \, a_k(t)\, x^{(k)}(t) \ + \ g(t)\, ,
\end{equation}
где $a_0(t), \ldots , a_{d-1}(t)$ и $g(t)$ -- непрерывные функции, а решение
$x(t)$ предполагается $d$ раз непрерывно дифференцируемым. Как и в случае линейных
систем, мы вначале рассматриваем соответствующее {\em однородное уравнение}:
\begin{equation}\label{eq.lh-order-d}
x^{(d)}(t) \ = \  \sum_{k=0}^{d-1} \, a_k(t)\, x^{(k)}(t)\, .
\end{equation}
Уравнение порядка $d$ приводится к линейной системе порядка $1$ с помощью
очевидной замены $x^1 = x, \, x^2 = \dot x , \ldots , x^d \, = \, x^{(d-1)}$,
после которой однородное уравнение~(\ref{eq.lh-order-d}) принимает вид
\begin{equation}\label{eq.lh-syst-d}
\left\{
\begin{array}{lcl}
\dot x^1 & = & x^2\\
\dot x^2 & = & x^3\\
\vdots & {} & \vdots \\
\dot x^{d - 1} & = & x^{d}\\
\dot x^{d} & = & a_0x^{1} \, + \, \cdots \, a_{d-1}x^{d}
\end{array}
\right.
\end{equation}
Таким образом, получаем линейное уравнение~(\ref{eq.lin})  на вектор-функцию
\linebreak $\bigl(x(t), \dot x (t), \ldots , x^{(d -1)}(t)\bigr) \in \re^d$ с матрицей
\begin{equation}\label{eq.matr-d}
A \quad  = \quad  \left(
\begin{array}{cccccc}
0 & 1 & 0 & 0 & \cdots & 0\\
0 & 0 & 1 & 0 & \cdots & 0\\
\vdots & \vdots & \ddots & \ddots & \vdots & \vdots \\
 0 & 0 & 0 & \ddots & 1 & 0 \\
 0 & 0 & 0 & \cdots & 0  & 1\\
 a_0(t) & a_1(t) & \cdots & \cdots & a_{d-2}(t)  & a_{d-1}(t)
\end{array}
\right)
\end{equation}
По аналогии с линейными системами, для любых $d$ решений $x_1, \ldots, x_d$ уравнения~(\ref{eq.lh-order-d})
рассматривается матрица $X(t)$ состоящая из столбцов
$(x_i, \dot x_i, \ldots , x_i^{(d -1)})^T, \, i = 1, \ldots , d$ и
определитель Вронского $W(t) \, = \, \det \, X(t)$. Заметим, что линейная независимость
функций $x_1, \ldots , x_d$ равносильна линейной независимости столбцов матрицы $X$, т.е.,
невырожденности этой матрицы. Любая системы из $d$ линейно независимых решений уравнения~(\ref{eq.lh-order-d})
называется ее {\em фундаментальной системой решений}. Применяя доказанные результаты о линейных системах к
системе с матрицей~(\ref{eq.matr-d}), получаем
\begin{theorem}\label{th.l-order-d}
Пространство решений   уравнения~(\ref{eq.lh-order-d}) имеет размерность $d$.
Определитель Вронского этого уравнения удовлетворяет соотношению
 $$
 \, W(t) \ = \ W(t_0) \ e^{\, \int_{t_0}^t \, a_{d-1}(\tau) \, d \tau }\, .
 $$
  В частности
 система решений $x_1, \ldots , x_d$ является фундаментальной тогда и только тогда когда $W(t_0)\ne 0$.
\end{theorem}
  Для поиска частного решения неоднородного уравнения~(\ref{eq.l-order-d}) мы воспользуемся
  теоремой~\ref{th.lin-nonh}. Вектор-функция в правой части неоднородной системы теперь имеет вид
  $\vec{g}(t) = (0, \ldots , 0, g(t))$ (ставим стрелочку над вектором, чтобы отличать его от функции~$g(t)$).
   Поэтому, система уравнений  $X \, \dot c \, = \, \vec{g}$
для нахождения коэффициентов $c_1, \ldots , c_d$ теперь выглядит так:
  \begin{equation}\label{eq.d-nonh}
\left\{
\begin{array}{lcl}
\sum_{k=1}^d \, \dot c_k(t) \, x_k(t)   & = & 0\\
\sum_{k=1}^d \, \dot c_k(t) \, \dot x_k(t)   & = & 0\\
\vdots & {} & \vdots \\
\sum_{k=1}^d \, \dot c_k(t) \,  x_k^{(d-2)}(t)   & = & 0\\
\sum_{k=1}^d \, \dot c_k(t) \,  x_k^{(d-1)}(t)   & = & g(t)
\end{array}
\right.
\end{equation}
 \begin{theorem}\label{th.lin-d-nonh}
Частное решение неоднородного уравнения~(\ref{eq.l-order-d}) можно найти по формуле
$x(t) = \sum_{k=1}^d c_k(t) \, x_{k}(t)$, где $x_1, \ldots , x_d$ -- фундаментальные  решения однородного уравнения, а
функции $\{c_k(t)\}_{k=1}^d$ определяются из системы уравнений~(\ref{eq.d-nonh}).
\end{theorem}

Итак, в линейных системах и линейных уравнениях решения образуют линейное пространство размерность которого~$d$ равна
порядку уравнения. Любые $d$ решений, линейно независимых в начальный момент времени~$t_0$, образуют базис этого пространства, т.е., фундаментальную систему решений. Частное решение неоднородного уравнения может быть найдено
методом вариации постоянной.
\smallskip

Главная проблема при изучении линейных систем состоит в том, что фундаментальную систему решений бывает очень непросто найти. Стандартный метод нахождения есть только для уравнений первого порядка. Уже для
уравнений второго порядка (например, для уравнения Штурма-Лиувилля) таких методов не существует. Исключения составляют системы с постоянными коэффицентами, для которых фундаментальные решения выписываются в явном виде.

\bigskip

\newpage 

\begin{center}
\textbf{4.2. Линейные системы с постоянными коэффициентами}
\end{center}
\bigskip

Система с постоянными  коэффициентами -- это линейная система, матрица $A$ которой не зависит~$t$.
 Система с постоянными коэффициентами может быть однородной: $\dot x = Ax$, и неоднородной:
 $\dot x = Ax + g$, где $g(t)$ -- произвольная непрерывная функция.

Как мы знаем из теоремы~\ref{th.lin-f}, любая однородная линейная система размерности~$d$ имеет
$d$ фундаментальных решений. Они линейно независимы и   порождают вс\"е пространство решений.
Векторы-столбцы фундаментальных решений образуют матрицу фундаментальных решений (или {\em фундаментальную матрицу}) $X(t)$. Она определена неоднозначно. Все фундаментальные матрицы получаются из одной по формуле $X(t) C$, где $C$ -- произвольная
постоянная матрица. Каждая фундаментальная матрица удовлетворяем матричному дифференциальному уравнению
\begin{equation}\label{eq.lin-matr}
\dot X(t) \quad = \quad A\, X(t)\, .
\end{equation}
Это уравнение равносильно исходной системе $\dot x = Ax$ с той только разницей, что его решениями
являются матрицы со столбцами, составленными из решений этой системы.  Поэтому, решить исходную систему
$\dot x = Ax$ и матричное уравнение~(\ref{eq.lin-matr}) -- одна и та же задача.
\smallskip


 Как мы отмечали, решения системы с постоянными коэффициентами можно найти в явном виде. Для этого понадобится понятие матричной экспоненты.
 \begin{defi}\label{d.exp}
 Для произвольной $d\times d$-матрицы $A$, определим $\, e^{\,tA} \, = \, \sum_{k=0}^{\infty} \frac{t^k}{k!}\, A^k$.
 \end{defi}
Мы воспользовались стандартным соглашением, что $0! = 1$ и $A^0 = I$ (единичная матрица).
\begin{ex}\label{ex.matr-exp}
{\em Если $A = \bigl({ \, a \, 0 \atop \, 0 \, b} \bigr)$ -- диагональная матрица, то
$$
e^{\, tA} \ = \ \left(
\begin{array}{ll}
  e^{at} &  0 \\
   0 &  e^{bt}
\end{array}
\right)
$$
\smallskip

Если $A = \bigl({ \, 0 \, 1 \atop \, 1 \, 0} \bigr)$ -- матрица симметрии относительно биссектрисы
координатного угла, то
$$
e^{\, tA} \ = \ \left(
\begin{array}{ll}
  \ch t &  \sh t \\
   \sh t &  \ch t
\end{array}
\right)
$$
Если $A = \bigl({ \ \,  0 \, 1 \atop - 1 \, 0} \bigr)$ -- матрица поворота на $90^o$, то
$$
e^{\, tA} \ = \ \left(
\begin{array}{ll}
  \cos t  &  - \sin t  \\
    \sin t  &  \ \cos t
  \end{array}
\right)
$$
-- матрица поворота на угол~$t$.
}
\end{ex}

В случае $t = 1$ принято обозначение $e^{\, A} = e^{\, 1 \cdot  A}$. Ясно, что $e^{\, 0 \, \cdot A} = I$.
Если матрицы $A$ и $B$ коммутируют, т.е., $AB = BA$, то $\, e^{\, A + B} \, = \, e^{\, A} e^{\, B}$.
В частности, $e^{\, (t+s)A} \, = \, e^{\, tA}\, e^{\, sA}$ для любых $s, t \in \re$. Например,
$e^{\, t(I+A)} \, = \, e^{\, tI} \, e^{\, tA} \, = \, e^{\, t} e^{\, tA}$. Кроме того,
$e^{-tA} e^{tA} = I$, т.е., $\, e^{\, -tA}$ -- матрица обратная к $\, e^{\, tA}$. Таким образом, мы доказали
\begin{cor}\label{c.lin-exp2}
Для любой матрицы $A$ и для любого $t \in \re$ матрица $e^{\, tA}$ невырождена и имеет  обратную матрицу
$e^{-tA}$.
\end{cor}
\smallskip

Корректность определения матричной экспоненты вытекает из того, что $\|A^k\| \le \|A\|^k$, поэтому данный степенной ряд сходится
абсолютно и равномерно на любом отрезке прямой.
Более того, ряд из производных $\sum_{k=0}^{\infty} \frac{k\, t^{k-1}}{k!}\, A^k\, = \,
\sum_{k=0}^{\infty} \frac{t^{k-1}}{(k-1)!} \, A^k \, = \, A \, e^{\, tA}$ также абсолютно и равномерно сходится. Поэтому ряд для
$e^{\,t A}$ можно
почленно дифференцировать. Следовательно,  $\frac{d}{dt}\, e^{\, tA}\, = \, A\, e^{\, tA}$. Таким образом,
матрица $X(t) = e^{\, tA}$ удовлетворяет уравнению $\dot X = AX$. Кроме того, $X(0) = I$, поэтому, в силу теоремы
\ref{th.lin-f}, столбцы матрицы $X$ представляют собой систему фундаментальных решений уравнения $\dot x = Ax$.
 Итак, доказана
\begin{theorem}\label{th.exp}
Уравнение $\dot x = Ax$ с постоянной матрицей $A$ имеет матрицу  фундаментальных решений $X(t) = e^{\, tA}$.
Эта матрица является единственным решением матричного уравнения $\dot X = AX$ с начальным условием $X(0) = I$.
Все остальные решения даются формулой $e^{\, t A}C$, где $C$ -- произвольная матрица, а все фундаментальные матрицы даются той же формулой с невырожденной матрицей~$C$.
\end{theorem}
\begin{cor}\label{c.lin-exp3}
Решение уравнения $\dot x = Ax$ с начальным условием $x(t_0) = x_0$ дается формулой
$e^{\, (t-t_0)A}x_0$.

 Решение уравнения $\dot X = AX$ с начальным условием $X(t_0) = X_0$ дается формулой
$e^{\, (t-t_0)A}X_0$.
\end{cor}
{\tt Доказательство.} Матрица $e^{\, (t-t_0)A}$ удовлетворяет уравнению~(\ref{eq.lin-matr}).
Умножив это уравнение справа на матрицу $X_0$, получаем, что матрица $X = e^{\, (t-t_0)A}X_0$
также удовлетворяют этому уравнению. При $t = t_0$ имеем $e^{\, (t-t_0)A} = I$, поэтому $X(t_0) = X_0$.
Также показываем, что  вектор-функция $x(t) = e^{\, (t-t_0)A}x_0$ удовлетворяет уравнению $\dot x = Ax$
и $x(t_0) = x_0$.




   {\hfill $\Box$}
\medskip


Итак, если мы в состоянии вычислять матричную экспоненту, то фундаментальные решения
системы с постоянными коэффициентами найдены. Однако, найдены они в виде суммы бесконечного ряда.
Хотелось бы выписать их в явном виде, через элементарные функции. Это удобно сделать, перейдя к
другому базису пространства $\re^d$, в котором матрица $A$ имеет жорданову форму.

\begin{defi}\label{d.jor}
Жордановой клеткой размера $n \ge 1$ называется квадратная $n\times n$ матрица вида
\begin{equation}\label{eq.chamb}
\Lambda \quad  = \quad
\left(
\begin{array}{ccccc}
\lambda & 1 & 0  & \cdots & 0\\
0 & \lambda & 1  & \cdots & 0\\
\vdots & \ddots & \ddots & \ddots & \vdots \\
0 & \cdots  & 0 & \lambda & 1\\
0 & \cdots  & 0 & 0 & \lambda
\end{array}
\right)
\end{equation}
где $\lambda$ -- действительное или комплексное число.
Жордановой формой произвольной квадратной матрицы $A$ называется
эквивалентная ей матрица (после перехода к новому базису), имеющая блочно-диагональный вид:
на диагонали стоят несколько жордановых клеток, остальные элементы -- нули. Базис, в котором
матрица имеет жорданову форму, называется жордановым.
\end{defi}
Согласно классической теореме линейной алгебры, каждая матрица имеет единственную, с точностью до перестановки
клеток, жорданову форму. Обозначим матрицу жордановой формы через~$J = J_A$.
 Позднее мы напомним как получить эту форму и как искать жорданов базис.
Пока установим несколько простых свойств матрицы~$J$.

Заметим, что число~$\lambda$, стоящее на диагонали
жордановой клетки, является собственным значением матрицы $\Lambda$ кратности~$n$.
Следовательно, оно является и собственным значением матрицы $J$, а значит, поскольку
$J$ эквивалентна~$A$, и собственным значением матрицы~$A$.
Обозначим через $\lambda_1, \ldots , \lambda_r$ все {\em различные} собственные
значения матрицы~$A$, и $m_1, \ldots , m_r$ их кратности.
\smallskip

\noindent \textbf{Факт 1}. {\em Все диагональне элементы блоков жордановой формы~$J$ являются собственными значениями матрицы~$A$.
Каждому собственному значению~$\lambda_i$ матрицы $A$ соответствует одна или несколько жордановых клеток,
сумма  их размеров равна~$m_i$, а их количество равно $d \, - \, {\rm rank} (A - \lambda_i I)$.}
 \smallskip

Итак, сумма размеров жордановых клеток, соотвествующих данному собственному значению $\lambda$ равна его кратности,
а число клеток равно ``падению ранга'' матрицы $A - \lambda I$ по сравнению с полным рангом.
В частности, если ${\rm rank} (A - \lambda I) = d -1$, то у данного собственного числа есть ровно одна клетка.
\smallskip

 \noindent \textbf{Факт 2}. {\em Если все элементы матрицы~$A$ действительны,   то каждой
 паре комплексно сопряженных собственных значений $\lambda, \bar \lambda$ соответствует одна или несколько пар
 сопряженных клеток. В каждой паре размеры клеток одинаковы, одна соответствует~$\lambda$, другая $\bar \lambda$.}
\smallskip

\begin{cor}\label{c.jor1}
Жорданова форма является диагональной матрицей тогда и только тогда когда все клетки имеют размер~$1$.
Так будет, в частности, если все собственные значения матрицы~$A$ -- простые.
\end{cor}
\smallskip

\noindent \textbf{Факт 3}. {\em Жорданова форма симметрической матрицы (когда $A = A^T$) является диагональной
и все элементы на диагонали действительны. Жорданов базис в этом случае является ортогональным.}
\smallskip

Жарданова форма хороша тем, что для нее удобно вычислять степени и экспоненту.
Мы обозначаем ${k \choose j} = \frac{k!}{(k-j)! j!}$ -- биномиальный коэффициент, т.е.,
число неупорядоченных сочетаний из $k$ по $j$. Если $j=0$ или $j=k$, то
${k \choose j} = 1$, если же $j < 0$ или $j > k$, то формально полагаем ${k \choose j} = 0$.
Формула для $\Lambda^k$ легко проверяется по индукции относительно~$k$. После этого формула
для $e^{\, t\, \Lambda}$ легко доказывается суммированием ряда $\sum_{k=0}^{\infty}\frac{t^k}{k!}\, \Lambda^k$.
\smallskip

\noindent \textbf{Факт 4}. {\em Для жордановой клетки $\Lambda$ верны формулы:}
\begin{equation}\label{eq.jor-power}
\Lambda^k \quad = \quad
\left(
\begin{array}{lllccl}
\lambda^k & k \lambda^{k-1} &  {k \choose 2} \, \lambda^{k-2} & \cdots & {k \choose n-2} \, \lambda^{k-n+2} & {k \choose n-1} \, \lambda^{k-n+1}\\
   0 &  \lambda^k &  k \lambda^{k-1} &   \cdots & \cdots &{k \choose n-2} \, \lambda^{k-n+2}\\
\vdots & \ddots & \ddots & \ddots & {} & \vdots \\
{}& {} & {} & {} & {} & {}\\
\vdots & {} & \ddots & \ddots & \ddots & \vdots \\
{}& {} & {} & {} & {} & {}\\
0 & \cdots & \cdots & 0 & \lambda^k &  k \lambda^{k-1}\\
0 & \cdots & \cdots  & 0 & 0 & \lambda^k
\end{array}
\right)
\end{equation}
\begin{equation}\label{eq.jor-exp}
e^{\, t \Lambda} \quad = \quad
e^{\, \lambda \, t} \ \left(
\begin{array}{lllccl}
 1 &  t & \frac{t^2}{2!} & \cdots  & \frac{t^{n-2}}{(n-2)!} & \frac{t^{n-1}}{(n-1)!}\\
0& 1 &  t &  \cdots  & \cdots & \frac{t^{n-2}}{(n-2)!} \\
% {}& {} & {} & {} & {} & {}\\
\vdots  & {} & \ddots  & \ddots & {} & \vdots \\
0 & \cdots & {}  & 1 & t & \frac{t^2}{2!} \\
0 & \cdots & \cdots & 0 & 1 &  t\\
0 & \cdots & \cdots  & 0 & 0 & 1
\end{array}
\right)
\end{equation}
\smallskip

Теперь мы можем применить теорему~\ref{th.exp} и найти в явном виде фундаментальные решения
линейного однородного уравнения $\dot x = Ax$, сначала для случая жордановой
формы, а затем и в общем случае. Обозначим через $P$ матрицу, столбцы которой
являются векторами жорданова базиса. Это матрица перехода к жорданову базису.
Вектор $x \in \re^d$ будет иметь в жордановом базисе координаты $y$, где $x = Py$.
Тогда $J = P^{-1}AP$, и, соответственно, $A = PJP^{-1}$.
\begin{theorem}\label{th.sol-l-jor}
Уравнение $\dot y = Jy$ с жордановой $d\times d$ матрицей $J$ имеет $d$ фундаментальных
решений. Каждой жордановой клетке $\Lambda$ размера $n$ соответствует $n$ решений:
$$
\begin{array}{lclcclll}
y_1(t) & = & \Bigl(0, \ldots , 0, & e^{\lambda \, t}, & 0 , & \ldots , & 0, & 0, \ldots , 0  \Bigr)\, , \\
y_2(t) & = & \Bigl(0, \ldots , 0&  t e^{\, \lambda \, t}, & e^{\lambda \, t\, }, & \ldots , & 0, & 0, \ldots , 0  \Bigr)\, , \\ \cdots & \cdots & {} & {} & {} & {} & {} & \cdots \\
y_n(t) & = & \Bigl(0, \ldots , 0, &\frac{t^{n-1}}{(n-1)!} e^{\, \lambda\, t},&
\frac{t^{n-2}}{(n-2)!} e^{\, \lambda \, t}, & \ldots , & e^{t\, \lambda}, & 0, \ldots , 0  \Bigr)\, .
\end{array}
$$
(положения ненулевых координат соответствуют позиции данной жордановой клетки). Если $\lambda = \alpha + \beta i\,
\notin \re$,
то можно перейти от комплексных решений к действительным следующим образом. Каждой паре сопряженных жордановых
клеток размера $n$ соответствует $2n$ фундаментальных решений:
$$
\begin{array}{lclcclll}
y_1(t) & = & \Bigl(0, \ldots , 0, & e^{\, \alpha t } \cos \, \beta t , & 0, & \ldots , & 0, & 0, \ldots , 0  \Bigr)\, , \\
y_2(t) & = & \Bigl(0, \ldots , 0, & e^{\, \alpha t}, \sin \beta t, & 0 , & \ldots , & 0, & 0, \ldots , 0  \Bigr)\, ,\\
\cdots & \cdots & {} & {} & {} & {} & {} & \cdots \\
y_{2n-1}(t) & = & \Bigl(0, \ldots , 0, & \frac{t^{n-1}}{(n-1)!} e^{\, \alpha t } \cos \, \beta t, &
\frac{t^{n-2}}{(n-2)!} e^{\, \alpha t } \cos \, \beta t, & \ldots , & e^{\, \alpha t } \cos \, \beta t, & 0, \ldots , 0  \Bigr)\, ,\\
y_{2n}(t) & = & \Bigl(0, \ldots , 0, & \frac{t^{n-1}}{(n-1)!} e^{\, \alpha t } \sin \, \beta t,&
\frac{t^{n-2}}{(n-2)!} e^{\, \alpha t } \sin \, \beta t, & \ldots , & e^{\, \alpha t }
\sin \, \beta t, & 0, \ldots , 0  \Bigr)\, .
\end{array}
$$
Фундаментальные решения уравнения $\dot x = Ax$ находятся по формуле $x_i(t) = Py_i(t)$,
где $y_i(t)$ -- фундаментальные решения уравнения $\dot y = Jy$ с матрицей $J = J_A$ жордановой
формы матрицы $A$, $P$ -- матрица перехода к жорданову базису матрицы~$A$.
\end{theorem}
Для решения исходной системы нужно найти жорданов базис. Для этого найдем
основное свойство этого базиса. Матрица~(\ref{eq.chamb}) имеет первый базисный вектор $e_1$
в качестве собственного  вектора: $\, \Lambda e_1 = \lambda e_1$.
Запишем это соотношение так: $\, (\Lambda - \lambda I)e_1 = 0$.
Кроме того, легко показать, что $(\Lambda - \lambda I)e_{i+1} = e_i, \, i = 1, \ldots , n-1$.
Переходим теперь к исходному базису. Векторы жорданова базиса, соответствующие
клетке $\Lambda$ обозначим через $v_1, \ldots , v_n$. Мы видим, что эти векторы удовлетворяют соотношениям:
\begin{equation}\label{eq.j-bas1}
(A - \lambda I)v_1 \, = \, 0\ ; \quad (A - \lambda I)v_2 \, = \, v_1 \ ; \ldots ; \quad
(A - \lambda I)v_n \, = \, v_{n-1}\, .
\end{equation}
Таким образом, векторы $v_1, \ldots , v_n$, составляющие часть жорданова базиса,
находятся последовательно: сначала $v_1$ -- собственный вектор матрицы $A$, затем
вектор $v_2$ из линейного уравнения $(A - \lambda I)v_2  = v_1$, затем $v_3$, и т.д.
Векторы $v_2, \ldots , v_n$, в отличие от $v_1$, не являются собственными. Они называются
{\em корневыми} или {\em присоединенными}. Отыскав эти векторы,
мы находим $n$ фундаментальных решений системы $\dot x = Ax$:
\begin{equation}\label{eq.j-bas2}
\begin{array}{lll}
x_1(t) & = & e^{\, \lambda t}\, v_1\, ; \\
x_2(t) & = & e^{\, \lambda t}\, \left( \, v_1\, t \, + \, v_2 \right) \, ; \\
\cdots & \cdots & \cdots \\
x_n(t) & = & e^{\, \lambda t}\, \left( \, v_1\, \frac{1}{(n-1)!}\, t^{\, n-1} \, + \, v_2\, \frac{1}{(n-2)!}\, t^{\, n-2} \, + \, \cdots \, + v_n \right) \, .
\end{array}
\end{equation}
Запоминаются эти формулы по простому правилу: многочлен в скобках в каждой строке является первообразной
многочлена в предыдущей строке.
\begin{cor}\label{c.lin-exp4}
Каждой жордановой клетке размера $n$ матрицы $A$ соответствуют $n$ фундаментальных решений~(\ref{eq.j-bas2}),
где $v_1, \ldots , v_n$ -- элементы жорданова базиса, которые находятся из соотношений~(\ref{eq.j-bas1}).
\end{cor}
Если известен весь жорданов базис или, что тоже, матрица перехода $P$ (напомним, что ее столбцы и составляют жорданов базис), то фундаментальные решения находятся из следующего простого соотношения:
\begin{cor}\label{c.lin-exp5}
Если $P$ -- матрица перехода к жорданову бизису, то матрица $X\,  = \,  P\, e^{\, tJ}$ является фундаментальной.
\end{cor}
{\tt Доказательство.} Матрица $e^{\, tA} = Pe^{\, tJ}P^{-1}$ является фундаментальной.
Поэтому, для любой невырожденной матрицы $C$, матрица $e^{\, tA}C = Pe^{\, tJ}P^{-1}C$ также фундаментальна
(теорема~\ref{th.exp}). В частности, при $C = P$ получаем требуемое.

   {\hfill $\Box$}
\medskip

 Итак, мы можем найти часть жорданова базиса, соответствующего одной жордановой клетке.
Для полного решения нам остается найти матрицу перехода~$P$ в общем случае, для чего нужно
повторить алгоритм получения жорданова базиса из курса линейной алгебры. Возьмем
произвольное собственное значение~$\lambda$ матрицы~$A$. Оператор $A - \lambda I$ вырожденный, поэтому
у него есть ненулевое ядро. Обозначим его $V_1$. Это {\em первое корневое подпространство}, состоящее из собственных
векторов оператора~$A$ с собственным значением~$\lambda$: $Av = \lambda v$ для любого $v \in V_1$.
Обозначим  $m_1 = {\rm dim}\, V_1$. Заметим, что $m_1 = d - {\rm rank} (A- \lambda I)$.
Возьмем теперь {\em второе корневое подпространство}~$V_2$ -- ядро оператора $(A - \lambda I)^2$.
Ясно, что $V_1 \subset V_2$. Если эти пространства совпадают, то алгоритм закончен.
Если нет, то полагаем $m_2 = {\rm dim}\, V_2 - {\rm dim}\, V_1$, и т.д. Получаем систему
вложенных подпространств $\{V_j\}_{j=1}^n$. Каждое $V_j$ -- ядро оператора $(A - \lambda I)^j$,
$V_{j-1} \subset V_j$,
$m_j = {\rm dim} V_j - {\rm dim} V_{j-1}  \, = \, {\rm rank} \, (A-\lambda)^{j-1} - {\rm rank} \, (A-\lambda)^{j}$.
Число $n$ является наименьшим, для которого $V_{n+1} = V_n$.

Возьмем произвольные векторы $v_n^{(1)}, \ldots , v_n^{(m_n)}$, которые
дополняют пространство $V_{n-1}$ до $V_n$. Полагаем $Av_n^{(i)} = v_{n-1}^{(i)}, \, i = 1, \ldots , m_n$ и
дополняем эту систему произвольными $m_{n-1} - m_n$ векторами до системы $\{v_{n-1}^{(i)}\}_{i=1}^{m_{n-1}}$,
которая дополняет пространство $V_{n-2}$ до $V_{n-1}$, и т.д. В результате мы получаем
систему векторов $\{v_k^{(j)}\}_{k, j}$, составляющую базис пространства $V_n$.
Тогда матрица $A$ имеет $m_n$ жордановых клеток размерности~$n$,   $m_{n-1} - m_n$
жордановых клеток размерности~$n-1$, и т.д., $m_1 - m_2$ клеток размерности~$1$.
Каждая клетка порождается  базисом $v_1^{(i)}, \ldots , v_k^{(i)}$.

\begin{ex}\label{ex.jor2}
{\em У  матрицы
$$
J \quad = \quad \left(
\begin{array}{ccccc}
\lambda & 1 & 0 & 0 & 0 \\
0 & \lambda & 1 & 0 & 0\\
0 & 0 & \lambda & 0 & 0 \\
0 & 0 & 0 & \lambda & 1\\
0 & 0 & 0 & 0 & \lambda
\end{array}
\right)
$$
есть две жордановых клетки размерностей $3$ и $2$, соответствующих одному и тому же собственному значению $\lambda$.
Цепочка $\{V_k\}$ состоит из трех подпространств:
$V_1$ является ядром $A - \lambda I$, ${\rm dim } V_1 = 2 $, оно порождено векторами $e_1$ и $e_4$;
 $V_2$ является ядром $(A - \lambda I)^2$, ${\rm dim } V_2 = 4 $, оно порождено теми же векторами $e_1, e_4$
 и новыми векторами $e_2, e_5$; $V_3$ является ядром $(A - \lambda I)^3$, ${\rm dim } V_3 = 5 $, у него появляется дополнительный порождающий вектор  $e_3$. Итак,
 $$
 (A - \lambda I)\, e_1 = 0\, ; \qquad (A - \lambda I)\, e_3 = 0\, ;
 $$
 $$
 (A - \lambda I)\, e_2 = e_1\, ; \qquad (A - \lambda I)\, e_4 = e_3\, ;
 $$
 $$
 \qquad \qquad \qquad \qquad \quad \ \ (A - \lambda I)\, e_5 = e_4\, .
 $$
}
\end{ex}
Таким образом, для построения жорданова базиса нужно найти все собственные значения
матрицы $A$, для каждого из них найти полную систему корневых подпространств $\{V_i\}_{i=1}^{n}$, а затем в каждой системе  подпространств последовательно выбрать базисы $v_{k}^{(1)} , \cdots , v_{k}^{(m_k)}$,
дополняющие пространство $V_{k-1}$ до пространства $V_{k}$, $\, m_k = {\rm dim }\, V_{k} \, - \, {\rm dim}\, V_{k-1}$,
 $k = 1, \ldots , n$. Объединение этих базисов по всем корневым подпространствам и по всем собственным
 значениям составляет жорданов базис матрицы~$A$. Структура жорданова базиса, соответствующего
 одному собственному значению, показана на следующей диаграмме:
\begin{equation}\label{eq.jor-basis}
\begin{array}{lcccccccccccc}
V_n \setminus V_{n-1} & : \ & v_{n}^{(1)} & \cdots & v_{n}^{(m_n)} & {} & {}& {} & {} & {} & {} & {} & {}\\
{} & {} & \downarrow &  \cdots  & \downarrow & {} & {} & {} & {} & {} & {} & {} & {}\\
V_{n-1} \setminus V_{n-2} & : \ & v_{n-1}^{(1)} & \cdots & v_{n-1}^{(m_n)} & \cdots & v_{n-1}^{(m_{n-1})} & {} & {} & {} & {} & {}\\
\qquad \vdots & : & \downarrow & \cdots & \downarrow & \cdots & \downarrow & {} & {} & {} & {} & {}\\
\qquad \vdots &  :  & \downarrow & \vdots &  \vdots & \vdots & \vdots & \ddots & {} & {} & {} & {}& {} \\
{} & {} & \downarrow & \cdots & \downarrow & \cdots & \downarrow & \downarrow &  \downarrow & {} & {} & {} & {} \\
V_3 \setminus V_2 & : \ & v_3^{(1)} &  \cdots & \cdots & \cdots & \cdots & \cdots & v_3^{(m_3)} & {} & {} & {} & {}\\
{} & \downarrow & \cdots & \downarrow & \cdots & \downarrow & \downarrow & \downarrow & {} & {} & {} & {}\\
V_2 \setminus V_1 & : \ & v_2^{(1)} &  \cdots & \cdots & \cdots & \cdots & \cdots & v_2^{(m_3)} & \cdots & v_2^{(m_2)} & {} & {}\\
{} & {} & \downarrow & \cdots & \downarrow & \cdots & \downarrow & \downarrow & \downarrow & \cdots & \downarrow & {} & {}\\
V_1  & : \ & v_1^{(1)} & \cdots & \cdots &  \cdots & \cdots & \cdots & v_1^{(m_3)}  & \cdots & v_1^{(m_2)} & \cdots & v_1^{(m_1)}
 \end{array}
\end{equation}
\medskip

\begin{center}
\textbf{4.3. Неоднородные системы с постоянными коэффициентами}
\end{center}
\smallskip

Как мы знаем, все решения  неоднородной линейной системы $\dot x = Ax + g$ представляются в виде суммы
одного частного решения и всевозможных решений однородной системы~$\dot x = Ax$. Поэтому, для решения неоднородных систем нам достаточно научиться находить одно частное решение. В общем виде это можно сделать точно так же как и для систем с переменными коэффициентами, по теореме~\ref{th.lin-nonh}: взять произвольную фундаментальную матрицу $X$,
определить $\dot c = X^{-1} g$. Тогда $x = cX$ -- частное решение. При этом, первообразная $c(t)$ вектора
$\dot c(t) = X^{-1}(t)g(t)$ может быть взята с произвольными константами в каждой координате.
Теперь, когда коэффициенты уравнения постоянны, мы можем взять $X = e^{tA}$. Тогда $X^{-1} = e^{-tA}$.
Таким образом,
\begin{equation}\label{eq.lin-nonh-c}
x \ = \  e^{tX}\, c\ ; \qquad \dot c \ = \  e^{-t X}g(t)\, .
\end{equation}
Особый интерес представляет случай, когда каждая координата вектора $g(t)$ есть квазиполином,
т.е., сумма функций вида $P(t)e^{\alpha t}, P(t) \, \sin \beta t, \, P(t)\, \cos \beta t$,
где $P(t)$ -- алгебраический полином от~$t$. Начнем с того, что эти три случая умещаются в один случай $P(t)e^{\mu t}$ с комплексным $\mu$. Далее, можно представить $g$ в качестве суммы нескольких векторов, найти частное решение для
каждого из них, а затем сложить. Мы разложим~$g$ в сумму векторов, у каждого из которых все координаты имеют вид $g^k(t) = P_k(t)\, e^{\, \mu t}$,
где показатель $\mu$ один и тот же для всех координат, а полиномы $P_1(t), \ldots , P_d(t)$ могут быть различны.
\begin{theorem}\label{th.lin-nonh-c}
Если все координаты вектора $g(t)$
 имеют вид $g^k(t) = P_k(t)\, e^{\, \mu t}$ с алгебраическими полиномами
 $P_1, \ldots , P_k$ степеней  не выше $s\, \ge \, 0$ и с некоторым (комплексным) показателем $\mu$, одним и тем же для всех координат, то существует частное решение~$x(t)$ однородного уравнения $\dot x = Ax + g$, все координаты которого
 имеют вид $x^k = Q_k(t)e^{\, \mu t}$, где $Q_k$ -- полином степени не выше $s$, если $\mu$ не является собственным значением матрицы $A$, либо не выше $s+n$, если $\mu$ -- собственное значение $A$, и $n$ -- размер его максимальной
 жордановой клетки.
 \end{theorem}
 В доказательстве нам понадобится следующая простая
 \begin{lemma}\label{l.lin-nonh}
Любое решение уравнения  $\dot x = \lambda x  + P(t) e^{\, \mu t}$, где $P$ -- алгебраический полином степени~$s$,
имеет вид $x = Q(t) e^{\, \mu t}$, где $Q$ -- алгебраический полином степени~$s$ в случае $\mu \ne \lambda$
и  степени~$s+1$ в случае $\mu = \lambda$.
 \end{lemma}
 {\tt Доказательство}. Решение однородного уравнения: $x(t) = C e^{\, \lambda t}$.
 Варьируя постоянную, получаем $\dot C \, = \, P(t) e^{\, (\mu - \lambda )t}$.
 Если $\mu = \lambda$, то $C(t) = \int P (\tau) \, d \tau$ -- полином, степени~$s+1$, а если
 $\mu \ne \lambda$, то $C(t) \, = \, \int P (\tau) e^{\, (\mu - \lambda )\tau} \, d \tau \ = \
  Q(t)e^{\, (\mu - \lambda )t}$, где $Q$ -- полином степени $s$. Показать это можно с помощью интегрирования по частям.


   {\hfill $\Box$}
\medskip






{\tt Доказательство теоремы~\ref{eq.lin-nonh-c}.}  Заметим вначале, что если перейти к другому базису в пространстве
$\re^d$, то все координаты вектора $x$ по-прежнему будут иметь вид $Q_k e^{\, t\mu}$, с тем же показателем $\mu$
и с другими полиномами $Q_k$, у которых осталась та же наибольшая степень. Поэтому, можно сразу перейти к жорданову базису.
Иными словами, мы предполагаем, что $A$ является жордановой матрицей. Далее, поскольку каждая жорданова клетка матрицы~$A$
отвечает за ``свои'' координаты векторов  $x$ и $g$, расположенные на соответствуюших позициях,
достаточно доказать теорему для одной жордановой клетки. Итак, $A = \Lambda$, где клетка $\Lambda$ задана
формулой~(\ref{eq.chamb}). В первой координате получаем уравнение $\dot x^n = \lambda x^n  + P_n(t) e^{\, \mu t}$.
Согласно лемме~\ref{l.lin-nonh},  $x^n (t) = Q_n(t)e^{\mu t}$, где
${\rm deg}\, Q_n \, = \, {\rm deg}\,  P_n$, если $\lambda \ne \mu$ и ${\rm deg}\, Q_n \, = \, {\rm \deg} \,  P_n\, + \, 1$, если $\lambda \ne \mu$.
Далее последовательно находим $x^{n-1}, \ldots , x^1$. По лемме~\ref{l.lin-nonh}, если $\mu \ne \lambda$, то степень полинома $Q_k$ остается постоянной на каждом шаге, а если $\mu = \lambda$, то  каждый раз  увеличивается на $1$.


   {\hfill $\Box$}
\medskip

\medskip

\begin{center}
\textbf{4.4. Линейные уравнения высших порядков с постоянными коэффициентами}
\end{center}
\smallskip

Для уравнения
\begin{equation}\label{eq.l-order-d-c}
x^{(d)}(t) \ = \ \sum_{k=0}^{d-1} \, a_k\, x^{(k)}(t) \ + \ g(t)\, ,
\end{equation}
где $x(t)$ предполагается $d$ раз непрерывно дифференцируемым, мы также выделяем однородную часть
\begin{equation}\label{eq.const10h}
x^{(d)}(t) \ = \  \sum_{k=0}^{d-1} \, a_k\, x^{(k)}(t)\, .
\end{equation}
и приводим ее к равносильной линейной системе порядка $1$ с помощью
замены $x^1 = x, \, x^2 = \dot x , \ldots , x^d \, = \, x^{(d-1)}$,
после которой однородное уравнение принимает вид~(\ref{eq.lh-syst-d})
Получили линейное уравнение на вектор-функцию
$\bigl(x(t), \dot x (t), \ldots , x^{(d -1)}(t)\bigr) \in \re^d$ с матрицей
\begin{equation}\label{eq.matr-d-c}
A \quad  = \quad  \left(
\begin{array}{cccccc}
0 & 1 & 0 & 0 & \cdots & 0\\
0 & 0 & 1 & 0 & \cdots & 0\\
\vdots & \vdots & \ddots & \ddots & \vdots & \vdots \\
 0 & 0 & 0 & \ddots & 1 & 0 \\
 0 & 0 & 0 & \cdots & 0  & 1\\
 a_0 & a_1 & \cdots & \cdots & a_{d-2}  & a_{d-1}
\end{array}
\right)
\end{equation}
Для того, чтобы применить результаты предыдущих разделов к решению однородного уравнения порядка~$d$, в частности, теорему~\ref{th.sol-l-jor}, дающую явные формулы решений, нам нужно найти
жорданову форму матрицы~(\ref{eq.matr-d-c}).
\begin{prop}\label{p.lin-order-d}
Матрица~(\ref{eq.matr-d-c}) имеет характеристический полином $p(\lambda) \, = \, x^d \, - \, \sum_{k=0}^{d-1} a_k \lambda^k$.
Жорданова форма этой матрицы имеет по одной клетке на каждое собственное значение (корень полинома~$p$),
размер этой клетки равен кратности этого значения.
\end{prop}
{\tt Доказательство.}  Если $x = (x^1, \ldots , x^{d -1})$ является собственным вектором матрицы~$A$,
то $\lambda x = Ax$, откуда $\lambda x^{k} = x^{k+1}, k = 1, \ldots , d-1$, и следовательно  $x = (x_1, \lambda x_1, \ldots , \lambda^{d-1}x_1)$.
Последнее уравнение (последняя строка матрицы) дает уравнение $\lambda^d = \sum_{k=0}^{d-1} a_k \lambda^k$.
Таким образом, характеристический полином имеет требуемый вид. Если $\lambda$ -- собственное значение кратности
$n$, то, вычеркнув из матрицы $A - \lambda I$ последнюю строку и последний столбец, получим $(d -1)\times (d -1)$ матрицу.
Она является нижне-треугольной с единицами на диагонали. Значит, ее определитель равен~$1$. Таким образом, эта матрица невырождена и имеет ранг $d-1$. Поэтому ${\rm rank}\, (A - \lambda I) \, \ge \, d-1$. Следовательно, количество жордановых
клеток с собственным значением~$\lambda$ не превосходит $d - (d -1) = 1$. Поскольку $\lambda$ -- собственное значение,
должна быть хотя бы одна жорданова клетка, значит это количество равно~$1$ (см. Факт 1 о жордановой форме).


   {\hfill $\Box$}
\medskip


Применив теперь теорему~\ref{th.sol-l-jor}, получаем список всех решений
онородного уравнения порядка~$d$:
\begin{theorem}\label{th.l-order-d-c}
Каждому корню характеристического полинома $p(\lambda) \, = \, x^d \, - \, \sum_{k=0}^{d-1} a_k \lambda^k$
кратности $n$ соответствует $n$ фундаментальных решений уравнения~(\ref{eq.const10h}):
$x_k(t) = t^k e^{\, \lambda t}, \, k = 1, \ldots , n$.
\end{theorem}
  Для поиска частного решения неоднородного уравнения~(\ref{eq.l-order-d-c}) мы применяем теорему~(\ref{th.lin-nonh-c}).
\begin{theorem}\label{th.lin-d-nonh-c}
Частное решение неоднородного уравнения~(\ref{eq.l-order-d-c}) с правой частью $g(t) = P(t) e^{\, \mu t}$
имеет вид~$x(t) =  t^nQ(t)e^{\, \mu t}$, где $Q$ -- полином той же степени, что и~$P$, $n = 0$, если $\mu$
не является корнем характеристического уравнения, а если является, то равно кратности этого корня.
\end{theorem}

\bigskip

\bigskip

\begin{center}
\large{\textbf{5. Асимптотика и устойчивость}}
\end{center}

\bigskip

\begin{center}
\textbf{5.1. Асимптотика решений линейных уравнений при $t \to \infty$. Резонанс.}
\end{center}

\bigskip

Один из важнейших вопросов в исследовании дифференциальных уравнений --
поведение  решения $x(t)$ при $t \to +\infty$, или {\em асимптотика} решения при $t\to \infty$.
Растет ли оно, или, напротив, стремится к нулю,
или не стремится к нулю, оставаясь, тем не менее, ограниченным? Поскольку дифференциальные уравнения
описывают эволюцию некоторой величины, или системы величин -- физических, биологических, экономических, и т.д.,
необходимо знать, к чему приходит система при $t \to +\infty$. Этот вопрос часто более важен, чем вид решения на конечном отрезке.  Например, для уравнения
роста популяции биологического вида (см. \S 3.1), соотношение $x(t)
\to 0$ означает, что вид вымирает, $x(t) \to \infty$ -- что неограниченно растет, а $C_1 \le x(t) \le C_2$ для всех 
 $\ t > N$, 
при некоторых положительных $C_1, C_2$ и $N$, означает наступление экологического равновесия.

Для линейных систем с постоянными коэффициентами этот вопрос решается легко. Явные формулы решений
(теоремы~\ref{th.exp}, \ref{th.lin-nonh-c}, \ref{th.l-order-d-c} и  \ref{th.lin-d-nonh-c}) дают нам точную асимптотику.
\begin{theorem}\label{th.asymp1}
Для любого решения системы $\dot x = Ax$ с постоянными коэффициентами, имеем $\|x(t)\| = O(t^{n-1}e^{\, \alpha t})$ при
$t \to \infty$, где $\alpha$ -- наибольшая действительная часть собственных значений матрицы~$A$,
$n$ -- размер наибольшей жордановой клетки, соответствующей собственным значениям с наибольшей действительной частью.
Эта асимптотика точна, что означает существование решения $x(t)$,
последовательности $\{t_k\}_{k \in \n}$ и положительной константы
$C$, для которых $t_k \to +\infty$ и $\|x(t_k)\| \ge C\, t_k^{n-1}e^{\, \alpha t_k}$.

То же верно для линейного уравнения порядка $d$. В этом случае $\alpha$ -- максимальная
действительная часть корней его характеристического полинома, $n$ -- кратность соответствующего корня.
\end{theorem}
Аналогично можно исследовать рост решений неоднородной  системы $\dot x = Ax + g$ с правой частью $g(t)$
в виде квазиполинома. В этом случае максимальный рост решения равен $e^{\, \gamma  t}t^{\, s+n}$,
где $\gamma$ -- максимальная действительная часть всех показателей экспонент в квазиполиноме $g(t)$ и всех
собственных значений матрицы системы,
$n$ -- размер максимальной жордановой клетки, если этот показатель является собственным значением~$A$, или
$n=0$ иначе. То же верно и для уравнения порядка~$d$, с заменой собственного значения на корень характеристического полинома и размера жордановой клетки на кратность этого корня.
\smallskip

Заметим, что рост решений не является непрерывной функцией от коэффициентов уравнения или от начальных условий.
Асимптотика решений может резко измениться даже при незначительном изменении уравнения.
И это -- несмотря на теорему~\ref{th.cont-cond} о непрерывной зависимости решения от правой части и от начальных условий!
Противоречия здесь нет,   поскольку теорема~\ref{th.cont-cond} гарантирует непрерывную зависимость решения
лишь на конечном отрезке $[t_0, t_1]$ и не распространяется на случай бесконечного промежутка.
\begin{ex}\label{ex.resonance1}
{\em Уравнение
$$
\ddot x + x \ = \ \varepsilon \ \sin \, \alpha t\ , \qquad
x(0) = 0, \ \dot x(0) = 1
$$
при любом положительном $\alpha \ne  1$ имеет ограниченное решение
$$
x(t) \quad  = \quad  \frac{\varepsilon}{1 - \alpha^2}\, \sin \alpha t\ + \ \left(
1\,  - \, \frac{\varepsilon \, \alpha }{1 - \alpha^2}\, \right)\, \sin t\, .
 $$
 Однако, при $\alpha =  1$ это решение становится неограниченным и приобретает
линейный рост на бесконечности:
$$
 x(t) \quad = \quad  - \ \frac{\varepsilon}{2} \, t \, \cos t \ + \
 \left(
1\,  +
 \, \frac{\varepsilon }{2}\, \right)\, \sin t
$$
Заметим, что если $\varepsilon = 0$, то уравнение принимает вид $\ddot x + x = 0$, а его решение $x = \sin t$
ограничено на всей прямой. Однако, при любом $\varepsilon \ne 0$, даже очень меленьком, оно становится линейно растущим при $t \to +\infty$. Таким образом, асимптотика решения может резко меняться с незначительным изменением
уравнения.
Происходит это из-за совпадения показателя экспоненты $\mu = 1$ в правой части уравнения с
корнем характеристического уравнения $\lambda^2 - 1 = 0$ (теорема~\ref{th.lin-d-nonh-c}).
}
\end{ex}
\begin{ex}\label{ex.resonance2}
{\em Линейное однородное уравнение $x = Ax$ с нулевой $2\times 2$ матрицей $A$ при начальных
условиях $x^1(0) = 0, x^2(0) = 1$ имеем ограниченное решение $x^1(t) \equiv 0, x^2 (t) \equiv 1$.
Однако, изменив один элемент матрицы $A$ с нулевого на произвольное $\varepsilon > 0$:
$$
A \ = \ \left(
\begin{array}{cc}
0 & \varepsilon\\
0 & 0
\end{array}
\right)\ ,
$$
мы получаем линейно растущее решение: $\, x^1(t) =  \varepsilon \, t, \, x^2 (t) = 1$
Оба собственных значения матрицы $A$ при этом остались нулевыми, но у матрицы появилась жорданова клетка,
вследствие чего у решения появилcя множитель $t$ (полином степени~$1$).
}
\end{ex}
В обоих примерах малое изменение коэффициентов уравнеия приводит к
появлению растущего при $t\to +\infty$ решения. В первом примере это происходит из-за совпадения
показателя экспоненты в правой части с корнем характеристического уравнения, а во втором -- из-за появления
жордановой клетки в матрице системы. В обоих случаях произошло {\em явление резонанса}.
\begin{defi}\label{d.resonance}
Резонансом в линейной системе $\dot x = Ax + g$ с постоянными коэффициентами называется возникновение степенного множителя в решении~$x(t)$ в следствие либо совпадения показателя экспоненты в функции $g(t)$ с собственным значением~$A$,
либо появления жордановой клетки в  матрице~$A$.

 Аналогично, резонанс в линейном уравнении порядка $d$
 происходит в следствие либо совпадения показателя экспоненты в функции $g(t)$ с корнем характеристического полинома,
либо появления кратного корня.
\end{defi}
Резонанс в уравнении колебаний $\ddot x \, + \, \omega^2 x \, = \, \varepsilon \, \sin \alpha t$
(примера~\ref{ex.resonance1}) возникает при $\omega = \alpha$ и означает совпадение частоты $\omega$ собственных колебаний системы с частотой $\alpha$ воздействия внешней силы (равной~$\varepsilon \, \sin \alpha t$). Амплитуда колебаний, которая была постоянной
при отсутствии резонанса $(\omega \ne \pm \alpha)$, теперь начинает линейно расти от времени~$t$. Этот феномен мы наблюдаем, когда качаемся на качелях или настраиваем радиоприемник на волну нужной радиостанции. Он же является причиной многих катастроф (известный из учебника физики пример, когда мост разрушается из-за взвода солдат, идущего по нему строем).

\smallskip

Наиболее важное следствие теоремы~\ref{th.asymp1} касается случая, когда все решения уравнения стремятся к нулю.
\begin{defi}\label{d.hurw}
Квадратная матрица называется гурвицевой если все ее собственные значения имеют отрицательную
действительную часть. Матрица называется слабо гурвицевой если она не имеет собственных значений с
положительной действительной частью, а все значения с нулевой действительной частью (если таковые есть),
не имеют жордановых клеток.
\end{defi}
Adolf Hurwitz (1859--1919) -- немецкий математик.
Применив теорему~\ref{th.asymp1}, получаем
\begin{theorem}\label{th.asymp2}
Все решения однородной системы с постоянными коэффициентами стремятся к нулю при $t \to +\infty$ тогда и только тогда когда е\"е матрица гурвицева. Все решения ограничены при $t \to +\infty$ тогда и только тогда когда матрица
слабо гурвицева.
\end{theorem}
Заметим, что если применить эту теорему к линейному уравнению порядка~$d$, то в возникающей матрице~(\ref{eq.matr-d-c})
каждое собственное значение имеет единственную жорданову клетку (предложение~\ref{p.lin-order-d}), т.е., его геометрическая кратность совпадает с алгебраической. Отсюда получаем
\begin{cor}\label{c.asymp2}
Все решения однородного линейного уравнения порядка~$d$ с постоянными коэффициентами стремятся к нулю при $t \to +\infty$ тогда и только тогда когда все корни характеристического полинома имеют отрицательную
действительную часть. Все решения ограничены при $t \to +\infty$ тогда и только тогда когда нет корней с положительной
действительной частью, а все корни с нулевой действительной частью -- простые.
\end{cor}
Существует много признаков, позволяющих распознать  гурвицевы матрицы без вычисления собственных значений.
Вот самый простой:
\begin{prop}\label{p.hurw1}
След действительной гурвицевой матрицы отрицателен.
\end{prop}
{\tt Доказательство.} След равен сумме собственных значений.

   {\hfill $\Box$}
\medskip

Таким образом, матрица с неотрицательным следом не может быть гувицевой.
В размерности $2$ есть простой критерий для распознавания гурвицевых матриц:
\begin{prop}\label{p.hurw2}
Действительная $2\times 2$ матрица является  гурвицевой тогда и только тогда
ее след отрицателен, а определитель положителен.
\end{prop}
{\tt Доказательство.} Так как ${\rm det } \, A \, = \, \lambda_1 \lambda_2$,
то определитель положителен в точности когда числа ${\rm Re} (\lambda_1)$ и ${\rm Re} (\lambda_2)$
-- одного знака. Этот знак отрицательный когда ${\rm tr}\, A \, = \, \lambda_1 + \lambda_2 < 0$.

   {\hfill $\Box$}
\bigskip

\begin{center}
\textbf{5.2. Устойчивость}
\end{center}

\bigskip

Перейдем теперь от линейных систем к общим уравнениям $\dot x = f(t, x)$ на вектор-функцию $x = (x^1, \ldots , x^d)$.
Для простоты мы будем предполагать, что функция $f$ определена при всех значениях переменных,
т.е., $\Omega = \re^{d+1}$. Предположим, уравнение имеет единственное решение
$x(t)$ на полупрямой $[t_0, + \infty)$ с начальным условием $x(t_0) = x_0$. Как оно изменится при
малом изменении начального условия $x_0$? Из теоремы~\ref{th.cont-cond}
мы знаем, что если через каждую точку проходит единственная интегральная кривая, то она непрерывно зависит
от точки. Таким образом, на любом конечном отрезке $[t_0, t_1]$ решение мало изменится
при малом изменении $x_0$. Однако, на бесконечном промежутке это свойство может нарушаться.
Теорема~\ref{th.cont-cond} гарантирует непрерывную зависимость лишь на конечном отрезке и не распространяется на случай $t_1 = +\infty$. Например, простейшее уравнение $\dot x = x$ при начальном условии $x(0) = 0$ имеет
единственное решение $x(t)\equiv 0$, но при любом другом начальном условии $x(0) = \varepsilon$ получаем решение
$x(t) = \varepsilon e^t$, которое сильно отличается от нулевого на $[0, +\infty)$ даже при очень малых~$\varepsilon$.
Таким образом, {\em решение уравнения $\dot x = f(t, x)$ может сильно меняться на бесконечном промежутке при малых
изменениях начальных условий.}
\begin{defi}\label{d.stab}
Решение уравнения $\dot x = f(t, x)$
на промежутке $[t_0, +\infty)$ называется устойчивым, если для
любого $\varepsilon > 0$ найдется $\delta > 0$, для которого при любом $\tilde x_0$ таком, что
$\|\tilde x_0 - x_0\| < \delta$,  уравнение с начальным условием $x(t_0) = \tilde x_0$
имеет единственное решение $\tilde x(t)$,  и $\|x - \tilde x\|_{C[t_0, +\infty)} < \varepsilon$.

 Решение называется асимптотически устойчивым, если оно устойчиво,  и
для любой точки $\tilde x_0$ из некоторой окрестности точки $x_0$,
решение $\tilde x(t)$  с начальным условием $\tilde x(t_0) = \tilde x_0$ обладает свойством
  $\|x(t) - \tilde x (t)\| \to 0$ при $t \to +\infty$.
\end{defi}
Итак, устойчивость означает, что решение уравнения не уходит далеко от исходного решения при $t \to +\infty$, если мы мало изменим начальные условия. Асимптотическая устойчивость означает, что решение, более того, стремится к исходному  при $t \to +\infty$.
\begin{ex}\label{ex.stab1}
{\em Обратимся у уравнению~(\ref{eq.popul1}) описывающему рост популяции вида, образующего семейные пары.
У этого уравнения есть стационарная точка, т.е., решение, являющееся тождественной константой $x(t) \equiv 1/k$.
Это решение, как было показано, асимптиточески устойчиво. И даже глобально, т.е., решение
$x(t) =  \frac{1}{k  + (1/x_0 - k)\, e^{-\alpha t}}$ стремится к $1/k$ при $t \to + \infty$ не только при малых, но и при любых изменениях начального  условия~$x_0$.

То же верно и для  уравнения~(\ref{eq.popul1}), описывающего рост численности при наличии квоты отлова~$q$.
     При $q \le  \frac{\alpha}{4k}$ стационарная точка $x(t)\, \equiv \, \frac{1 + \sqrt{1 - \frac{4kq}{\alpha}}}{2k}$
является глобально асимптотичски устойчивой: решение стремится к нему при любых начальных условиях.

Наконец, в уравнении~(\ref{eq.popul3}) роста популяции вида, не образующего семейных пар,
та же стационарная точка $x = 1/k$ уже не является устойчивой. При любом уменьшении начального значения $x_0 = 1/k$,
получим решение~$x(t) = \frac{1}{k \, + \, (1/x_0-k)\, e^{\alpha t}}$ стремящееся к нулю при $t \to +\infty$, а при любом его увеличении -- решение, стремящееся к $+\infty$ за конечное время.

}
\end{ex}
Исследование устойчивости начнем с нескольких несложных наблюдений.
\begin{prop}\label{p.stab1}
Устойчивость и асимптотическая устойчивость решения не зависят от выбора начальной точки~$t_0$.
\end{prop}
{\tt Доказательство.} Покажем, что если решение устойчиво на $[t_0, +\infty)$,
то оно устойчиво и на любом луче $[t_1, +\infty)$ при $t_1 > t_0$. В силу непрерывной зависимости
решения от начальных условий на отрезке (теорема~\ref{th.cont-cond}), для любого $\delta > 0$
найдется такое $\gamma > 0$, что неравенство $\|\tilde x(t_1) -  x(t_1)\| < \gamma$
влечет $\|\tilde x - x\|_{C[t_0, t_1]} < \delta$, а значит  $\|\tilde x(t_0) -  x(t_0)\| < \delta$.
Последнее,  в силу устойчивости на луче $[t_0, +\infty)$, означает, что $\|\tilde x - x\|_{C[t_0, +\infty)} < \varepsilon$.
С асимптотической устойчивостью доказательство аналогично.

   {\hfill $\Box$}
\bigskip

Вопрос устойчивости произвольного решения~$x(t) = \varphi (t)$ уравнения $\dot x = f(t, x)$
можно свести простой заменой
$x = z + \varphi$  к устойчивости нулевого решения уравнения~$\dot z = f(t, z + \varphi) - f(t, \varphi)$.
\begin{prop}\label{p.stab2}
Решение $x(t) = \varphi (t)$ уравнения $\dot x = f(t, x)$
(асимптотически) устойчиво тогда и только тогда когда (асимптотически) устойчиво решение $z\equiv 0$
 уравнения~$\dot z = f(t, z + \varphi) - f(t, \varphi)$.
\end{prop}
{\tt Доказательство.} Функция $\varphi(t)$ является решением исходного уравнения с начальным условием
$x(t_0) = x_0$ тогда и только тогда когда функция $z \equiv 0$ -- решение
уравнения~$\dot z = f(t, z + \varphi) - \dot \varphi(t)$. Остается заменить~$\dot \varphi (t)$ на $f(t, \varphi)$.

   {\hfill $\Box$}
\bigskip

Наиболее просто исследуется  устойчивость линейных уравнений. Применив теорему~\ref{th.asymp2}, получаем
\begin{theorem}\label{th.stab1}
Нулевое решение однородной линейной системы $\dot x = A x $ с постоянными коэффициентами
устойчиво/асимптотически устойчиво тогда и только тогда когда матрица~$A$ слабо гурвицева/гурвицева.

Устойчивость всех решений неоднородной системы $\dot x = Ax + g$ одинакова. Решения устойчивы/асимптотически устойчивы тогда и только тогда когда матрица~$A$ слабо гурвицева/гурвицева.
\end{theorem}

Устойчивость общих уравнений можно исследовать с помощью их линеарицации, т.е., локального
приближения линейной системой. Следующая теорема, доказанная Александром Михайловичем Ляпуновым (1857 -- 1918) играет центральную роль во всей теории устойчивости.
Мы примем ее без доказательства.
\begin{theorem}\label{th.stab2}[А.М.Ляпунов, 1892]. Предположим, что
правая часть дифференциального уравнения может быть представлена в виде суммы
$$
f(t, x) \quad  = \quad  Ax \ + \ \omega(t, x)\, ,
$$
где $A$ -- постоянная матрица,
 и  $\frac{\omega (t, x)}{\|x\|}$ равномерно по всем $t \in [t_0, + \infty)$ стремится к нулю при $x \to 0$.
 Тогда если матрица $A$ гурвицева, то нулевое решение уравнения $\dot x = f(t, x)$ асимптотически устойчиво.
 Если же эта матица имеет хотя бы одно собственное значения с положительной действительной частью,
 то нулевое  решение неустойчиво.
\end{theorem}
Таким образом, если систему можно линеаризовать в окрестности нулевого решения, то
ее устойчивость устанавливается с помощью соответствующей линейной системы.
Для автономных уравнений такая линеаризация всегда существует, если только функция $f$
дифференцируема.
\begin{cor}\label{c.stab2}
Если уравнение автономно, т.е. $f(t, x) = f(x)$, причем $f(0) = 0$, и функция $f$ дифференцируема в нуле, то условие
теоремы Ляпунова выполнено, при этом $A = f'(0)$.
\end{cor}
Наиболее сложен вопрос устойчивости в ``пограничном'' случае, когда матрица $A$ имеет собственные значения
с нулевой действительной частью. В этом случае теорема Ляпунова не гарантирует ни устойчивости ни неустойчивости решения.
Так происходит например, при исследовании уравнения Лотки-Вольтерры~(\ref{eq.lv}). Как мы показали, линеаризация
этого уравнения в стационарной точке всегда приводит к матрице с двумя чисто мнимыми собственными значениями, а значит исходное уравнение может быть как асимптотически устойчиво так и нейстойчиво. Исследование подобных уравнений требует
дополнительного анализа.
\begin{theorem}\label{th.stab3}
Стационарное решение уравнения Лотки-Вольтерры~(\ref{eq.lv}) устойчиво, но не асимптотически.
\end{theorem}
{\tt Доказательство.} Как мы показали в \S \,3.2, любое решение уравнения Лотки-Вольтерры периодическое.
Поскольку оно непрерывно зависит от начальных условий, если $(\tilde x_0, \tilde y_0)$
достаточно мало отличается от стационарной точки $(x_0, y_0)$, решение $(\tilde x(t), \tilde y(t))$
будет периодическим, мало отличающимся от тождественной константы на периоде, а значит и на всей прямой.
Следовательно, решение $(x, y) = (x_0, y_0)$ устойчиво. C другой стороны, если периодическая функция
$\tilde x(t)$ стремится к константе $x_0$ при $t \to + \infty$, то она должна совпадать с этой константой тождественно.
Таким образом,  $\tilde x (t)
\equiv x_0$ и аналогично, $\tilde y(t) \equiv y_0$. В частности, $(\tilde x_0, \tilde y_0)  = (x_0, y_0)$.
Следовательно, ни для какой начальной точки, отличной от стационарной $(x_0, y_0)$,  решение не может стремиться к стационарному. В частности, асимптотической устойчивости нет.

   {\hfill $\Box$}
\bigskip


\bigskip

\begin{center}
\large{\textbf{6. Приложения к вариационнному исчислению}}
\end{center}

\bigskip

\begin{center}
\textbf{6.1. Простейшая задача вариационного исчисления. \\
Уравнения
 Эйлера-Лагранжа}
\end{center}
\bigskip

Вариационное исчисление -- наука о поиске максимумов и минимумов функционалов специального вида.
Как правило, это интегралы от некоторых функций плюс величины, зависящие от значений функций в концах отрезка.
Необходимо найти  экстремальные значения, а также оптимальные функции, на которых эти значения достигаются.
Подобные задачи возникают повсеместно, от классической механики и геометрии, до математической экономики и социологии.
Оптимальные  функции находятся как решения  дифференциальных  уравнений специального вида.

Мы начнем с так называемой {\em простейшей задачи вариационного исчисления}:
\begin{equation}\label{simpl-prob}
\left\{
\begin{array}{l}
\cJ(x)\quad = \quad \int\limits_{t_0}^{t_1}\, L\, \bigl(t, x, \dot
x
\bigr)\, d t\quad \to \quad \min\, ,\\
{}\\
 x \, \in \, C^1 [t_0, t_1] , \\
x(t_0)\, = \, x_0\, , \quad x(t_1)\, = \, x_1\, ,
\end{array}
\right.
\end{equation}
где $\, x(t) \, = \, \bigl(x^1(t), \ldots , x^d(t) \bigr)$ --
непрерывно-дифференцируемая вектор-функция из отрезка $[t_0, t_1]$
в $\re^d$, $x_0, x_1 \in \re^d$ -- заданные  точки  (граничные
условия), $L\in C\, \bigl( \, [t_0, t_1]\times \re^d \times
\re^d\, \bigr)$ -- заданная непрерывная функция, называемая {\em интегрантом}.

Итак,
среди всех   непрерывно-дифференцируемых  функций, принимающих
заданные значения на концах отрезка, найти такую, которая доставляет
минимум интегральному  функционалу $\cJ(x)$. Вектор-функции $x \in C^1[t_0, t_1]$, удовлетворяющие
данным граничным условиям, будем называть {\em допустимыми}.



\begin{defi}\label{d.feasible-simpl}
Допустимая функция $\hat x \, \in \, C^1\,  [t_0, t_1]\,$ доставляет локальный минимум в
задаче~(\ref{simpl-prob}), если существует $\varepsilon
> 0$ такое, что $\, \cJ (x)\ \ge \ \cJ(\hat x)\, $ для любой
допустимой функции $x$, удовлетворяющей условию $\bigl\| x- \hat x
\bigr\|_{C^1[t_0, t_1]}\,  < \, \varepsilon$.
\end{defi}
Таким образом, точка $\hat x$ доставляет локальный минимум,
если для всех допустимых $x$, расположенных достаточно близко от
$\hat x$ в метрике пространства $C^1$, имеем $\, \cJ (x)\ \ge \
\cJ(\hat x)$. Следующая теорема да\"ет необходимые условия слабого
локального минимума.
\begin{theorem}\label{th.EL}
Предположим, что в задаче~(\ref{simpl-prob}) функции $L, L_x$ и
$L_{\dot x}$ непрерывны.  Если функция~$\hat x$ доставляет
локальный минимум, то функция $L_{\dot x}(t, \hat x (t), \dot {\hat x}(t))$ дифференцируема по $t$ и выполнено уравнение Эйлера-Лагранжа \footnote{Леонард Эйлер (Leonhard Euler, 1707--1783)  -- швейцарско-немецко-российский математик,
один из трех, наряду с Архимедом и Гауссом, величайших математиков в истории человечества.
Жозеф Луи Лагранж  (Joseph Louis Lagrange, 1736-- 1813) -- итальянско-французский математик.}:
\begin{equation}\label{eq.EL}
\frac{d}{dt}\, L_{\, \dot x}\, (t, \hat x , \dot {\hat x})\quad =
\quad  L_{\, x}\, (t, \hat x , \dot {\hat x})\, .
\end{equation}
\end{theorem}
Любое решение~$\hat x (t)$ уравнения~(\ref{eq.EL}) называется {\em
экстремалью.} Уравнение~(\ref{eq.EL}) является фактически системой
из $d$ уравнений $\frac{d}{dt}\, L_{\, \dot x^i}\ = \  L_{\,
x^i}\, , \ i = 1, \ldots , d$. Таким образом, для поиска
экстремали имеется система из $d$ дифференциальных уравнений
второго порядка и $2d$ граничных условий $x(t_0)=x_0, x(t_1)=x_1$
(т.е., $x^i(t_0)=x_{0}^i, \, x^i(t_1)=x_{1}^i\, \, \ i = 1, \ldots,
d$). Согласно теореме~\ref{th.EL},   любая функция, доставляющая
 локальный минимум, является экстремалью. Но, вообще говоря,
не наоборот.

Мы докажем теорему~\ref{th.EL} в случае $d=1$.
Многомерный случай полностью аналогичен, мы оставляем его
читателю. Пространство
функций из $C^1[t_0, t_1]$, удовлетворяющих условию $h(t_0) =
h(t_1)=0$ будем обозначать $C_0^1[t_0, t_1]$ или просто $C_0^1$.
Доказательство опирается на следующее утверждение, принадлежащее
немецкому математику П.Д.Г. Дюбуа-Реймону (1831 -- 1889).

\begin{lemma}\label{l.dbr}[Du Bois-Reymond, 1873].
Пусть $a(t), b(t) \in C\, [t_0, t_1]$, и  для любой функции $h \in
C_0^1[t_0, t_1]$ имеем
$$
\int_{t_0}^{t_1}\, \Bigl(\, a(t) \, h(t) \, + \, b(t)\, \dot h
(t)\, \Bigr)\, d t\ = \ 0\, .
$$
Тогда функция $b(t)$ непрерывно дифференцируема и $\,  \dot b(t)\, =
\, a(t)$.
\end{lemma}
{\tt Доказательство.} Пусть $A(t)$ -- любая  первообразная функции
$a(t)$, т.е., $A(t)\, = \, \int_{t_0}^{t}\, a(\tau)\, d \tau \, +
\, K$. Интегрируя по частям, имеем
$$
0 \ = \ \int_{t_0}^{t_1}\, \Bigl(\, a(t) \, h(t) \, + \, b(t)\,
\dot h (t)\, \Bigr)\, d t\ = \ A(t)h(t)\Bigl|_{t_0}^{t_1}\ + \
\int_{t_0}^{t_1}\, \Bigl(\, -\, A(t)  \, + \, b(t)\, \Bigr)\, \dot
h (t)\, \, d t\, .
$$
Теперь  учитываем, что $h(t_0)\, =\, h(t_1)\, = \, 0\, $:
$$
\int_{t_0}^{t_1}\, \Bigl(\, -\, A(t)  \, + \, b(t)\, \Bigr)\, \dot
h (t)\, \, d t\ \quad = \quad 0\, .
$$
Это равенство выполнено для любой функции~$h \in  C_0^1[t_0,
t_1]$. Выберем теперь нужную функцию~$h$. Для этого подберем
константу $K$ таким образом, чтобы интеграл функции $-\, A(t)  \,
+ \, b(t)$ по отрезку $[t_0, t_1]$ был равен нулю. Тогда функция
$h(t)\, = \, \int_{t_0}^t\, \bigl( -\, A(\tau)\, + \, b(\tau)\,
\bigr)\, d \tau\, $ принадлежит~$ C_0^1[t_0, t_1]$, и при этом
$$
\int_{t_0}^{t_1}\, \Bigl(\, -\, A(t)  \, + \, b(t)\, \Bigr)\, \dot
h (t)\, d t\ \ = \ \int_{t_0}^{t_1}\, \Bigl(\, -\, A(t)  \, + \,
b(t)\, \Bigr)^2\, d t\ = \ 0\, .
$$
Следовательно, $\, b(t)\, \equiv \,  A(t)$. Поэтому $b \in
C^1[t_0, t_1]$ и $\ \dot b(t)\, =\, a(t)$.


{\hfill $\Box$}
\smallskip


{\tt Доказательство теоремы~\ref{th.EL}.}
Для любой функции $h \in C^1_0$ рассмотрим
следующую функцию одного переменного: $\varphi(\lambda) = \cJ(\hat x +
 \lambda h)$.
Это -- ограничение функционала $\cJ$ на прямую $\{\hat x + \lambda h \ | \ \lambda \in \re\}$,
проведенную в пространстве~$C^1[t_0, t_1]$ через точку~$\hat x$ в направлении вектора~$h$.
Если функционал $\cJ$ достигает локального минимума в точке~$\hat x$, то
$\varphi$ достигает локального минимума в точке $\lambda = 0$.
Так как частные производные $L_{x}$ и
$L_{\dot x}$ непрерывны, то функция $\, \varphi (\lambda)\, = \,
L(t, \hat x + \lambda h, \dot {\hat x} + \lambda \dot h)$ дифференцируема по
$\lambda$. Пользуясь правилом дифференцирования функции многих
переменных и дифференцированием интеграла
 по параметру, имеем
% \begin{figure}[htb]
% \center
% \includegraphics[scale=0.3]{lemma-6.3.eps}
% \caption{{\footnotesize Вариация функции~$x(t)$}}
% \label{lemma-6.3}       % Give a unique label
% \end{figure}
 $$
0 \ = \ \dot \varphi(0)\ = \ \int_{t_0}^{t_1}\, \, \frac{d}{d \lambda} \, L(t, \hat x+\lambda h,
\dot {\hat x} + \lambda \dot h)\, \Bigr|_{\lambda =0} \, d t   \, \quad =
\quad \int_{t_0}^{t_1}\, \Bigl(\, L_{\, x} \, h \, + \, L_{\, \dot
x}\, \dot h\, \Bigr)\,  d t\, .
 $$
Применяя теперь лемму~\ref{l.dbr} для $a = L_x$ и  $\ b = L_{\dot x}$, завершаем доказательство.


{\hfill $\Box$}
\smallskip

Заметим, что уравнение Эйлера-Лагранжа дает лишь необходимое условие локального минимума.
Для того, чтобы доказать, что найденная экстремаль действительно дает локальный или абсолютный минимум, есть несколько способов. Иногда можно показать прямой подстановкой, что $\cJ(\hat x + h) \ge \cJ(\hat x)$ для всех
$h \in C^1_0$ и следовательно $\hat x$ доставляет абсолютный минимум, либо, что это неравенство верно для всех малых по норме $h \in C^1_0$, и тогда $\hat x$ -- локальный минимум. В ряде случаев можно воспользоваться выпуклостью. Как известно, если функция
$f$ выпуклая, то  условие
$f'(x) = 0$ не только необходимо, но и достаточно для того, чтобы точка $x$ доставляла абсолютный минимум. Аналогичный факт верен и для функционалов. Мы примем его без доказательства:
\begin{cor}\label{c.2conv}
Если интегрант в простейшей задаче является выпуклым функционалом
от~$x$, т.е.,
$$
L\Bigl(\, t\, , \, (1-\lambda)x  + \lambda y\, , \,
(1-\lambda)\dot x  + \lambda \dot y\, \Bigr)\quad \le \quad
(1-\lambda)\, L(t, x, \dot x)\ + \ \lambda \, L(t, y, \dot y)
$$
для любых допустимых $x, y \in C^1[t_0, t_1]$ в любой точке $t \in
[t_0, t_1]$, то уравнение Эйлера-Лагранжа является достаточным
условием абсолютного минимума.
\end{cor}
\begin{ex}\label{ex.prost1}

Найти минимальное значение интеграла $\cJ(x) = \int_{0}^1  \dot x^2 \, dt$
при условиях $x(0)= 1, x(1) = 2$.
{\em Подынтегральная функция неотрицательна, поэтому~$\cJ(x) \ge 0$ для любой функции~$x$.
Однако, значение $\cJ(x) = 0$ не достигается. В самом деле, если интеграл равен нулю, то $\dot x \equiv 0$,
a значит $x(t)$ -- тождественная константа, что противоречит начальным условиям: $x(0)= 1, x(1) = 2$.
Поэтому, решить задачу столь простым рассуждением не удается. Применим теорему~\ref{th.EL}.
Имеем $L_{\dot x} = 2\dot x, \, L_{x} = 0$, поэтому уравнение Эйлера-Лагранжа имеет вид $2\ddot x = 0$,
откуда $x = at + b$. Подставляя начальные условия, получаем единственную экстремаль $\hat x (t) = t+1$.
Так как функционал $\cJ(x)$ выпуклый, эта экстремаль дает абсолютный минимум. Поэтому минимальное значение
интеграла равно $\int_0^1 {\dot {\hat x}}^2 \, dt = \int_0^1  \, dt = 1$.

}
\end{ex}



В некоторых частных случаях уравнение Эйлера-Лагранжа может быть
сведено к уравнению первого порядка. Введем еще два определения:
интеграла импульса $p(t)\, = \, L_{\, \dot x}$ и интеграла
энергии~$\, H(t)\, = \, \dot x \, L_{\dot x}\,  - \, L$.
\begin{prop}\label{p.impulse}
Если интегрант $L(t)$ не зависит явно от $\, \dot x$, т.е., $\,
L(t, x, \dot x)\, = \, L(t, x)$, то уравнение Эйлера-Лагранжа
равносильно уравнению $L_x(t)\, \equiv \, 0$.

Если интегрант $L(t)$ не зависит явно от $\, x$, то уравнение
Эйлера-Лагранжа равносильно уравнению $p(t)\, = \,  L_{\,
\dot x}(t)\, \equiv \, {\rm const}$.

Если интегрант $L(t)$ не зависит явно от $\, t$, то из уравнения
Эйлера-Лагранжа следует, что $ H(t)\, = \,  L_{\, \dot
x} {\hat x}\, - \, L\, \equiv \, {\rm const}$. Если известно,
что экстремаль $\hat x$ не является тождественной константой ни на
каком интервале, то верно и обратное: из уравнения $ H \,
\equiv \, {\rm const}$ следует уравнение Эйлера-Лагранжа.
\end{prop}
{\tt Доказательство.} Первые два пункта очевидны, докажем третий.
Имеем
$$
\frac{d}{dt}\, H\, = \,\, \Bigl( \, L_{\dot x}\, \Bigr)\, \dot x \
+ \ L_{\, \dot x}\, \ddot x \ - \ L_{\,  x}\, \dot x \ -  \ L_{\,
\dot x}\, \ddot x \ = \ \Bigl(\, \frac{d}{dt}\,  \, L_{\dot x}\, -
\, L_{\, x}\, \Bigr)\, \dot x.
$$
Поэтому из уравнения Эйлера-Лагранжа следует, что $ H' \equiv
0$, т.е., $\,  H \,  \equiv \, {\rm const}$. Если
функция~$\dot {\hat x}(t)$ не обращается в ноль ни на каком
интервале, то функция $\frac{d}{dt}\,  \,  L_{\dot x}\, - \,
 L_{\, x}$ равна нулю на всюду плотном подмножестве отрезка
$[t_0, t_1]$, а значит (в силу непрерывности), и на всем отрезке.

{\hfill $\Box$}
\smallskip

\begin{ex}\label{ex.gilb}(Пример Гильберта).
{\em Рассмотрим задачу
\begin{equation}\label{eq.gilb}
\left\{
\begin{array}{l}
\cJ(x) \ = \ \int\limits_0^1 \, t^{2/3}\, \dot x^2\, d t \ \ \to \ \min\, ;\\
x(0) = 0\, , \ x(1) = 1\, .
\end{array}
\right.
\end{equation}
Так как интегрант не зависит явно от $x$, то уравнение Эйлера-Лагранжа дает $\frac{d}{dt}\, L_{\dot x} = 0$,
откуда $L_{\dot x} = {\rm const}$, следовательно $2 t^{2/3} \dot x = c$. Единственное решение
этого дифференциального уравнения, удовлетворяющее начальным условиям, это $\, \hat x (t) = t^{1/3}$.
Докажем, что $\hat x \in {\rm absmin}$. Можем, как и в примере~\ref{ex.prost1}, воспользоваться выпуклостью функционала.
А можем и прямой подстановкой: для любой
допустимой вариации $h\in C^1_0[0,1]$ имеем
$$
\cJ(\hat x + h) - \cJ(\hat x) \, = \,
\int_{0}^1 [t^{2/3}(\dot {\hat x}+\dot h)^2 - t^{2/3}\dot {\hat x}^2]\, d t \, = \,
\int_0^1 [2 t^{2/3} \dot {\hat x} \dot h \, + \,  t^{2/3}\dot h^2]\, d t \, \ge \,
$$
$$
 \int_0^1 2 t^{2/3} \dot {\hat x} \dot h \, d t \, = \, \int_0^1 2 t^{2/3} \frac{1}{3} t^{-2/3}\, \dot h\, d t \, = \,  \frac{2}{3} \bigl( h(1) - h(0) \bigr)\, = \, 0\, .
$$
Таким образом, $\hat x$ доставляет абсолютный минимум. Тем не менее, эта функция не является
экстремалью! В самом деле, экстремаль, по определению, принадлежит $C^1[0,1]$, а функция $\hat x$ -- нет. Поэтому, в данной задаче вообще нет допустимых экстремалей. Абсолютный минимум достигается, но  на функции, которая 
не принадлежит пространству~$C^1$. 

Этот пример показывает, что в некоторых случаях пространство $C^1$ слишком узко для решения простейшей задачи, и имеет смысл искать
экстремали в более широких пространствах, например в пространствах Соболева.
}
\end{ex}
\begin{ex}\label{ex.area}(Задача о минимальной площади поверхности вращения).
{\em Общая задача Лагранжа-Плато состоит в нахождении поверхности минимальной площади,
содержащей заданное компактное множество в $\re^3$. Мы рассмотрим случай, когда это множество --
два круга радиусом~$1$, причем отрезок между их центрами равен $2a$ и перпендикулярен  плоскостям
 кругов. Мы ограничимся только поверхностями
вращения (что выглядит естественно, но не так просто обосновывается). Если поверхность
 образована вращением графика функции $x(t)$ такой, что $x(-a) = x(a) = 1$, вокруг оси абсцисс, то задача
 формализуется в виде:
\begin{equation}\label{eq.area}
\left\{
\begin{array}{l}
\cJ(x) \ = \ \int\limits_{-a}^a \, 2\pi x \sqrt{1 + \dot x^2}\, d t \ \to \ \min\, ;\\
x(-a) = x(a) = 1\, .
\end{array}
\right.
\end{equation}
%\begin{figure}[htb]
%\center
%\includegraphics[scale=0.4]{catenoid-1.eps}
%\caption{{\footnotesize }}
%\label{catenoid-1}       % Give a unique label
%\end{figure}
Так как интегрант не зависит явно от $t$, можем воспользоваться интегралом энергии:
$H = \, \dot x L_{\dot x} - L \, = {\rm const}$. Вычислив производные и проделав очевидные преобразования,
получаем $\frac{x}{\sqrt{1 + \dot x^2}} = c$, откуда $\, \frac{d x}{\sqrt{\bigl(\frac{x}{c}\bigr)^2-1}}\, = \, d t$.
С помощью замены $x = c \, \cho \tau$, находим решение данного дифференциального уравнения, удовлетворяющее
начальным условиям: $\hat x (t)=  c \, \cho \bigl( \frac{t}{c}\bigr)$. Остается найти параметр $c$ из краевого условия $\hat x (a) = 1$ (условие $\hat x(-a)=1$ будет тогда выполнено в силу ч\"етности функции). Обозначив $s = \frac{a}{c}$,
получаем уравнение $\cho s = \frac{s}{a}$. Пусть число $a_0$ таково, что прямая $y = \frac{s}{a_0}$ касается
графика функции $y = \cho s$. 
%(рис.~\ref{catenoid-2}).
%\begin{figure}[htb]
%\center
%\includegraphics[scale=0.4]{catenoid-2.eps}
%\caption{{\footnotesize }}
%\label{catenoid-2}       % Give a unique label
%\end{figure}
Имеем $a_0 = 0.662\ldots $. При каждом $a < a_0$ прямая пересекает
график в  двух точках, поэтому существует два значения параметра~$c$, т.е., задача имеет две допустимые экстремали.
При $a = a_0$ экстремаль единственна, а при $a> a_0$ экстремалей нет. Последнее объясняется тем, что
при $a> a_0$  площадь любой поверхности вращения становится больше суммы площадей двух кругов радиусом~$1$, поэтому
минимальная поверхность ``распадается'' в объединение двух кругов. В случае $a< a_0$ из двух экстремалей
абсолютный минимум дает одна, соответствующая меньшему из двух значений~$c$, т.е., меньшему
корню уравнения $\, c\, \cho \bigl(\frac{a}{c} \bigr) =1 $; вторая не доставляет даже локального минимума. Это мы доказывать не будем.
}
\end{ex}

\begin{ex}\label{ex.brakhist}\textbf{(Задача о брахистохроне)}.
{\em В 1696 году швейцарский математик
Иоганн Бернулли опубликовал в журнале ``Acta Eruditorum'' первую
задачу, которая относится к вариационному исчислению -- задачу о
брахистохроне:}

Две данные точки вертикальной плоскости соединить линией, по
которой тело, двигающееся под действием силы тяжести с нулевой
начальной скоростью без трения, проходит от одной точки к другой
за наименьшее время.
\smallskip 

{\em Бернулли пригласил к решению задачи своих современников,  обещая
 ``воздать хвалу тому,  кто справится с е\"е решением''. За короткое время он получил три письма с решениями:
 от Лейбница. Лопиталя, от собственного брата -- Якоба Бернулли, и одно анонимное письмо
 (автор которого, впрочем, сразу был опознан по оригинальной  и свежей идее,
 ``Как по когтям узнают льва'' -- это Исаак Ньютон). }
\smallskip 

Формализация. {\em Введем на плоскости систему координат $(t,x)$, начальную точку
поместим в начало координат, а конечную -- в точку $(a, -b)$.
Нужно найти кривую $x(t)$, по которой тело спускается от начальной
точки к конечной за наименьшее время. По закону сохранения энергии
скорость тела в  точке $(t, x)$ равна $\sqrt{-2gx}$.
А дифференциал длины кривой равен~$\sqrt{1+\dot x^2}$. Поэтому, промежуток времени, 
за который тело проходит бесконечно малый участок кривой от точки 
$(t, x)$ до точки $(t + dt, x + dx)$, равен $\frac{\sqrt{1+\dot x^2}}{\sqrt{-2gx}}$. 
Следовательно, полное время, за которое тело проходит кривую, равно интегралу от этой функции 
по отрезку $[0, a]$. Получаем задачу 
\begin{equation}\label{eq.brach}
\left\{
\begin{array}{l}
\int_0^a \frac{\sqrt{1+\dot x^2}\, dt}{\sqrt{-2gx}}\\
x(0)\, = \, 0\, , \ x(a)\, = \, -b\, . 
\end{array}
\right.
\end{equation}  
{\em Решение.}   Имеем $L_{\dot x}\, = \, \frac{\dot x}{\sqrt{1+\dot x^2}\, \sqrt{-2gx}}$, 
 $L_{x}\, = \, -\, \frac{\sqrt{1+\dot x^2}}{2\sqrt{-2gx^3}}$
Следовательно, уравнение Эйлера-Лагранжа имеет вид: 
$$
\left( \frac{\dot x}{\sqrt{1+\dot x^2}\, \sqrt{-2gx}}\right)'\ = \ -\, \frac{\sqrt{1+\dot x^2}}{2\sqrt{-2gx^3}}
\, .
$$
После взятия производной и приведения подобных получаем уравнение второго порядка, которое 
выглядит чрезвычайно сложно. Поэтому, попробуем действовать по-другому, через интеграл энергии.  
Получаем
$$
\dot x \, L_{\, \dot x}\, - \, L\ = \ \frac{\dot x^2}{\sqrt{1+\dot x^2}\, \sqrt{-2gx}} \ - \ 
\frac{\sqrt{1+\dot x^2}\, dt}{\sqrt{-2gx}} \ = \ \frac{1}{{\sqrt{-2gx}} \sqrt{1+\dot x^2}}\, . 
$$
Таким образом, 
$$
{\sqrt{-2gx}} \sqrt{1+\dot x^2} \quad \equiv \ \quad {\rm const}\, .
$$
Выражая из этого равенства $\dot x$ и учитывая, что $x(t)<0$, приходим к дифференциальному уравнению:
$$
-\, \frac{\dot x}{\sqrt{-\, \frac{C}{x}\, - \, 1}}\quad = \quad
1\, ,
$$
или
$$
-\, \frac{d\,  x}{\sqrt{-\, \frac{C}{x}\, - \, 1}}\quad = \quad
d\, t\, .
$$
После замены $\, x\, = \, -\, C\, \sin^2\, \frac{\tau}{2}\, = \,
-\, \frac{C}{2}\, \bigl[ \, 1 \, - \, \cos \, \tau\, \bigr]$,
имеем
$$
t\, = \, \int \ \frac{2\, C\, \sin \frac{\tau}{2}\, \cos
\frac{\tau}{2}\, \frac12\, d\, \tau}{\sqrt{\frac{1}{\sin^2\,
\frac{\tau}{2}}\, - \, 1}}\quad = \quad \int\, C\, \sin^2\, \frac{\tau}{2}\,
d\, \tau\, \quad = \quad \frac{C}{2}\, \Bigl[ \, \tau \ - \
\sin\,  \tau  \, \Bigr].
$$
Получаем уравнение} циклоиды:
$$
\left\{
\begin{array}{lcl}
t & = & \ \frac{C}{2}\, \Bigl[ \, \tau \ - \  \sin\, \tau  \, \Bigr]\\
x & = & -\, \frac{C}{2}\,  \Bigl[\, 1\ - \ \cos\, \tau  \,
\Bigr]\, .
\end{array}
\right.
$$
{\em Эту кривую описывает точка
 окружности диаметра  $C$, когда окружность катится по оси
$OX$.
Отсюда, в частности, следует, что все циклоиды подобны,
коэффициент их подобия -- отношение радиусов окружностей.}
\end{ex}




\bigskip

\begin{center}
\textbf{6.2. Задача Больца. Условия трансверсальности}
\end{center}
\bigskip







{\it  Задачей Больца} [O.Bolza, 1913] называется задача  минимизации следующего
функционала на пространстве~$C^1[t_0, t_1]$:
\begin{equation}\label{Boltz}
\cJ(x)\quad = \quad \int\limits_{t_0}^{t_1}\, L\, \bigl(t, x, \dot
x \bigr)\, d t\ + \ l(x_0, x_1)\quad \to \quad \min\, ,
\end{equation}
где  $\, x(t) \, = \, (x^1, \ldots , x^d)$ -- вектор-функция из~$C^1\,[t_0, t_1]$, $x_0 =
x(t_0), x_1 = x(t_1)$; {\em интегрант} $L\in C\, \bigl( \, [t_0,
t_1]\times \re^d \times \re^d\, \bigr)$ и {\em
терминант} $l \in C\bigl( \, \re^d \times \re^d\, 
\bigr)$ -- заданные функции.

Задачу впервые исследовал немецкий математик
Оскар Больца (1857 –- 1942). Правильнее было бы называть ее задачей Больцы, а не Больца, но здесь мы следуем общепринятой терминологии.

В отличие от простейшей задачи,
задача Больца является безусловной, т.е., никаких дополнительных
условий (например, граничных) на функцию $x$ не накладывается.
Вместо этого граничные значения появляются в терминанте.

\begin{defi}\label{d.feasible-boltz}
Функция $\hat x \, \in \, C^1\, \bigl( \, [t_0, t_1]\, , \, \re^d
\, \bigr)$ доставляет локальный минимум в
задаче~(\ref{Boltz}), если существует $\varepsilon
> 0$ такое, что $\, \cJ (x)\ \ge \ \cJ(\hat x)\, $ для любой
 функции $x \in C^1$, удовлетворяющей условию $\bigl\| x- \hat x
\bigr\|_{C^1[t_0, t_1]}\, < \, \varepsilon$.
\end{defi}
\begin{theorem}\label{th.Boltz}
Предположим, что в задаче~(\ref{Boltz}) функции $L, L_x, L_{\dot
x}, l_{\, x_0}$ и $\, l_{\, x_1}\, $ непрерывны.  Если функция
$\hat x$ доставляет локальный минимум, то выполнено
уравнение Эйлера-Лагранжа~(\ref{eq.EL}) и условия
трансверсальности:
\begin{equation}\label{eq.trans}
 L_{\, \dot x}(t_0, \hat x (t_0), \dot {\hat x}(t_0)) \ = \  l_{\, x_0}\, (\hat x_0, \hat x_1)\, ;
\qquad  L_{\, \dot x}\, (t_1, \hat x (t_1), \dot {\hat x}(t_1)) \
= \ -\, l_{\, x_1}\, (\hat x_0, \hat x_1),
\end{equation}
или в короткой записи $\, \hat L_{\, \dot x}(t_k)\, = \, (-1)^k
\hat l_{\, x_k}\, , \ k = 0,1$.
\end{theorem}
{\tt Доказательство} проведем для случая $d=1$.
Зафиксируем значения $x(t_0) = x_0, x(t_1) = x_1$. Тогда
величина $l(x_0, x_1)$ постоянна. Следовательно, мы получаем простейшую задачу.
Согласно теореме~\ref{th.EL}, если $\hat x \in {\rm locmin}$, то
функция $L_{\dot x}(t, \hat x, \dot {\hat x})$ непрерывно дифференцируема и
выполнено уравнение Эйлера-Лагранжа. Вернемся теперь к исходной задаче,
со свободными значениями $x(t_0), x(x_1)$. Для призвольного $h \in C^1[t_0, t_1]$, функция
$\varphi(\lambda) = \cJ(\hat x +
 \lambda h)$ достигает локального минимума в точке $\lambda = 0$, поэтому
 $$
0 \ = \ \dot \varphi(0)\ = \ \quad \int_{t_0}^{t_1}\, \Bigl(\, L_{\, x} \, h \, + \, L_{\, \dot
x}\, \dot h\, \Bigr)d t\, \ + \ l_{x_0}h(t_0) \ + \ l_{x_1}h(t_1)\, .
 $$
 Интегрируя по частям и учитывая уравнение Эйлера-Лагранжа,
получаем
$$
\int_{t_0}^{t_1}\, \Bigl(\, L_{\, x} \, h \, + \,
 L_{\, \dot x}\, \dot h\, \Bigr)\, d t\ =  \
  L_{\, \dot x}(t)\,  h(t)\, \Bigl|_{t_0}^{t_1} \ + \ \int_{t_0}^{t_1}\, \Bigl(\, - \, \frac{d}{dt} L_{\, x}  \, + \,
 L_{\, x}\, \Bigr)\, h\,\, d t \ = \ L_{\, \dot x}(t)\,  h(t)\, \Bigl|_{t_0}^{t_1}\, .
$$
 Таким образом, 
$$
 L_{\, \dot x}(t)\,  h(t)\, \Bigl|_{t_0}^{t_1}  \ + \
l_{\, x_0}\, h(t_0)\ +  l_{\, x_1}\, h(t_1) \quad = \quad 0\, .
$$
Итак,
$$
\Bigl(  l_{\, x_0} \ - \  L_{\, \dot x}(t_0)\Bigr)\,
h(t_0) \quad + \quad \Bigl( l_{\, x_1} \ + \  L_{\, \dot
x}(t_1)\Bigr)\, h(t_1)\quad = \quad 0\,
$$
для любой функции~$h \in C^1[t_0, t_1]$. Значит данная сумма равна
нулю при всех значениях $h(t_0)$ и $h(t_1)$, следовательно, $\,
 l_{\, x_0} \, - \,  L_{\, \dot x}(t_0)\, = \, 0\, $ и $\,
l_{\, x_1} \, + \, L_{\, \dot x}(t_1)\, = \, 0$.

{\hfill $\Box$}
\smallskip

Аналогично следствию~\ref{c.2conv} получаем
\begin{cor}\label{c.2conv-boltz}
Если интегрант в задаче Больца является выпуклым функционалом от
$x$, а терминант -- выпуклой функцией на $\re^2$,  то уравнение
Эйлера-Лагранжа и условия трансверсальности достаточны для
абсолютного минимума.
\end{cor}
\medskip

\begin{center}
\textbf{6.3. Изопериметрическая задача}
\end{center}
\bigskip

{\em  Изопериметрической задачей} называется задача минимизации
интегрального функционала:
\begin{equation}\label{Isom}
\cJ_0(x)\quad = \quad \int\limits_{t_0}^{t_1}\, f_0\, \bigl(t, x,
\dot x \bigr)\, d t\ \quad \to \quad \min\, ,
\end{equation}
при ограничениях $\, \cJ_k(x)\ = \ \int\limits_{t_0}^{t_1}\, f_k\,
\bigl(t, x, \dot x \bigr)\, d t\ \ = \ a_k\, , \ k \ = \ 1\, , \,
\ldots \, , m, $ и при условиях  ${x(t_0)\ = \ x_0\, , \quad
x(t_1)\ = \ x_1\, ,
 \ x(t) \, \in \,
C^1\,[t_0, t_1]}$.  При этом $f_k\in C([t_0,
t_1]\, \times \re^d\, \times \re^d)\, , \, k =0, \ldots , m$ -- заданные
функции, $a_k$ -- заданные числа. Функция $\hat x \, \in \, C^1\,
[t_0, t_1]\, , $ доставляет
локальный минимум, если существует $\varepsilon
> 0$ такое, что $\, \cJ_0 (x)\ \ge \ \cJ_0(\hat x)\, $ для любой
 функции~$x$, удовлетворяющей всем условиям задачи~(\ref{Isom}) и условию ${\bigl\| x- \hat x
\bigr\|_{C^1[t_0, t_1]}\, < \, \varepsilon}$.

Решение изопериметрической задачи опирается на известное {\em правило множителей Лагранжа}.
Если точка $\hat x$ является локальными  минимумом функции $g_0(x)$ при ограничениях
$g_1(x) = c_1, \ldots , g_m(x) = c_m$, то существуют НЕРОН (НЕ Равные Одновременно Нулю)
действительные множители $\lambda_0, \ldots , \lambda_m$ такие, что производная суммы
$\cL(x) = \sum_{i=0}^m \lambda_i g_i(x)$ обращается в ноль в точке~$\hat x$.
Поэтому, вполне естественно, что в решении изопериметрической задачи используется принцип Лагранжа.
Следующую теорему мы примем без доказательства:
\begin{theorem}\label{th.isop}
Предположим, что в изопериметрической задаче функции  $f_k,
(f_k)_x, (f_k)_{\, \dot x}$ непрерывны при всех $k = 0, \ldots  ,
m$. Если функция $\hat x$ доставляет  локальный минимум, то
существуют НЕРОН множители $\lambda_0, \ldots , \lambda_m$ такие,
что интегрант $L(t, x , \dot x)\, = \, \sum_{k=0}^m \, \lambda_k\,
f_k(t, x , \dot x)\, $ удовлетворяет при $x = \hat x$  уравнению
Эйлера-Лагранжа~(\ref{eq.EL}).
\end{theorem}

\begin{ex}\label{ex.Dido}
{\em (Задача Дидоны). } Среди всех линий данной длины с концами на
данной прямой линии найти ту, которая ограничивает фигуру
наибольшей площади.

{\em Данную задачу можно считать первой задачей теории экстремма,
поскольку она была поставлена в IX в. до н.э., задолго до Архимеда
или Евклида.   История, которую поведал древне-римский поэт Публий
Вергилий Марон в поэме ``Энеида'' происходила в IX веке до н.э. на
ближнем Востоке. Финикийская царевна Дидона, дочь Финикийского
царя, спасаясь от преследований  своего брата, отправилась на
корабле в сторону  запада, вдоль южного берега Средиземного моря.
Остановилась в живописном месте на берегу Тунисского залива.
Переговоры с предводителем местного племени Ярбом о выделении
участка земли не дали результата. Ярб заявил, что даст царевне
столько земли, сколько можно ограничить одной бычьей шкурой.
Дидона распорядилась разрезать шкуру на тонкие полоски, из которых
сплели длинную вер\"евку. Этой веревкой о огородили фигуру
наибольшей площади от прямолинейного берега. Так была основана
крепость, а рядом -- город Карфаген.

Равносильная задача: основное изопериметрическое неравенство.  }
Среди всех замкнутых кривых данной длины наибольшую площадь
ограничивает круг.

{\em Эта задача равносильна задаче Дидоны. Достаточно сделать
осевую симметрию относительно линии берега. Строго это было
доказано лишь в XX веке, в книге В.Бляшке ``Круг и шар''. Мы
запишем задачу Дидоны в следующем виде: } Среди всех функций
$x(t)$, для которых $x(t_0)= x_0, x(t_1) = x_1$, а график имеет
заданную длину $l$ найти ту, график которой ограничивает
наибольшую площадь. {\em Если доказать, что ответ -- дуга
окружности, то все будет сделано. И в задаче Дидоны, и в основном
изопериметрическом неравенстве получаем, что локально
экстремальная кривая является окружностью (для любых двух точек
кривой, расположенных достаточно близко друг от друга, дуга
кривой, соединяющая их, является дугой окружности.) Отсюда следует
основное изопериметрическое неравенство и решение задачи Дидоны --
полукруг. Итак:
$$
\left\{
\begin{array}{l}
\int\limits_{t_0}^{t_1}\, x(t) \, d \, t\quad \to \quad \max\, ,\\
\int\limits_{t_0}^{t_1}\, \sqrt{1\, +\, \dot x^2(t)}\ d t \ = \ l\,  ,\\
x(t_0) \, = \, x_0,\ x(t_1)= x_1\, .
\end{array}
\right.
$$
Согласно теореме~\ref{th.isop} интегрант $L \, = \, - \, \lambda_0\, x
\, + \, \lambda_1\, \sqrt{1\, +\, \dot x^2}$ удовлетворяет
уравнению Эйлера-Лагранжа. Так как интегрант не зависит явно от
$t$, то можно воспользоваться интегралом энергии
(предложение~\ref{p.impulse}): $H(t)\, = \dot x\, L_{\,
\dot x}\, - \, L\, \equiv \, c$. В нашем случае получаем (для
удобства далее мы не ставим крышки над $x$):
$$
H(t)\ = \ \frac{\lambda_1\, \dot x^2}{\sqrt{1\, +\, \dot
x^2}}\ - \ \lambda_1 \, \sqrt{1\, +\, \dot x^2} \ + \ \lambda_0 \,
x\quad \equiv \quad c\, ,
$$
откуда
$$
 \ \lambda_0 \, x \ - \ \frac{\lambda_1}{\sqrt{1\, +\, \dot
x^2}}\quad = \quad  c\, .
$$
Если $\lambda_0 = 0$, то $\hat x (t)\equiv \, {\rm const}$, значит
$x(t)$ -- линейная функция, и ее график -- прямая. Это -- частный
случай, когда расстояние между точками $(t_0, x_0)$ и $(t_1, x_1)$
равно $l$. В этом случае задача имеет только одну подходящую
функцию -- линейную (для остальных длина графика будет больше
$l$). Если $\lambda_0 \ne 0$, то, полагая $\lambda_0 = 1$ и
проведя элементарные преобразования, получаем
$$
\frac{\lambda_1^2}{(x-c)^2}\quad = \quad 1\, + \, \dot x^2\, .
$$
Следовательно,
$$
\frac{(x-c)\, d x}{\sqrt{\lambda_1^2 \, - \, (x-c)^2}}\quad =
\quad d\, t\, .
$$
Интегрируя, получаем $\, \sqrt{\lambda_1^2 \, - \, (x-c)^2}\, = \,
-\, t \, + \, c_0$, где $c_0$ -- некоторая константа. Таким
образом, $\, (t-c_0)^2 \, + \, (x-c)^2\, = \, \lambda_1^2$. Это --
уравнение окружности с центром $(c_0, c)$ и радиусом
$|\lambda_1|$.

Заметим, что мы не доказали, что дуга окружности является
решением задачи. Мы лишь доказали, что она является единственной
экстремалью. }
\end{ex}
\medskip

\begin{center}
\textbf{6.4. Условия второго порядка в простейшей задаче. \\ Уравнение Якоби}
\end{center}
\bigskip

Как для функции одной переменной $g(x)$ доказать, что она достигает локального минимума в точке  $x$?
Сначала проверить, что $\dot g(x) = 0$, а потом вычислить вторую производную.
Если $\ddot g (x) > 0$, то $x$ -- точка локального минимума. Условия на вторую производную функции 
называются {\em условиями второго порядка}. Мы получим сейчас подобные условия
 для задачи вариационного исчисления.

Рассмотрим функционал $\cJ(x)\,  = \, \int_{t_0}^{t_1}L(t, x, \dot x)\, d t$ в простейшей задаче~(\ref{simpl-prob}).
Предполагаем, что вторые частные производные $L_{x\, x}\, , \,
L_{x\, \dot x}\, , \,  L_{\dot x \, \dot x}$ существуют и
непрерывны. Тогда для функции $\varphi (\lambda) = \cJ(\hat x + \lambda h)$ получаем
\begin{equation}\label{eq.second1}
\ddot \varphi(0)\quad = \quad \int_{t_0}^{t_1}\, \Bigl( \, L_{x\,
x}\, { h}^2\, + \, 2\, L_{ x\, \dot x}\, h\, {\dot h}\, +\,
L_{\dot x\, \dot x}\, {\dot h}^2   \, \Bigr)\, d t\, .
\end{equation}
Если $\hat x \in {\rm locmin} \, \cJ (x)$, то $\ 0 \in {\rm locmin}\, \varphi(\lambda)$, и
следовательно  $\ddot \varphi(0) \ge 0$. Итак, получаем:
\begin{prop}\label{p.second2}
Если $\hat x$ доставляет локальный минимум в простейшей задаче~(\ref{simpl-prob}), то
\begin{equation}\label{eq.second2}
\quad \int_{t_0}^{t_1}\, \Bigl( \, L_{x\,
x}\, { h}^2\, + \, 2\, L_{ x\, \dot x}\, h\, {\dot h}\, +\,
L_{\dot x\, \dot x}\, {\dot h}^2   \, \Bigr)\, d t\quad \ge \quad 0\, , \qquad h \in C_0^1[t_0, t_1].
\end{equation}
\end{prop}
Наша первая цель -- получить условия на экстремаль~$\hat x$, гарантирующее выполнение
свойства~(\ref{eq.second2}).
\begin{defi}\label{d.Legandre}
Экстремаль $\hat x$ в простейшей задаче удовлетворяет условию
Лежандра (усиленному условию Лежандра), если $L_{\dot x \,
\dot x}(t, \hat x(t), \dot {\hat x}(t))\, \ge \, 0\, $ ($> 0$) для всех
$\, t \in [t_0, t_1]$.
\end{defi}
Это условие вывел французский математик  Адриен Мари Лежандр (1752 -- 1833).
\begin{theorem}\label{th.Legandre}[A.M.Legendre].
Если выполнено условие~(\ref{eq.second2}), то выполнено условие Лежандра.
\end{theorem}
В доказательстве мы используем следующий известный и вполне очевидный факт.
Обозначим через $PC^1[t_0, t_1]$ пространство функций, непрерывных на $[t_0, t_1]$
и кусочно непрерывно-дифференцируемых. Это значит, что отрезок $[t_0,t_1]$
можно разбить на конечное число отрезков, на каждом из которых функция дифференцируема,
ее производная непрерывна, а в концах существуют односторонние пределы производной.
Например, функция $x(t) = |t|$ принадлежит $PC^1[-1, 1]$. Функции пространства $PC^1[t_0, t_1]$,
обращающиеся в ноль в концах отрезка, формируют пространство~$PC^1_0[t_0, t_1]$.
 \begin{lemma}\label{l.skrug}(О скруглении углов)
Для функционала простейшей задачи~$\cJ(x)$ выполнено
$$
\inf \, \Bigl\{ \, \cJ(x) \ \Bigr| \ x \in C^1\, , \ x(t_i) = x_i, \, i = 0, 1\, \Bigr\}\quad = \quad
\inf \, \Bigl\{ \, \cJ(x) \ \Bigr| \ x \in PC^1\, , \ x(t_i) = x_i, \, i = 0, 1\, \Bigr\}
$$
\end{lemma}

{\tt Доказательство теоремы~\ref{th.Legandre}.} От противного, пусть $L_{\dot x\, \dot
x}(\tau )< 0$ при некотором $\tau  \in [t_0, t_1]$. В силу
непрерывности можно считать, что $\tau \in [t_0 + \delta  , t_1 -
\delta]$ и что $ L_{\, \dot x\, \dot x}(t )< -\varepsilon$ на
отрезке $[\tau - \delta, \tau + \delta]$. Положим
$$
\bar h(t)\quad = \quad \left\{
\begin{array}{lll}
\delta \, - \, |\, t \, - \, \tau\, |\ & \mbox{при} \ & \ |\, t \, - \, \tau\, |\ < \, \delta\, ; \\
0\ & \mbox{при} \ & \ |\, t \, - \, \tau\, |\ \ge \, \delta\, .
\end{array}
\right.
$$
Тогда
$$
\int_{\tau -
\delta}^{\tau + \delta}\, \Bigl( \, L_{x\, x}\, { \bar h}^2\, + \,
2\, L_{ x\, \dot x}\, \bar h\, {\dot {\bar h}}\, + \, L_{\dot  x\,
\dot  x}\, {\dot {\bar h}}^2
  \, \Bigr)\,
d t\,
$$
В подынтегральной сумме первое слагаемое равно $O(\delta^2)$,
второе -- $O(\delta)$, а третье по модулю не меньше $\varepsilon$.
При малых $\delta$
 данный интеграл отрицателен. Это еще не конец доказательства, поскольку
$\bar h \notin C_0^1[t_0, t_1]$. Воспользовавшись теперь леммой о
скруглении углов (лемма~\ref{l.skrug}), получаем, противоречие.

{\hfill $\Box$}
\smallskip

\begin{cor}\label{c.Legandre}
Если в простейшей задаче вторые производные $L_{x\, x}\, , \,
L_{x\, \dot x}\, , \,  L_{\dot x \, \dot x}$ существуют и
непрерывны, а  $\hat x$ -- локальный минимум, то на н\"ем
выполнено
 условие Лежандра.
\end{cor}

Итак, мы получили одно условие второго порядка на слабый минимум
-- условие Лежандра. Для формулировки  дальнейших условий мы
вначале повторим следующее (ошибочное) рассуждение принадлежащее
Лагранжу. В н\"ем ``доказывается'', что для выполнения
условия~(\ref{eq.second2}) достаточно усиленного  условия Лежандра.

Заметим вначале, что $\int_{t_0}^{t_1}\, 2B\, h \, \dot h \, d t
\, = \, -\, \int_{t_0}^{t_1}\, \dot B \, { h}^2 \, d t$
(проинтегрировали по частям) для любой функции $B\in C^1[t_0,
t_1]$ и любой $h \in C_0^1[t_0, t_1]$. Проделав это со вторым
слагаемым в интеграле $\, \ddot \varphi(0)\ = \ \int_{t_0}^{t_1}\,
\Bigl( \, L_{x\, x}\, { h}^2\, + \, 2\, L_{ x\, \dot x}\, h\,
{\dot h}\, +\, L_{\dot x\, \dot x}\, {\dot h}^2   \, \Bigr)\, d
t\, , $ получим
$$
\ddot \varphi(0)\quad = \quad \int_{t_0}^{t_1} \, p(t)\, {\dot
h}^2(t)\, +\, q(t)\, h^2(t)\, d  t\, ,
$$
где $p, q$ -- некоторые функции. Пусть выполнено усиленное условие
Лежандра: $p(t)>0$ для всех $t \in [t_0, t_1]$. Прибавим под
интегралом выражение $\frac{d}{dt}\bigl[\, \omega(t)\, h^2(t)
\bigr]$. Оно не изменит значение интеграла, поскольку $h \in
C^1_0[t_0, t_1]$. Подберем функцию $\omega(t)$ так, чтобы
подынтегральное выражение $p\, {\dot h}^2 \, + \, 2\omega \, \dot
h \, h \, + \, (\dot \omega + q)\, h^2$ стало полным квадратом.
Это означает, что
\begin{equation}\label{eq.rikatti1}
\dot \omega \, + \, q \ = \ p^{-1}\, \omega^2.
\end{equation}
Это -- дифференциальное уравнение Рикатти. Решив его, получаем функцию $\omega (t)$, для которой 
\begin{equation}\label{eq.result-rikatti1}
\ddot \varphi(0) \quad = \quad \int_{t_0}^{t_1} \,  \Bigl(\,
p^{1/2}\, \dot h\, + \, p^{-1/2}\, \omega \, h\,  \Bigr)^2 \, d t
\ \ge \ 0.
\end{equation}
Таким образом, усиленное условие Лежандра достаточно для~(\ref{eq.second2}). Однако, приведенное рассуждение
неверно, и вот соответствующий контрпример:
\begin{ex}\label{ex.oscyl1}(Гармонический осциллятор).
{\em Пусть $\cJ (x)\, = \, \int_{0}^{2\pi}\, \bigl( \, {\dot x}^2
\, - \, x^2\, \bigr)\, d t$. Поскольку $L_{\dot x \, \dot x}\, =
\, 2 \, > \, 0$, усиленное условие Лежандра выполнено. Тем не
менее, вторая производная $\ddot \varphi(0)\, = \, 2\,
\int_{0}^{2\pi}\, \bigl( \, {\dot h}^2 \, - \, h^2\, \bigr)\, d t$
равна $\, - \, \frac{3\pi}{2}$ при $\, h \, = \, \sin \,
\frac{t}{2}\, \in \, C^{1}_0[0\, , \, 2\pi]$. Следовательно,
квадратичный  функционал~$\ddot \varphi$ не является
неотрицательно-определ\"енным, несмотря на то, что  удовлетворяет
усиленному условию Лежандра.
 }
\end{ex}


В ч\"ем же дело? Ответ: уравнение Рикатти~(\ref{eq.rikatti1}) не
обязано иметь решение $\omega(t)$, непрерывное на вс\"ем отрезке~$[t_0, t_1]$.
Решение существует лишь локально, и при продолжении может уйти на
бесконечность в какой-то точке отрезка.
\begin{ex}\label{ex.rikatti}
{\em Для функционала из предыдущего примера $\ddot \varphi(0) \, = \, 2\,
\int_{0}^{2\pi }\, \bigl( \, {\dot h}^2 \, - \, h^2\, \bigr)\, d
t$, и уравнение Рикатти~(\ref{eq.rikatti1}) имеет вид $\dot \omega
\, - \, 1 \, = \, \omega^2$. Отсюда ${\rm arctg}\, \omega\, = \, t
+ C$, т.е. $\omega = {\rm tg}(t+C)$. Поскольку  длина отрезка
больше $\pi$,  функция ${\rm tg}(t+C)$ обязательно имеет на н\"ем особую
точку, в которой она обращается в бесконечность. }
\end{ex}


Тем не менее, рассуждения Лагранжа можно исправить, что и было
сделано впоследствии Карлом Густавом Якоби (Carl Gustav Jacobi, 1804-1851), немецким
математиком, родным братом известного русского физика Бориса Семеновича Якоби. Для дальнейших
рассуждений мы перейд\"ем к более общим обозначениям. Рассмотрим
квадратичный функционал вида
$$
\cK(h)\, = \, \int_{t_0}^{t_1}\, \Bigl(\, Ah^2\, + 2\, B\, h \,
\dot h \, + \, C\, {\dot h}^2\, \Bigr)\, d t\, ,
$$
где $A, B, C\in C^1[t_0, t_1]$. В частности, тот же вид имеет
$\ddot \varphi(0)$. {\em Уравнением Якоби} называется уравнение
Эйлера-Лагранжа на этот функционал:
\begin{equation}\label{Jakobi-gen}
\frac{d}{dt}\, \Bigl(\, B\, h  +   C\, \dot h \, \Bigr)\quad =
\quad A\, h  \ +\  B\, \dot h\, .
\end{equation}
Так, для функционала~(\ref{eq.second1}) уравнение Якоби выглядит так:
\begin{equation}\label{Jakobi}
\frac{d}{dt}\, \Bigl(\, {\hat L}_{x \, \dot x}\, h  +   {\hat
L}_{\dot x \, \dot x}\, \dot h \, \Bigr)\quad = \quad {\hat L}_{x
\,  x}\, h  +\  {\hat L}_{x \, \dot x}\, \dot h\, .
\end{equation}
Уравнение Якоби -- однородное дифференциальное уравнение второго
порядка. Если $h(t)$ -- его нетривиальное (т.е., ненулевое)
решение с начальным условием $h(t_0)=0$, то и $\lambda\, h(t) $ --
тоже решение. По теореме~\ref{th.eu-1}, $h$ -- единственное, с точностью до
умножения на константу, решение уравнение Якоби с начальным
условием $h(t_0)=0$.
\begin{defi}\label{d.sopr}
Точка $\tau \in (t_0, t_1]$ называется сопряженной к точке $t_0$,
если $h(\tau)=0$, где $\tau$ -- решение уравнения Якоби.
\end{defi}
Будем говорить, что выполнено {\em условие Якоби} ({\em усиленное
условие Якоби}), если интервал $(t_0, t_1)$ (соответственно,
полуинтервал $(t_0, t_1]$) не содержит сопряж\"енных точек.



\begin{theorem}\label{th.Jakobi}
Если экстремаль $\hat x$ доставляет локальный минимум в
простейшей задаче и выполнено усиленное условие Лежандра, то
выполнено условие Якоби.
\end{theorem}
Для доказательства нам понадобится следующий факт, который мы примем без доказательства.
\begin{theorem}\label{th.ve}[теорема Вейерштрасса-Эрдмана]
Если экстремаль $\hat x \in PC^1$ доставляет абсолютный  минимум в
простейшей задаче, то функция импульса $L_{\dot x}(t, \hat x(t), \dot {\hat x}(t))$
на этой экстремали непрерывна от~$t$.
\end{theorem}

{\tt Доказательство теоремы~\ref{th.Jakobi}.} Если $\hat x\in {\rm locmin}$, то согласно предложению~\ref{p.second2},
 $\cK \ge 0$ на пространстве $C_0^1[t_0, t_1]$, а значит, поскольку~$\cK(0) = 0$, имеем
 $\min\limits_{h \in
C^1_0[t_0, t_1]}\cK(h)\, = \, 0$, и по лемме о скруглении
углов~(лемма~\ref{l.skrug}),  $\min\limits_{h \in
PC^1_0[t_0, t_1]}\cK(h)\, = \, 0$. Предположим теперь, что
 решение
уравнения Якоби~$\hat h$ обращается в ноль в некоторой точке~$\tau
\in (t_0, t_1)$. Рассмотрим простейшую задачу
$$
{\cK}_{\, \tau}(h)\, = \, \int_{t_0}^{\tau}\, \bigl(\, Ah^2\, +
2\, B\, h \, \dot h \, + \, C\, {\dot h}^2\, \bigr)\, d t\, \to \,
\min \, , \qquad h(t_0)\, = \, h(\tau)\, = \, 0.
$$
Для любого $\lambda $ функция $\lambda \hat h$ -- решение
уравнения Якоби. Положив $f(\lambda)\, = \, {\cK}_{\,
\tau}(\lambda \hat h)$, получаем
$$
f\,'(\lambda)\ = \ \lim_{\alpha \to 0}\, \frac{{\cK}_{\,
\tau}(\lambda \hat h \, + \, \alpha \hat h)\, - \, {\cK}_{\,
\tau}(\lambda \hat h )}{\alpha}\ = \  \ 0\, ,
$$
так как $\lambda \hat h$ удовлетворяет уравнению Эйлера-Лагранжа
на функционал $\cK$. Итак, $f(\lambda)\, \equiv \, {\rm const}$ и
$\, f(0)\, = \, 0$, откуда $f(\lambda)\, \equiv \, 0$, а значит
${\cK}_{\, \tau}(\hat h)\, = \, 0$. Следовательно, для функции
$$
\tilde h (t)\quad = \quad \left\{
\begin{array}{lll}
\hat h\, (t) & \mbox{при} \ & \ t \, \in \, [t_0, \tau] ; \\
0\ & \mbox{при} \ & \, t \, \in \, (\tau, t_1] \, .
\end{array}
\right.
$$
имеем $\cK(\tilde h)\, = \, 0$. Таким образом,  функция $\tilde h$
 доставляет абсолютный  локальный минимум функционала~$\cK$. Теперь
воспользуемся теоремой Вейерштрасса-Эрдмана (теорема~\ref{th.ve}).
 Обозначив через $\tilde p(t)$ импульс
функционала $\cK$ на экстремали $\tilde h$, получаем
$$
\tilde p\, (\tau - 0)\ = \ \tilde p\, (\tau +0)\, .
$$
 Так как $\tilde p(t)\, = \,
2B(t)\, \tilde h(t)\, + \, 2C(t)\, \dot {\tilde h}(t)$, то $\,
\tilde p(\tau +0)\, = \, 0$ (поскольку $\tilde h(\tau + 0)\, = \,
\dot {\tilde h}(\tau + 0)\, = \, 0$). Значит $2B(\tau - 0)\,
\tilde h(\tau - 0)\, + \, 2C(\tau - 0)\, \dot {\tilde h}(\tau -
0)$. Далее, $\tilde h(\tau - 0)\, = \, \tilde h (\tau)\, = \, 0$,
откуда $2C(\tau)\, \dot {\tilde h}(\tau - 0)$. По условиям
Лежандра $2C(\tau) \, > \, 0$, откуда $\, \dot {\tilde h}(\tau -
0)\, = \, 0$, а значит $\dot {\hat h}(\tau)\, = 0$. Последнее
невозможно в силу теоремы существования и единственности решения
дифференциального уравнения (уравнение Якоби -- второго порядка, и
оно имеет только нулевое решение с граничными условиями $\dot
h(\tau)\, =  \, h(\tau)\, = \, 0$).


{\hfill $\Box$}
\smallskip

Итак, необходимым условием локального минимума в простейшей задаче
является выполнение условия Лежандра, а при выполнении усиленного
условия Лежандра -- условие Якоби. Перейд\"ем теперь к достаточным
условиям.
\begin{theorem}\label{th.LJ}
Если в простейшей задаче на  экстремали $\hat x$ выполнены
усиленные условия Лежандра и Якоби, то $\hat x$ доставляет локальный
минимум.
\end{theorem}
Для доказательства мы воспользуемся
формулой~(\ref{eq.result-rikatti1}), где функция $\omega(t)$
ищется из уравнения Рикатти~(\ref{eq.rikatti1}). Для простейшей
задачи формула выглядит так:
\begin{equation}\label{eq.result-rikatti2}
\ddot \varphi(0)\quad = \quad \int_{t_0}^{t_1} \,  \Bigl(\,
C^{1/2}\, \dot h\, + \, C^{-1/2}\, \bigl( B\, +\, \omega \,
\bigr)\, h\,  \Bigr)^2 \, d t \, ,
\end{equation}
а уравнение Рикатти:
\begin{equation}\label{eq.rikatti2}
\dot \omega \ = \ \frac{\bigl( B \, + \, \omega\bigr)^2}{C}\ - \
A\, ,
\end{equation}
где $\, A \, = \, \hat L_{x \, x}\, , \ B \, = \, \hat L_{x \,
\dot x}\, , \ C \ = \ \hat L_{\dot x \, \dot x}$. Вначале
установим два вспомогательных утверждения.
\begin{lemma}\label{l.Jakobi-positive}
Если выполнены усиленные условия Лежандра и Якоби, то существует
решение уравнения Якоби, положительное всюду на отрезке $[t_0,
t_1]$.
\end{lemma}
{\tt Доказательство}. Как мы знаем, существует решение $h_0$
которое положительно на полуинтервале $(t_0, t_1]$ и $h_0(t_0)\, =
\, 0$. C другой стороны, существует решение $h_1$ с начальными
условиями $h_1(t_0)\, = \, 1, {\dot h_1}(t_0)\, = \, 0$. Из
непрерывности этих решений следует что функция $h_0 + \alpha h_1$
положительна при любом малом $\alpha > 0$.


{\hfill $\Box$}
\smallskip

\begin{lemma}\label{l.rikatti-positive}
Если $h$ -- решение уравнения Якоби, положительное всюду на
отрезке $[t_0, t_1]$, то $\omega \, = \, - \, B\, - \, C\,
\frac{\dot h}{h}$ -- решение уравнения
Рикатти~(\ref{eq.rikatti2}).
\end{lemma}
{\tt Доказательство}. Имеем
$$
\omega \, h \quad  = \quad  - B\, h \ - \ C\, \dot h\, .
$$
Далее,
$$
\frac{d}{dt}\, \bigl( \omega \, h\, \bigr)\quad  = \quad \dot
\omega\, h \ + \  \omega\, \dot h\, .
$$
Из уравнения  Якоби следует, что
$$
\frac{d}{dt}\, \bigl( \omega \, h\, \bigr)\quad  = \quad
\frac{d}{dt}\, \bigl( \, - B\, h \ - \ C \, \dot h\, \bigr)\quad =
\quad - A\, h \ - \ B \, \dot h\,
$$
Приравнивая полученные выражения для $\, \frac{d}{dt}\, \bigl(
\omega \, h\, \bigr)$, получаем
$$
\bigl( \dot \omega \ + \ A\,  \bigr)\, h \quad = \quad \bigl( -\,
\omega \ - \ B\, \bigr)\, \dot h
$$
Выражая из этого равенства $\dot \omega$ и подставляя $\,
\frac{\dot h}{h}\, = \, - \, \frac{\omega + B}{C}$, получаем, что
$\omega$ удовлетворяет уравнению Рикатти:
$$
\dot \omega \ = \ - \, (\omega + B)\frac{\dot h}{h} \, - \, A \ = \ \frac{(\omega + B)^2}{C}\, - \, A\, .
$$
  Применив теперь
формулу~(\ref{eq.result-rikatti2}), получаем
\smallskip

Теперь мы в состоянии доказать теорему~\ref{th.LJ}.
\smallskip

{\tt Доказательство теоремы~\ref{th.LJ}.} Разложим функцию $L(t,
x, \dot x)$ по формуле Тейлора до второго члена в точке $(t, \hat
x, \dot {\hat x})$. Получим
$$
\begin{array}{l}
L(t, \hat x + h, \dot {\hat x} + \dot h)\ =  \ L(t, \hat x, \dot
{\hat x} )\ + \
\hat L_{x}(t)\, h(t) \ + \ \hat L_{\dot x}(t)\, \dot h (t)\ +\  \\
\ +\ \hat L_{x\, x}(t)\, h^2(t) \ + \ 2\, \hat L_{x\, \dot x}(t)\,
h (t)\, \dot h (t)\ + \ \hat L_{\dot x \, \dot x}(t)\, {\dot h}^2
(t)\ + \ o\bigl(\, h^2(t)\, + \, {\dot h}^2(t)\bigr)\, ,
\end{array}
$$
причем ``о малое'' -- равномерно по $t$ при $h^2(t)\, + \, {\dot
h}^2(t) \, \to \, 0$. Следовательно, при малых значениях
$\|h\|_{C^1[t_0, t_1]}$ имеем
$$
\begin{array}{l}
L(t, \hat x + h, \dot {\hat x} + \dot h)\ \ge  \ L(t, \hat x, \dot
{\hat x} )\ + \
\hat L_{x}(t)\, h(t) \ + \ \hat L_{\dot x}(t)\, \dot h (t)\ +\  \\
\ +\ \hat L_{x\, x}(t)\, h^2(t) \ + \ 2\, \hat L_{x\, \dot x}(t)\,
h (t)\, \dot h (t)\ + \ \hat L_{\dot x \, \dot x}(t)\, {\dot h}^2
(t)\ - \ \delta \bigl(\, h^2(t)\, + \, {\dot h}^2(t)\bigr)\, ,
\end{array}
$$
где $\delta >0$ -- сколь угодно малое число. Проинтегрировав
данное равенство по отрезку $[t_0, t_1]$,  и учитывая, что
$\int_{t_0}^{t_1}\, \Bigl( \, \hat L_{x}(t)\, h(t) \, + \, \hat
L_{\dot x}(t)\, \dot h (t)\, \Bigr)\, d  t \ = \ 0$ (поскольку
$\hat x$ удовлетворяет уравнению Эйлера-Лагранжа), а также
обозначая $\hat L_{x\, x}(t)\, = \, A(t)\, , \, \hat L_{x\, \dot
x}(t)\, = \, B(t)\, , \, \hat L_{\dot x \, \dot x}(t)\, = \,
C(t)$, получаем
$$
\cJ\, \bigl( \hat x + h\bigr)\ \ge  \ \cJ\, \bigl( \hat x \bigr)\
+ \ \int_{t_0}^{t_1}\, \Bigl(\, \bigl( \, A(t)\, - \, \delta \,
\bigr) \, h^2(t) \ + \ 2\, B(t)\, h (t)\, \dot h (t)\ + \ \bigl(
\, C(t)\, - \delta \, \bigr)\, {\dot h}^2 (t)\, \Bigr)\, d t\, .
$$
Если $\delta$ достаточно мало, то второе слагаемое (квадратичный
функционал, который мы обозначим через~$\cK_{\delta}$)
удовлетворяет усиленному условию Лежандра ($C(t)\, - \, \delta \,
> \, 0$ при $t \it [t_0, t_1]$). Кроме того, в силу непрерывной
зависимости решений линейного дифференциального уравнения от его
коэффициентов, квадратичный функционал~$\cK_{\delta}$ при малых
$\delta$ также удовлетворяет усиленному условию Якоби. Применяя
леммы~\ref{l.Jakobi-positive} и \ref{l.rikatti-positive},
получаем, что уравнение Рикатти, соответствующее
функционалу~$\cK_{\delta}$,  имеет непрерывное решение на отрезке
$[t_0, t_1]$, из чего, пользуясь
формулой~(\ref{eq.result-rikatti2}), заключаем, что $\,
\cK_{\delta}\, \ge \, 0$.



{\hfill $\Box$}
\smallskip

В случае, когда функционал $\cJ$ является суммой квадратичного и
линейного функционала, т.е., имеет вид $\cJ(x)\  = \  \cJ_{\mbox{к}}(x)\, + \, \cJ_{\mbox{л}}(x) \ = \ $
 $$
\int_{t_0}^{t_1}\, \Bigl[ \, A(t)x^2(t)\, + \,
2\, B(t)x(t){\dot x}(t)\, + \, C(t)\, {\dot x}^2\, \Bigr] \ + \ \Bigl[
\, 2\, D(t)\, x (t)\, + \, 2\, E(t)\, \dot x (t)\, \Bigr]\   d \,
t\, ,
$$
условия Лежандра и Якоби позволяют провести полный (вернее,
почти полный) анализ простейшей задачи на абсолютный минимум.
Обозначим через $S_{\min}$ наименьшее значение $\cJ(x)$ среди всех
допустимых $C^1$-функций (а значит, в силу леммы о срезании углов,
и среди $PC^1$-функций).
\begin{theorem}\label{th.quadratic}
Если $\cJ(x)$ -- ``квадратичный плюс линейный'' функционал, и
$\hat x$ -- экстремаль, то

а) если не выполнено условие Лежандра, то $S_{\min}\, = \, -\,
\infty$;

б) если выполнено усиленное условие Лежандра, но не выполнено
условие Якоби, то ${S_{\min} = -\, \infty}$;

в) если выполнены усиленные условия Лежандра и Якоби, то $\hat
x\in {\rm absmin}$ и $S_{\min}\, = \, \cJ(\hat x)$.
\end{theorem}
{\tt Доказательство}. Если $\cJ$ -- квадратичный функционал, то
$\varphi (\lambda) = \cJ(\hat x +  \lambda \hat h)$ -- квадратичная функция,
а значит она совпадает во своим разложением в ряд
Тейлора до второго члена: $\varphi(\lambda) = \varphi(0) + \dot \varphi(0)\lambda +
\frac12 \, \ddot \varphi(0)\lambda^2$. Поскольку $\hat x$ -- экстремаль, $\dot \varphi (0) = 0$.
Значит $\varphi(\lambda) -  \varphi(0) \, = \, \frac12 \, \ddot \varphi(0)\lambda^2$.
 В
случаях (а) и (б), в силу теорем~\ref{th.Legandre} и
\ref{th.Jakobi}, найдется $h \in C_0^1$, при котором
  $\ddot \varphi (0) < 0$,
а значит $\cJ(\hat x + \lambda h) = \varphi (\lambda) = \varphi (0) + \frac12 \ddot \varphi(0)\lambda^2 \to -\infty$
при $\lambda \to \infty$.
 В случае~(с)
из теоремы~\ref{th.LJ} заключаем, что $\ddot \varphi (0) \ge 0$ при всех $h\in C^1_0$,
а значит $\cJ(\hat x + h) - \cJ(\hat x) \, = \, \varphi (1) - \varphi (0) \, = \,
\frac12 \varphi (0) \, \ge \, 0$, следовательно, $\hat x$ -- абсолютный минимум.

{\hfill $\Box$}
\smallskip



\newpage

\smallskip

\begin{center}
\textbf{ЛИТЕРАТУРА}
\end{center}
\bigskip

\noindent 1. И.Г.Петровский, {\em Лекции по теории обыкновенных дифференциальных уравнений},
М. Издательство Московского Университета, 1984.
\smallskip

\noindent 2. Л.С.Понтрягин, {\em Обыкновенные дифференциальные уравнения}, М. Наука, 1974
\smallskip

\noindent 3. А.Ф.Филиппов, {\em Сборник задач по дифференциальным уравнениям},  М. Интеграл-Пресс, 1998.
\smallskip

\noindent 4. В.С.Владимиров (ред.),  {\em Сборник задач по уравнениям математической физики}, М. Физматлит, 2004.
\smallskip

\noindent 5. В.Ю.Протасов, {\em Вариационное исчисление и оптимальное управление}, лекции, 2014,
http://new.math.msu.su/department/opu/node/92
\smallskip

\noindent 6. Н.П.Осмоловский, В.М.Тихомиров (ред.), {\it
Теория экстремальных задач}, М. МЦНМО, 2008.
\smallskip

\noindent 7. Дж.Ортега, У.Пул, {\em Введение в численные методы решения дифференциальных уравнений}, М. Наука, 1986.
\smallskip



\end{document}





